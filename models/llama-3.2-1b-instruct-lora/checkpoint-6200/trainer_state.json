{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.496,
  "eval_steps": 500,
  "global_step": 6200,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 8e-05,
      "grad_norm": 0.3169328272342682,
      "learning_rate": 4e-05,
      "loss": 0.867,
      "step": 1
    },
    {
      "epoch": 0.00016,
      "grad_norm": 0.37641480565071106,
      "learning_rate": 8e-05,
      "loss": 0.9624,
      "step": 2
    },
    {
      "epoch": 0.00024,
      "grad_norm": 0.4001370370388031,
      "learning_rate": 0.00012,
      "loss": 1.1647,
      "step": 3
    },
    {
      "epoch": 0.00032,
      "grad_norm": 0.3358885645866394,
      "learning_rate": 0.00016,
      "loss": 1.044,
      "step": 4
    },
    {
      "epoch": 0.0004,
      "grad_norm": 0.3077332079410553,
      "learning_rate": 0.0002,
      "loss": 0.8336,
      "step": 5
    },
    {
      "epoch": 0.00048,
      "grad_norm": 0.32603031396865845,
      "learning_rate": 0.00019999466595546075,
      "loss": 1.0567,
      "step": 6
    },
    {
      "epoch": 0.00056,
      "grad_norm": 0.26944106817245483,
      "learning_rate": 0.00019998933191092146,
      "loss": 0.747,
      "step": 7
    },
    {
      "epoch": 0.00064,
      "grad_norm": 0.325325071811676,
      "learning_rate": 0.0001999839978663822,
      "loss": 1.1114,
      "step": 8
    },
    {
      "epoch": 0.00072,
      "grad_norm": 0.2544408142566681,
      "learning_rate": 0.00019997866382184291,
      "loss": 1.0045,
      "step": 9
    },
    {
      "epoch": 0.0008,
      "grad_norm": 0.2643159031867981,
      "learning_rate": 0.00019997332977730365,
      "loss": 0.8796,
      "step": 10
    },
    {
      "epoch": 0.00088,
      "grad_norm": 0.22393567860126495,
      "learning_rate": 0.00019996799573276437,
      "loss": 0.9978,
      "step": 11
    },
    {
      "epoch": 0.00096,
      "grad_norm": 0.27400368452072144,
      "learning_rate": 0.0001999626616882251,
      "loss": 1.2519,
      "step": 12
    },
    {
      "epoch": 0.00104,
      "grad_norm": 0.31987661123275757,
      "learning_rate": 0.00019995732764368585,
      "loss": 1.0613,
      "step": 13
    },
    {
      "epoch": 0.00112,
      "grad_norm": 0.26268866658210754,
      "learning_rate": 0.00019995199359914656,
      "loss": 0.7457,
      "step": 14
    },
    {
      "epoch": 0.0012,
      "grad_norm": 0.2419602870941162,
      "learning_rate": 0.0001999466595546073,
      "loss": 1.0004,
      "step": 15
    },
    {
      "epoch": 0.00128,
      "grad_norm": 0.2844768464565277,
      "learning_rate": 0.000199941325510068,
      "loss": 0.7347,
      "step": 16
    },
    {
      "epoch": 0.00136,
      "grad_norm": 0.2982957065105438,
      "learning_rate": 0.00019993599146552875,
      "loss": 1.1424,
      "step": 17
    },
    {
      "epoch": 0.00144,
      "grad_norm": 0.3522827923297882,
      "learning_rate": 0.00019993065742098946,
      "loss": 0.9744,
      "step": 18
    },
    {
      "epoch": 0.00152,
      "grad_norm": 0.2445269078016281,
      "learning_rate": 0.0001999253233764502,
      "loss": 0.8752,
      "step": 19
    },
    {
      "epoch": 0.0016,
      "grad_norm": 0.31813329458236694,
      "learning_rate": 0.00019991998933191094,
      "loss": 1.009,
      "step": 20
    },
    {
      "epoch": 0.00168,
      "grad_norm": 0.3151523470878601,
      "learning_rate": 0.00019991465528737166,
      "loss": 0.9908,
      "step": 21
    },
    {
      "epoch": 0.00176,
      "grad_norm": 0.2699331045150757,
      "learning_rate": 0.0001999093212428324,
      "loss": 0.9798,
      "step": 22
    },
    {
      "epoch": 0.00184,
      "grad_norm": 0.23686955869197845,
      "learning_rate": 0.0001999039871982931,
      "loss": 1.1367,
      "step": 23
    },
    {
      "epoch": 0.00192,
      "grad_norm": 0.3374638855457306,
      "learning_rate": 0.00019989865315375385,
      "loss": 0.986,
      "step": 24
    },
    {
      "epoch": 0.002,
      "grad_norm": 0.25948187708854675,
      "learning_rate": 0.00019989331910921456,
      "loss": 0.7572,
      "step": 25
    },
    {
      "epoch": 0.00208,
      "grad_norm": 0.2864637076854706,
      "learning_rate": 0.0001998879850646753,
      "loss": 0.9454,
      "step": 26
    },
    {
      "epoch": 0.00216,
      "grad_norm": 0.34859323501586914,
      "learning_rate": 0.00019988265102013604,
      "loss": 0.9668,
      "step": 27
    },
    {
      "epoch": 0.00224,
      "grad_norm": 0.3126269578933716,
      "learning_rate": 0.00019987731697559676,
      "loss": 0.897,
      "step": 28
    },
    {
      "epoch": 0.00232,
      "grad_norm": 0.2837940752506256,
      "learning_rate": 0.0001998719829310575,
      "loss": 1.2496,
      "step": 29
    },
    {
      "epoch": 0.0024,
      "grad_norm": 0.30828285217285156,
      "learning_rate": 0.0001998666488865182,
      "loss": 1.174,
      "step": 30
    },
    {
      "epoch": 0.00248,
      "grad_norm": 0.3027263283729553,
      "learning_rate": 0.00019986131484197895,
      "loss": 0.8114,
      "step": 31
    },
    {
      "epoch": 0.00256,
      "grad_norm": 0.2155446708202362,
      "learning_rate": 0.0001998559807974397,
      "loss": 0.63,
      "step": 32
    },
    {
      "epoch": 0.00264,
      "grad_norm": 0.3194091320037842,
      "learning_rate": 0.0001998506467529004,
      "loss": 0.7449,
      "step": 33
    },
    {
      "epoch": 0.00272,
      "grad_norm": 0.25644975900650024,
      "learning_rate": 0.00019984531270836114,
      "loss": 0.6914,
      "step": 34
    },
    {
      "epoch": 0.0028,
      "grad_norm": 0.28142598271369934,
      "learning_rate": 0.00019983997866382185,
      "loss": 0.8757,
      "step": 35
    },
    {
      "epoch": 0.00288,
      "grad_norm": 0.30799931287765503,
      "learning_rate": 0.0001998346446192826,
      "loss": 1.0986,
      "step": 36
    },
    {
      "epoch": 0.00296,
      "grad_norm": 0.32541218400001526,
      "learning_rate": 0.0001998293105747433,
      "loss": 1.0212,
      "step": 37
    },
    {
      "epoch": 0.00304,
      "grad_norm": 0.24506057798862457,
      "learning_rate": 0.00019982397653020405,
      "loss": 0.8106,
      "step": 38
    },
    {
      "epoch": 0.00312,
      "grad_norm": 0.2824341952800751,
      "learning_rate": 0.00019981864248566478,
      "loss": 0.875,
      "step": 39
    },
    {
      "epoch": 0.0032,
      "grad_norm": 0.2708032429218292,
      "learning_rate": 0.0001998133084411255,
      "loss": 1.121,
      "step": 40
    },
    {
      "epoch": 0.00328,
      "grad_norm": 0.31670835614204407,
      "learning_rate": 0.00019980797439658624,
      "loss": 0.8487,
      "step": 41
    },
    {
      "epoch": 0.00336,
      "grad_norm": 0.3072170317173004,
      "learning_rate": 0.00019980264035204695,
      "loss": 1.1738,
      "step": 42
    },
    {
      "epoch": 0.00344,
      "grad_norm": 0.25911131501197815,
      "learning_rate": 0.0001997973063075077,
      "loss": 0.8704,
      "step": 43
    },
    {
      "epoch": 0.00352,
      "grad_norm": 0.29210278391838074,
      "learning_rate": 0.0001997919722629684,
      "loss": 0.9483,
      "step": 44
    },
    {
      "epoch": 0.0036,
      "grad_norm": 0.2868219017982483,
      "learning_rate": 0.00019978663821842914,
      "loss": 0.8587,
      "step": 45
    },
    {
      "epoch": 0.00368,
      "grad_norm": 0.29112303256988525,
      "learning_rate": 0.00019978130417388988,
      "loss": 0.98,
      "step": 46
    },
    {
      "epoch": 0.00376,
      "grad_norm": 0.3012864291667938,
      "learning_rate": 0.0001997759701293506,
      "loss": 0.8823,
      "step": 47
    },
    {
      "epoch": 0.00384,
      "grad_norm": 0.24212494492530823,
      "learning_rate": 0.00019977063608481134,
      "loss": 0.7625,
      "step": 48
    },
    {
      "epoch": 0.00392,
      "grad_norm": 0.37867146730422974,
      "learning_rate": 0.00019976530204027205,
      "loss": 1.1299,
      "step": 49
    },
    {
      "epoch": 0.004,
      "grad_norm": 0.2918904721736908,
      "learning_rate": 0.0001997599679957328,
      "loss": 1.1287,
      "step": 50
    },
    {
      "epoch": 0.00408,
      "grad_norm": 0.20897312462329865,
      "learning_rate": 0.0001997546339511935,
      "loss": 0.5252,
      "step": 51
    },
    {
      "epoch": 0.00416,
      "grad_norm": 0.21717709302902222,
      "learning_rate": 0.00019974929990665424,
      "loss": 1.0597,
      "step": 52
    },
    {
      "epoch": 0.00424,
      "grad_norm": 0.7762017846107483,
      "learning_rate": 0.00019974396586211498,
      "loss": 1.503,
      "step": 53
    },
    {
      "epoch": 0.00432,
      "grad_norm": 0.2856839895248413,
      "learning_rate": 0.0001997386318175757,
      "loss": 0.7813,
      "step": 54
    },
    {
      "epoch": 0.0044,
      "grad_norm": 0.3716669976711273,
      "learning_rate": 0.00019973329777303643,
      "loss": 1.2216,
      "step": 55
    },
    {
      "epoch": 0.00448,
      "grad_norm": 0.2545906603336334,
      "learning_rate": 0.00019972796372849715,
      "loss": 1.2678,
      "step": 56
    },
    {
      "epoch": 0.00456,
      "grad_norm": 0.22216464579105377,
      "learning_rate": 0.00019972262968395789,
      "loss": 0.847,
      "step": 57
    },
    {
      "epoch": 0.00464,
      "grad_norm": 0.24205493927001953,
      "learning_rate": 0.0001997172956394186,
      "loss": 0.9825,
      "step": 58
    },
    {
      "epoch": 0.00472,
      "grad_norm": 0.24749809503555298,
      "learning_rate": 0.00019971196159487934,
      "loss": 0.8541,
      "step": 59
    },
    {
      "epoch": 0.0048,
      "grad_norm": 0.28282734751701355,
      "learning_rate": 0.00019970662755034005,
      "loss": 1.0447,
      "step": 60
    },
    {
      "epoch": 0.00488,
      "grad_norm": 0.2805071473121643,
      "learning_rate": 0.0001997012935058008,
      "loss": 0.8423,
      "step": 61
    },
    {
      "epoch": 0.00496,
      "grad_norm": 0.3353039622306824,
      "learning_rate": 0.00019969595946126153,
      "loss": 1.005,
      "step": 62
    },
    {
      "epoch": 0.00504,
      "grad_norm": 0.23542776703834534,
      "learning_rate": 0.00019969062541672224,
      "loss": 0.9051,
      "step": 63
    },
    {
      "epoch": 0.00512,
      "grad_norm": 0.515824019908905,
      "learning_rate": 0.00019968529137218298,
      "loss": 1.3386,
      "step": 64
    },
    {
      "epoch": 0.0052,
      "grad_norm": 0.24302107095718384,
      "learning_rate": 0.0001996799573276437,
      "loss": 0.8851,
      "step": 65
    },
    {
      "epoch": 0.00528,
      "grad_norm": 0.25633540749549866,
      "learning_rate": 0.00019967462328310444,
      "loss": 1.0408,
      "step": 66
    },
    {
      "epoch": 0.00536,
      "grad_norm": 0.36186712980270386,
      "learning_rate": 0.00019966928923856515,
      "loss": 0.9521,
      "step": 67
    },
    {
      "epoch": 0.00544,
      "grad_norm": 0.27965936064720154,
      "learning_rate": 0.0001996639551940259,
      "loss": 0.9095,
      "step": 68
    },
    {
      "epoch": 0.00552,
      "grad_norm": 0.25076207518577576,
      "learning_rate": 0.0001996586211494866,
      "loss": 0.9717,
      "step": 69
    },
    {
      "epoch": 0.0056,
      "grad_norm": 0.24485066533088684,
      "learning_rate": 0.00019965328710494734,
      "loss": 0.7375,
      "step": 70
    },
    {
      "epoch": 0.00568,
      "grad_norm": 0.28044992685317993,
      "learning_rate": 0.00019964795306040805,
      "loss": 0.5917,
      "step": 71
    },
    {
      "epoch": 0.00576,
      "grad_norm": 0.25702568888664246,
      "learning_rate": 0.0001996426190158688,
      "loss": 0.8129,
      "step": 72
    },
    {
      "epoch": 0.00584,
      "grad_norm": 0.316682368516922,
      "learning_rate": 0.0001996372849713295,
      "loss": 1.0397,
      "step": 73
    },
    {
      "epoch": 0.00592,
      "grad_norm": 0.3025951087474823,
      "learning_rate": 0.00019963195092679025,
      "loss": 1.1144,
      "step": 74
    },
    {
      "epoch": 0.006,
      "grad_norm": 0.34156474471092224,
      "learning_rate": 0.00019962661688225096,
      "loss": 0.8262,
      "step": 75
    },
    {
      "epoch": 0.00608,
      "grad_norm": 0.3375294804573059,
      "learning_rate": 0.0001996212828377117,
      "loss": 0.8108,
      "step": 76
    },
    {
      "epoch": 0.00616,
      "grad_norm": 0.3259269595146179,
      "learning_rate": 0.0001996159487931724,
      "loss": 1.0816,
      "step": 77
    },
    {
      "epoch": 0.00624,
      "grad_norm": 0.34307950735092163,
      "learning_rate": 0.00019961061474863315,
      "loss": 0.8768,
      "step": 78
    },
    {
      "epoch": 0.00632,
      "grad_norm": 0.381291002035141,
      "learning_rate": 0.0001996052807040939,
      "loss": 1.2605,
      "step": 79
    },
    {
      "epoch": 0.0064,
      "grad_norm": 0.30994483828544617,
      "learning_rate": 0.0001995999466595546,
      "loss": 0.9451,
      "step": 80
    },
    {
      "epoch": 0.00648,
      "grad_norm": 0.2720746397972107,
      "learning_rate": 0.00019959461261501534,
      "loss": 0.7556,
      "step": 81
    },
    {
      "epoch": 0.00656,
      "grad_norm": 0.2873397469520569,
      "learning_rate": 0.00019958927857047606,
      "loss": 0.9801,
      "step": 82
    },
    {
      "epoch": 0.00664,
      "grad_norm": 0.32522401213645935,
      "learning_rate": 0.0001995839445259368,
      "loss": 1.0141,
      "step": 83
    },
    {
      "epoch": 0.00672,
      "grad_norm": 0.2928211987018585,
      "learning_rate": 0.0001995786104813975,
      "loss": 1.1559,
      "step": 84
    },
    {
      "epoch": 0.0068,
      "grad_norm": 0.24272221326828003,
      "learning_rate": 0.00019957327643685825,
      "loss": 0.977,
      "step": 85
    },
    {
      "epoch": 0.00688,
      "grad_norm": 0.24084442853927612,
      "learning_rate": 0.000199567942392319,
      "loss": 0.7016,
      "step": 86
    },
    {
      "epoch": 0.00696,
      "grad_norm": 0.2720614969730377,
      "learning_rate": 0.0001995626083477797,
      "loss": 0.9875,
      "step": 87
    },
    {
      "epoch": 0.00704,
      "grad_norm": 0.253480464220047,
      "learning_rate": 0.00019955727430324044,
      "loss": 0.7482,
      "step": 88
    },
    {
      "epoch": 0.00712,
      "grad_norm": 0.27480801939964294,
      "learning_rate": 0.00019955194025870115,
      "loss": 0.9245,
      "step": 89
    },
    {
      "epoch": 0.0072,
      "grad_norm": 0.22333067655563354,
      "learning_rate": 0.0001995466062141619,
      "loss": 1.2996,
      "step": 90
    },
    {
      "epoch": 0.00728,
      "grad_norm": 0.2987625300884247,
      "learning_rate": 0.0001995412721696226,
      "loss": 0.9006,
      "step": 91
    },
    {
      "epoch": 0.00736,
      "grad_norm": 0.2714812159538269,
      "learning_rate": 0.00019953593812508335,
      "loss": 0.6318,
      "step": 92
    },
    {
      "epoch": 0.00744,
      "grad_norm": 0.2965690493583679,
      "learning_rate": 0.00019953060408054409,
      "loss": 0.9389,
      "step": 93
    },
    {
      "epoch": 0.00752,
      "grad_norm": 0.22997945547103882,
      "learning_rate": 0.0001995252700360048,
      "loss": 0.8187,
      "step": 94
    },
    {
      "epoch": 0.0076,
      "grad_norm": 0.22219453752040863,
      "learning_rate": 0.00019951993599146554,
      "loss": 0.6095,
      "step": 95
    },
    {
      "epoch": 0.00768,
      "grad_norm": 0.4089166224002838,
      "learning_rate": 0.00019951460194692625,
      "loss": 0.706,
      "step": 96
    },
    {
      "epoch": 0.00776,
      "grad_norm": 0.30542564392089844,
      "learning_rate": 0.000199509267902387,
      "loss": 0.7847,
      "step": 97
    },
    {
      "epoch": 0.00784,
      "grad_norm": 0.34029605984687805,
      "learning_rate": 0.0001995039338578477,
      "loss": 0.8468,
      "step": 98
    },
    {
      "epoch": 0.00792,
      "grad_norm": 0.31844329833984375,
      "learning_rate": 0.00019949859981330844,
      "loss": 0.8608,
      "step": 99
    },
    {
      "epoch": 0.008,
      "grad_norm": 0.30265331268310547,
      "learning_rate": 0.00019949326576876918,
      "loss": 0.8517,
      "step": 100
    },
    {
      "epoch": 0.00808,
      "grad_norm": 0.3457029163837433,
      "learning_rate": 0.0001994879317242299,
      "loss": 0.7848,
      "step": 101
    },
    {
      "epoch": 0.00816,
      "grad_norm": 0.2846776247024536,
      "learning_rate": 0.00019948259767969064,
      "loss": 0.86,
      "step": 102
    },
    {
      "epoch": 0.00824,
      "grad_norm": 0.444862425327301,
      "learning_rate": 0.00019947726363515135,
      "loss": 0.91,
      "step": 103
    },
    {
      "epoch": 0.00832,
      "grad_norm": 0.267093300819397,
      "learning_rate": 0.0001994719295906121,
      "loss": 0.8271,
      "step": 104
    },
    {
      "epoch": 0.0084,
      "grad_norm": 0.3331499695777893,
      "learning_rate": 0.0001994665955460728,
      "loss": 1.1759,
      "step": 105
    },
    {
      "epoch": 0.00848,
      "grad_norm": 0.2983306348323822,
      "learning_rate": 0.00019946126150153354,
      "loss": 0.9001,
      "step": 106
    },
    {
      "epoch": 0.00856,
      "grad_norm": 0.3386725187301636,
      "learning_rate": 0.00019945592745699428,
      "loss": 1.0816,
      "step": 107
    },
    {
      "epoch": 0.00864,
      "grad_norm": 0.22041143476963043,
      "learning_rate": 0.000199450593412455,
      "loss": 0.6901,
      "step": 108
    },
    {
      "epoch": 0.00872,
      "grad_norm": 0.3184848129749298,
      "learning_rate": 0.00019944525936791573,
      "loss": 0.8262,
      "step": 109
    },
    {
      "epoch": 0.0088,
      "grad_norm": 0.2843732237815857,
      "learning_rate": 0.00019943992532337645,
      "loss": 0.855,
      "step": 110
    },
    {
      "epoch": 0.00888,
      "grad_norm": 0.27114319801330566,
      "learning_rate": 0.0001994345912788372,
      "loss": 0.7301,
      "step": 111
    },
    {
      "epoch": 0.00896,
      "grad_norm": 0.3299241364002228,
      "learning_rate": 0.0001994292572342979,
      "loss": 0.9623,
      "step": 112
    },
    {
      "epoch": 0.00904,
      "grad_norm": 0.2428872287273407,
      "learning_rate": 0.00019942392318975864,
      "loss": 1.1235,
      "step": 113
    },
    {
      "epoch": 0.00912,
      "grad_norm": 0.320768803358078,
      "learning_rate": 0.00019941858914521938,
      "loss": 1.0395,
      "step": 114
    },
    {
      "epoch": 0.0092,
      "grad_norm": 0.2438681274652481,
      "learning_rate": 0.0001994132551006801,
      "loss": 0.7453,
      "step": 115
    },
    {
      "epoch": 0.00928,
      "grad_norm": 0.2523404657840729,
      "learning_rate": 0.00019940792105614083,
      "loss": 0.9554,
      "step": 116
    },
    {
      "epoch": 0.00936,
      "grad_norm": 0.29921382665634155,
      "learning_rate": 0.00019940258701160154,
      "loss": 0.6683,
      "step": 117
    },
    {
      "epoch": 0.00944,
      "grad_norm": 0.3186550438404083,
      "learning_rate": 0.00019939725296706228,
      "loss": 0.9303,
      "step": 118
    },
    {
      "epoch": 0.00952,
      "grad_norm": 0.2794284224510193,
      "learning_rate": 0.000199391918922523,
      "loss": 0.774,
      "step": 119
    },
    {
      "epoch": 0.0096,
      "grad_norm": 0.29239344596862793,
      "learning_rate": 0.00019938658487798374,
      "loss": 0.5123,
      "step": 120
    },
    {
      "epoch": 0.00968,
      "grad_norm": 0.2362651228904724,
      "learning_rate": 0.00019938125083344448,
      "loss": 0.6974,
      "step": 121
    },
    {
      "epoch": 0.00976,
      "grad_norm": 0.26083073019981384,
      "learning_rate": 0.0001993759167889052,
      "loss": 0.9114,
      "step": 122
    },
    {
      "epoch": 0.00984,
      "grad_norm": 0.3440001606941223,
      "learning_rate": 0.00019937058274436593,
      "loss": 0.7619,
      "step": 123
    },
    {
      "epoch": 0.00992,
      "grad_norm": 0.2590058147907257,
      "learning_rate": 0.00019936524869982664,
      "loss": 0.9023,
      "step": 124
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.3631259799003601,
      "learning_rate": 0.00019935991465528738,
      "loss": 0.9243,
      "step": 125
    },
    {
      "epoch": 0.01008,
      "grad_norm": 0.2850075662136078,
      "learning_rate": 0.0001993545806107481,
      "loss": 0.8366,
      "step": 126
    },
    {
      "epoch": 0.01016,
      "grad_norm": 0.2396758645772934,
      "learning_rate": 0.00019934924656620883,
      "loss": 1.0469,
      "step": 127
    },
    {
      "epoch": 0.01024,
      "grad_norm": 0.24408258497714996,
      "learning_rate": 0.00019934391252166957,
      "loss": 0.6483,
      "step": 128
    },
    {
      "epoch": 0.01032,
      "grad_norm": 0.2857873737812042,
      "learning_rate": 0.0001993385784771303,
      "loss": 0.7632,
      "step": 129
    },
    {
      "epoch": 0.0104,
      "grad_norm": 0.24095189571380615,
      "learning_rate": 0.00019933324443259103,
      "loss": 0.955,
      "step": 130
    },
    {
      "epoch": 0.01048,
      "grad_norm": 0.2360854297876358,
      "learning_rate": 0.00019932791038805174,
      "loss": 0.9039,
      "step": 131
    },
    {
      "epoch": 0.01056,
      "grad_norm": 0.2412298321723938,
      "learning_rate": 0.00019932257634351248,
      "loss": 1.0956,
      "step": 132
    },
    {
      "epoch": 0.01064,
      "grad_norm": 0.2905096411705017,
      "learning_rate": 0.00019931724229897322,
      "loss": 0.7813,
      "step": 133
    },
    {
      "epoch": 0.01072,
      "grad_norm": 0.2832210958003998,
      "learning_rate": 0.00019931190825443393,
      "loss": 0.7546,
      "step": 134
    },
    {
      "epoch": 0.0108,
      "grad_norm": 0.24084357917308807,
      "learning_rate": 0.00019930657420989467,
      "loss": 0.4552,
      "step": 135
    },
    {
      "epoch": 0.01088,
      "grad_norm": 0.31552785634994507,
      "learning_rate": 0.00019930124016535538,
      "loss": 0.7368,
      "step": 136
    },
    {
      "epoch": 0.01096,
      "grad_norm": 0.2988850772380829,
      "learning_rate": 0.00019929590612081612,
      "loss": 0.67,
      "step": 137
    },
    {
      "epoch": 0.01104,
      "grad_norm": 0.306860089302063,
      "learning_rate": 0.00019929057207627684,
      "loss": 0.5922,
      "step": 138
    },
    {
      "epoch": 0.01112,
      "grad_norm": 0.3285124599933624,
      "learning_rate": 0.00019928523803173758,
      "loss": 0.5405,
      "step": 139
    },
    {
      "epoch": 0.0112,
      "grad_norm": 0.3360498249530792,
      "learning_rate": 0.00019927990398719832,
      "loss": 1.0464,
      "step": 140
    },
    {
      "epoch": 0.01128,
      "grad_norm": 0.2800007462501526,
      "learning_rate": 0.00019927456994265903,
      "loss": 0.8928,
      "step": 141
    },
    {
      "epoch": 0.01136,
      "grad_norm": 0.3185897767543793,
      "learning_rate": 0.00019926923589811977,
      "loss": 0.7885,
      "step": 142
    },
    {
      "epoch": 0.01144,
      "grad_norm": 0.307468056678772,
      "learning_rate": 0.00019926390185358048,
      "loss": 0.82,
      "step": 143
    },
    {
      "epoch": 0.01152,
      "grad_norm": 0.26600050926208496,
      "learning_rate": 0.00019925856780904122,
      "loss": 0.8766,
      "step": 144
    },
    {
      "epoch": 0.0116,
      "grad_norm": 0.3258781135082245,
      "learning_rate": 0.00019925323376450193,
      "loss": 0.8629,
      "step": 145
    },
    {
      "epoch": 0.01168,
      "grad_norm": 0.37385696172714233,
      "learning_rate": 0.00019924789971996267,
      "loss": 1.109,
      "step": 146
    },
    {
      "epoch": 0.01176,
      "grad_norm": 0.2359895557165146,
      "learning_rate": 0.00019924256567542341,
      "loss": 0.8088,
      "step": 147
    },
    {
      "epoch": 0.01184,
      "grad_norm": 0.25832992792129517,
      "learning_rate": 0.00019923723163088413,
      "loss": 0.9125,
      "step": 148
    },
    {
      "epoch": 0.01192,
      "grad_norm": 0.278411328792572,
      "learning_rate": 0.00019923189758634487,
      "loss": 0.9037,
      "step": 149
    },
    {
      "epoch": 0.012,
      "grad_norm": 0.2749476730823517,
      "learning_rate": 0.00019922656354180558,
      "loss": 0.7413,
      "step": 150
    },
    {
      "epoch": 0.01208,
      "grad_norm": 0.30966952443122864,
      "learning_rate": 0.00019922122949726632,
      "loss": 0.7979,
      "step": 151
    },
    {
      "epoch": 0.01216,
      "grad_norm": 0.5445422530174255,
      "learning_rate": 0.00019921589545272703,
      "loss": 1.3622,
      "step": 152
    },
    {
      "epoch": 0.01224,
      "grad_norm": 0.26388558745384216,
      "learning_rate": 0.00019921056140818777,
      "loss": 0.707,
      "step": 153
    },
    {
      "epoch": 0.01232,
      "grad_norm": 0.4523898661136627,
      "learning_rate": 0.0001992052273636485,
      "loss": 0.8257,
      "step": 154
    },
    {
      "epoch": 0.0124,
      "grad_norm": 0.24521329998970032,
      "learning_rate": 0.00019919989331910922,
      "loss": 0.7044,
      "step": 155
    },
    {
      "epoch": 0.01248,
      "grad_norm": 0.26015612483024597,
      "learning_rate": 0.00019919455927456996,
      "loss": 1.1471,
      "step": 156
    },
    {
      "epoch": 0.01256,
      "grad_norm": 0.3013669550418854,
      "learning_rate": 0.00019918922523003068,
      "loss": 0.8926,
      "step": 157
    },
    {
      "epoch": 0.01264,
      "grad_norm": 0.23304897546768188,
      "learning_rate": 0.00019918389118549142,
      "loss": 0.8486,
      "step": 158
    },
    {
      "epoch": 0.01272,
      "grad_norm": 0.2764665186405182,
      "learning_rate": 0.00019917855714095213,
      "loss": 0.878,
      "step": 159
    },
    {
      "epoch": 0.0128,
      "grad_norm": 0.26341933012008667,
      "learning_rate": 0.00019917322309641287,
      "loss": 1.2266,
      "step": 160
    },
    {
      "epoch": 0.01288,
      "grad_norm": 0.32487672567367554,
      "learning_rate": 0.0001991678890518736,
      "loss": 0.9556,
      "step": 161
    },
    {
      "epoch": 0.01296,
      "grad_norm": 0.2571658790111542,
      "learning_rate": 0.00019916255500733432,
      "loss": 1.0567,
      "step": 162
    },
    {
      "epoch": 0.01304,
      "grad_norm": 0.25417467951774597,
      "learning_rate": 0.00019915722096279506,
      "loss": 1.0839,
      "step": 163
    },
    {
      "epoch": 0.01312,
      "grad_norm": 0.2691808044910431,
      "learning_rate": 0.00019915188691825578,
      "loss": 0.7431,
      "step": 164
    },
    {
      "epoch": 0.0132,
      "grad_norm": 0.3507695198059082,
      "learning_rate": 0.00019914655287371652,
      "loss": 0.8887,
      "step": 165
    },
    {
      "epoch": 0.01328,
      "grad_norm": 0.32291537523269653,
      "learning_rate": 0.00019914121882917723,
      "loss": 0.7245,
      "step": 166
    },
    {
      "epoch": 0.01336,
      "grad_norm": 0.3115256130695343,
      "learning_rate": 0.00019913588478463797,
      "loss": 0.7687,
      "step": 167
    },
    {
      "epoch": 0.01344,
      "grad_norm": 0.2726995646953583,
      "learning_rate": 0.0001991305507400987,
      "loss": 0.999,
      "step": 168
    },
    {
      "epoch": 0.01352,
      "grad_norm": 0.2547569274902344,
      "learning_rate": 0.00019912521669555942,
      "loss": 0.9953,
      "step": 169
    },
    {
      "epoch": 0.0136,
      "grad_norm": 0.30661654472351074,
      "learning_rate": 0.00019911988265102016,
      "loss": 0.7909,
      "step": 170
    },
    {
      "epoch": 0.01368,
      "grad_norm": 0.3381497859954834,
      "learning_rate": 0.00019911454860648087,
      "loss": 0.8848,
      "step": 171
    },
    {
      "epoch": 0.01376,
      "grad_norm": 0.27235373854637146,
      "learning_rate": 0.0001991092145619416,
      "loss": 1.0715,
      "step": 172
    },
    {
      "epoch": 0.01384,
      "grad_norm": 0.26402708888053894,
      "learning_rate": 0.00019910388051740233,
      "loss": 0.9113,
      "step": 173
    },
    {
      "epoch": 0.01392,
      "grad_norm": 0.3332419991493225,
      "learning_rate": 0.00019909854647286307,
      "loss": 1.1029,
      "step": 174
    },
    {
      "epoch": 0.014,
      "grad_norm": 0.2840176224708557,
      "learning_rate": 0.0001990932124283238,
      "loss": 0.919,
      "step": 175
    },
    {
      "epoch": 0.01408,
      "grad_norm": 0.3366507887840271,
      "learning_rate": 0.00019908787838378452,
      "loss": 0.8494,
      "step": 176
    },
    {
      "epoch": 0.01416,
      "grad_norm": 0.3119632303714752,
      "learning_rate": 0.00019908254433924526,
      "loss": 0.8231,
      "step": 177
    },
    {
      "epoch": 0.01424,
      "grad_norm": 0.29874223470687866,
      "learning_rate": 0.00019907721029470597,
      "loss": 1.0251,
      "step": 178
    },
    {
      "epoch": 0.01432,
      "grad_norm": 0.30250605940818787,
      "learning_rate": 0.0001990718762501667,
      "loss": 0.8245,
      "step": 179
    },
    {
      "epoch": 0.0144,
      "grad_norm": 0.29283836483955383,
      "learning_rate": 0.00019906654220562745,
      "loss": 0.7851,
      "step": 180
    },
    {
      "epoch": 0.01448,
      "grad_norm": 0.24570517241954803,
      "learning_rate": 0.00019906120816108816,
      "loss": 0.9422,
      "step": 181
    },
    {
      "epoch": 0.01456,
      "grad_norm": 0.25042906403541565,
      "learning_rate": 0.0001990558741165489,
      "loss": 1.0061,
      "step": 182
    },
    {
      "epoch": 0.01464,
      "grad_norm": 0.2965678572654724,
      "learning_rate": 0.00019905054007200962,
      "loss": 0.9371,
      "step": 183
    },
    {
      "epoch": 0.01472,
      "grad_norm": 0.30839473009109497,
      "learning_rate": 0.00019904520602747036,
      "loss": 1.1462,
      "step": 184
    },
    {
      "epoch": 0.0148,
      "grad_norm": 0.22973300516605377,
      "learning_rate": 0.00019903987198293107,
      "loss": 0.8526,
      "step": 185
    },
    {
      "epoch": 0.01488,
      "grad_norm": 0.29283884167671204,
      "learning_rate": 0.0001990345379383918,
      "loss": 0.9679,
      "step": 186
    },
    {
      "epoch": 0.01496,
      "grad_norm": 0.26381024718284607,
      "learning_rate": 0.00019902920389385252,
      "loss": 0.7674,
      "step": 187
    },
    {
      "epoch": 0.01504,
      "grad_norm": 0.25290602445602417,
      "learning_rate": 0.00019902386984931326,
      "loss": 0.7858,
      "step": 188
    },
    {
      "epoch": 0.01512,
      "grad_norm": 0.24521446228027344,
      "learning_rate": 0.00019901853580477397,
      "loss": 0.8446,
      "step": 189
    },
    {
      "epoch": 0.0152,
      "grad_norm": 0.27342745661735535,
      "learning_rate": 0.0001990132017602347,
      "loss": 0.8716,
      "step": 190
    },
    {
      "epoch": 0.01528,
      "grad_norm": 0.23330383002758026,
      "learning_rate": 0.00019900786771569545,
      "loss": 0.9692,
      "step": 191
    },
    {
      "epoch": 0.01536,
      "grad_norm": 0.2096412479877472,
      "learning_rate": 0.00019900253367115617,
      "loss": 0.952,
      "step": 192
    },
    {
      "epoch": 0.01544,
      "grad_norm": 0.2625907063484192,
      "learning_rate": 0.0001989971996266169,
      "loss": 0.7937,
      "step": 193
    },
    {
      "epoch": 0.01552,
      "grad_norm": 0.2464924156665802,
      "learning_rate": 0.00019899186558207762,
      "loss": 0.864,
      "step": 194
    },
    {
      "epoch": 0.0156,
      "grad_norm": 0.28040385246276855,
      "learning_rate": 0.00019898653153753836,
      "loss": 1.0885,
      "step": 195
    },
    {
      "epoch": 0.01568,
      "grad_norm": 0.2793574631214142,
      "learning_rate": 0.00019898119749299907,
      "loss": 0.6698,
      "step": 196
    },
    {
      "epoch": 0.01576,
      "grad_norm": 0.23045296967029572,
      "learning_rate": 0.0001989758634484598,
      "loss": 0.5344,
      "step": 197
    },
    {
      "epoch": 0.01584,
      "grad_norm": 0.22919264435768127,
      "learning_rate": 0.00019897052940392052,
      "loss": 0.7508,
      "step": 198
    },
    {
      "epoch": 0.01592,
      "grad_norm": 0.37648314237594604,
      "learning_rate": 0.00019896519535938126,
      "loss": 1.0137,
      "step": 199
    },
    {
      "epoch": 0.016,
      "grad_norm": 0.24612416326999664,
      "learning_rate": 0.00019895986131484198,
      "loss": 0.5998,
      "step": 200
    },
    {
      "epoch": 0.01608,
      "grad_norm": 0.3362670838832855,
      "learning_rate": 0.00019895452727030272,
      "loss": 0.8639,
      "step": 201
    },
    {
      "epoch": 0.01616,
      "grad_norm": 0.32685214281082153,
      "learning_rate": 0.00019894919322576343,
      "loss": 0.967,
      "step": 202
    },
    {
      "epoch": 0.01624,
      "grad_norm": 0.29583829641342163,
      "learning_rate": 0.00019894385918122417,
      "loss": 0.7972,
      "step": 203
    },
    {
      "epoch": 0.01632,
      "grad_norm": 0.26041385531425476,
      "learning_rate": 0.00019893852513668488,
      "loss": 0.598,
      "step": 204
    },
    {
      "epoch": 0.0164,
      "grad_norm": 0.312116801738739,
      "learning_rate": 0.00019893319109214562,
      "loss": 0.8218,
      "step": 205
    },
    {
      "epoch": 0.01648,
      "grad_norm": 0.24607229232788086,
      "learning_rate": 0.00019892785704760636,
      "loss": 0.7919,
      "step": 206
    },
    {
      "epoch": 0.01656,
      "grad_norm": 0.2471255362033844,
      "learning_rate": 0.00019892252300306707,
      "loss": 0.7145,
      "step": 207
    },
    {
      "epoch": 0.01664,
      "grad_norm": 0.3102409839630127,
      "learning_rate": 0.0001989171889585278,
      "loss": 0.834,
      "step": 208
    },
    {
      "epoch": 0.01672,
      "grad_norm": 0.2522231340408325,
      "learning_rate": 0.00019891185491398853,
      "loss": 0.6963,
      "step": 209
    },
    {
      "epoch": 0.0168,
      "grad_norm": 0.26363837718963623,
      "learning_rate": 0.00019890652086944927,
      "loss": 1.0677,
      "step": 210
    },
    {
      "epoch": 0.01688,
      "grad_norm": 0.22545328736305237,
      "learning_rate": 0.00019890118682490998,
      "loss": 0.8859,
      "step": 211
    },
    {
      "epoch": 0.01696,
      "grad_norm": 0.46286892890930176,
      "learning_rate": 0.00019889585278037072,
      "loss": 0.685,
      "step": 212
    },
    {
      "epoch": 0.01704,
      "grad_norm": 0.2984165847301483,
      "learning_rate": 0.00019889051873583143,
      "loss": 0.94,
      "step": 213
    },
    {
      "epoch": 0.01712,
      "grad_norm": 0.25037306547164917,
      "learning_rate": 0.00019888518469129217,
      "loss": 1.1973,
      "step": 214
    },
    {
      "epoch": 0.0172,
      "grad_norm": 0.3641672730445862,
      "learning_rate": 0.0001988798506467529,
      "loss": 0.8732,
      "step": 215
    },
    {
      "epoch": 0.01728,
      "grad_norm": 0.24964193999767303,
      "learning_rate": 0.00019887451660221362,
      "loss": 0.8442,
      "step": 216
    },
    {
      "epoch": 0.01736,
      "grad_norm": 0.2107943296432495,
      "learning_rate": 0.00019886918255767436,
      "loss": 1.12,
      "step": 217
    },
    {
      "epoch": 0.01744,
      "grad_norm": 0.39682677388191223,
      "learning_rate": 0.00019886384851313508,
      "loss": 1.0179,
      "step": 218
    },
    {
      "epoch": 0.01752,
      "grad_norm": 0.25515276193618774,
      "learning_rate": 0.00019885851446859582,
      "loss": 0.8965,
      "step": 219
    },
    {
      "epoch": 0.0176,
      "grad_norm": 0.22894293069839478,
      "learning_rate": 0.00019885318042405653,
      "loss": 1.0707,
      "step": 220
    },
    {
      "epoch": 0.01768,
      "grad_norm": 0.2918274998664856,
      "learning_rate": 0.00019884784637951727,
      "loss": 0.7366,
      "step": 221
    },
    {
      "epoch": 0.01776,
      "grad_norm": 0.29969367384910583,
      "learning_rate": 0.000198842512334978,
      "loss": 1.0414,
      "step": 222
    },
    {
      "epoch": 0.01784,
      "grad_norm": 0.3039109408855438,
      "learning_rate": 0.00019883717829043872,
      "loss": 0.9487,
      "step": 223
    },
    {
      "epoch": 0.01792,
      "grad_norm": 0.3580251634120941,
      "learning_rate": 0.00019883184424589946,
      "loss": 0.846,
      "step": 224
    },
    {
      "epoch": 0.018,
      "grad_norm": 0.3270283639431,
      "learning_rate": 0.00019882651020136017,
      "loss": 0.7929,
      "step": 225
    },
    {
      "epoch": 0.01808,
      "grad_norm": 0.29825320839881897,
      "learning_rate": 0.00019882117615682091,
      "loss": 0.6042,
      "step": 226
    },
    {
      "epoch": 0.01816,
      "grad_norm": 0.29504698514938354,
      "learning_rate": 0.00019881584211228163,
      "loss": 1.0387,
      "step": 227
    },
    {
      "epoch": 0.01824,
      "grad_norm": 0.38412681221961975,
      "learning_rate": 0.00019881050806774237,
      "loss": 0.7269,
      "step": 228
    },
    {
      "epoch": 0.01832,
      "grad_norm": 0.3290736675262451,
      "learning_rate": 0.0001988051740232031,
      "loss": 0.6766,
      "step": 229
    },
    {
      "epoch": 0.0184,
      "grad_norm": 0.24506992101669312,
      "learning_rate": 0.00019879983997866382,
      "loss": 0.9642,
      "step": 230
    },
    {
      "epoch": 0.01848,
      "grad_norm": 0.2814512550830841,
      "learning_rate": 0.00019879450593412456,
      "loss": 0.88,
      "step": 231
    },
    {
      "epoch": 0.01856,
      "grad_norm": 0.31574100255966187,
      "learning_rate": 0.00019878917188958527,
      "loss": 0.7052,
      "step": 232
    },
    {
      "epoch": 0.01864,
      "grad_norm": 0.2985987663269043,
      "learning_rate": 0.000198783837845046,
      "loss": 0.6312,
      "step": 233
    },
    {
      "epoch": 0.01872,
      "grad_norm": 0.28226083517074585,
      "learning_rate": 0.00019877850380050675,
      "loss": 0.9239,
      "step": 234
    },
    {
      "epoch": 0.0188,
      "grad_norm": 0.2618023753166199,
      "learning_rate": 0.00019877316975596746,
      "loss": 1.2725,
      "step": 235
    },
    {
      "epoch": 0.01888,
      "grad_norm": 0.2958279550075531,
      "learning_rate": 0.0001987678357114282,
      "loss": 0.7162,
      "step": 236
    },
    {
      "epoch": 0.01896,
      "grad_norm": 0.3059598505496979,
      "learning_rate": 0.00019876250166688892,
      "loss": 1.098,
      "step": 237
    },
    {
      "epoch": 0.01904,
      "grad_norm": 0.31029489636421204,
      "learning_rate": 0.00019875716762234966,
      "loss": 0.9055,
      "step": 238
    },
    {
      "epoch": 0.01912,
      "grad_norm": 0.28326892852783203,
      "learning_rate": 0.00019875183357781037,
      "loss": 1.0089,
      "step": 239
    },
    {
      "epoch": 0.0192,
      "grad_norm": 0.24755632877349854,
      "learning_rate": 0.0001987464995332711,
      "loss": 1.0777,
      "step": 240
    },
    {
      "epoch": 0.01928,
      "grad_norm": 0.24760088324546814,
      "learning_rate": 0.00019874116548873185,
      "loss": 0.5263,
      "step": 241
    },
    {
      "epoch": 0.01936,
      "grad_norm": 0.347256600856781,
      "learning_rate": 0.00019873583144419256,
      "loss": 0.8921,
      "step": 242
    },
    {
      "epoch": 0.01944,
      "grad_norm": 0.3137722909450531,
      "learning_rate": 0.0001987304973996533,
      "loss": 0.7116,
      "step": 243
    },
    {
      "epoch": 0.01952,
      "grad_norm": 0.34850773215293884,
      "learning_rate": 0.00019872516335511401,
      "loss": 0.8914,
      "step": 244
    },
    {
      "epoch": 0.0196,
      "grad_norm": 0.21169234812259674,
      "learning_rate": 0.00019871982931057475,
      "loss": 0.8588,
      "step": 245
    },
    {
      "epoch": 0.01968,
      "grad_norm": 0.3885076642036438,
      "learning_rate": 0.00019871449526603547,
      "loss": 1.0618,
      "step": 246
    },
    {
      "epoch": 0.01976,
      "grad_norm": 0.4032736122608185,
      "learning_rate": 0.0001987091612214962,
      "loss": 1.3793,
      "step": 247
    },
    {
      "epoch": 0.01984,
      "grad_norm": 0.352999210357666,
      "learning_rate": 0.00019870382717695695,
      "loss": 0.8063,
      "step": 248
    },
    {
      "epoch": 0.01992,
      "grad_norm": 0.26421067118644714,
      "learning_rate": 0.00019869849313241766,
      "loss": 0.8508,
      "step": 249
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.27771198749542236,
      "learning_rate": 0.0001986931590878784,
      "loss": 0.7763,
      "step": 250
    },
    {
      "epoch": 0.02008,
      "grad_norm": 0.30744969844818115,
      "learning_rate": 0.0001986878250433391,
      "loss": 0.8207,
      "step": 251
    },
    {
      "epoch": 0.02016,
      "grad_norm": 0.26691532135009766,
      "learning_rate": 0.00019868249099879985,
      "loss": 0.9142,
      "step": 252
    },
    {
      "epoch": 0.02024,
      "grad_norm": 0.34571629762649536,
      "learning_rate": 0.00019867715695426056,
      "loss": 0.8215,
      "step": 253
    },
    {
      "epoch": 0.02032,
      "grad_norm": 0.2520934045314789,
      "learning_rate": 0.0001986718229097213,
      "loss": 1.1225,
      "step": 254
    },
    {
      "epoch": 0.0204,
      "grad_norm": 0.3258916735649109,
      "learning_rate": 0.00019866648886518204,
      "loss": 0.9198,
      "step": 255
    },
    {
      "epoch": 0.02048,
      "grad_norm": 0.23521193861961365,
      "learning_rate": 0.00019866115482064276,
      "loss": 1.0833,
      "step": 256
    },
    {
      "epoch": 0.02056,
      "grad_norm": 0.3391472101211548,
      "learning_rate": 0.0001986558207761035,
      "loss": 0.8549,
      "step": 257
    },
    {
      "epoch": 0.02064,
      "grad_norm": 0.3335699141025543,
      "learning_rate": 0.0001986504867315642,
      "loss": 0.8085,
      "step": 258
    },
    {
      "epoch": 0.02072,
      "grad_norm": 0.23929756879806519,
      "learning_rate": 0.00019864515268702495,
      "loss": 0.9142,
      "step": 259
    },
    {
      "epoch": 0.0208,
      "grad_norm": 0.27233800292015076,
      "learning_rate": 0.00019863981864248566,
      "loss": 1.0459,
      "step": 260
    },
    {
      "epoch": 0.02088,
      "grad_norm": 0.2433224767446518,
      "learning_rate": 0.0001986344845979464,
      "loss": 0.9726,
      "step": 261
    },
    {
      "epoch": 0.02096,
      "grad_norm": 0.3304094970226288,
      "learning_rate": 0.00019862915055340714,
      "loss": 0.5481,
      "step": 262
    },
    {
      "epoch": 0.02104,
      "grad_norm": 0.2389547973871231,
      "learning_rate": 0.00019862381650886785,
      "loss": 0.6195,
      "step": 263
    },
    {
      "epoch": 0.02112,
      "grad_norm": 0.3286142349243164,
      "learning_rate": 0.0001986184824643286,
      "loss": 1.1528,
      "step": 264
    },
    {
      "epoch": 0.0212,
      "grad_norm": 0.25401538610458374,
      "learning_rate": 0.0001986131484197893,
      "loss": 0.7505,
      "step": 265
    },
    {
      "epoch": 0.02128,
      "grad_norm": 0.26096588373184204,
      "learning_rate": 0.00019860781437525005,
      "loss": 0.6299,
      "step": 266
    },
    {
      "epoch": 0.02136,
      "grad_norm": 0.30436763167381287,
      "learning_rate": 0.00019860248033071076,
      "loss": 0.933,
      "step": 267
    },
    {
      "epoch": 0.02144,
      "grad_norm": 0.25950390100479126,
      "learning_rate": 0.0001985971462861715,
      "loss": 0.7749,
      "step": 268
    },
    {
      "epoch": 0.02152,
      "grad_norm": 0.35509634017944336,
      "learning_rate": 0.00019859181224163224,
      "loss": 0.8784,
      "step": 269
    },
    {
      "epoch": 0.0216,
      "grad_norm": 0.22117599844932556,
      "learning_rate": 0.00019858647819709295,
      "loss": 1.1068,
      "step": 270
    },
    {
      "epoch": 0.02168,
      "grad_norm": 0.23025329411029816,
      "learning_rate": 0.0001985811441525537,
      "loss": 0.461,
      "step": 271
    },
    {
      "epoch": 0.02176,
      "grad_norm": 0.2667991518974304,
      "learning_rate": 0.0001985758101080144,
      "loss": 0.9774,
      "step": 272
    },
    {
      "epoch": 0.02184,
      "grad_norm": 0.2508910298347473,
      "learning_rate": 0.00019857047606347514,
      "loss": 1.0992,
      "step": 273
    },
    {
      "epoch": 0.02192,
      "grad_norm": 0.31585589051246643,
      "learning_rate": 0.00019856514201893586,
      "loss": 0.7915,
      "step": 274
    },
    {
      "epoch": 0.022,
      "grad_norm": 0.24158746004104614,
      "learning_rate": 0.0001985598079743966,
      "loss": 0.7627,
      "step": 275
    },
    {
      "epoch": 0.02208,
      "grad_norm": 0.24745994806289673,
      "learning_rate": 0.00019855447392985734,
      "loss": 0.7815,
      "step": 276
    },
    {
      "epoch": 0.02216,
      "grad_norm": 0.2186170518398285,
      "learning_rate": 0.00019854913988531805,
      "loss": 0.4845,
      "step": 277
    },
    {
      "epoch": 0.02224,
      "grad_norm": 0.24985307455062866,
      "learning_rate": 0.0001985438058407788,
      "loss": 0.9941,
      "step": 278
    },
    {
      "epoch": 0.02232,
      "grad_norm": 0.3094275891780853,
      "learning_rate": 0.0001985384717962395,
      "loss": 0.9602,
      "step": 279
    },
    {
      "epoch": 0.0224,
      "grad_norm": 0.3514719307422638,
      "learning_rate": 0.00019853313775170024,
      "loss": 0.814,
      "step": 280
    },
    {
      "epoch": 0.02248,
      "grad_norm": 0.2669498026371002,
      "learning_rate": 0.00019852780370716098,
      "loss": 1.2177,
      "step": 281
    },
    {
      "epoch": 0.02256,
      "grad_norm": 0.286903440952301,
      "learning_rate": 0.0001985224696626217,
      "loss": 0.7729,
      "step": 282
    },
    {
      "epoch": 0.02264,
      "grad_norm": 0.21730808913707733,
      "learning_rate": 0.00019851713561808243,
      "loss": 0.567,
      "step": 283
    },
    {
      "epoch": 0.02272,
      "grad_norm": 0.3885485529899597,
      "learning_rate": 0.00019851180157354315,
      "loss": 1.0436,
      "step": 284
    },
    {
      "epoch": 0.0228,
      "grad_norm": 0.32252082228660583,
      "learning_rate": 0.0001985064675290039,
      "loss": 0.6774,
      "step": 285
    },
    {
      "epoch": 0.02288,
      "grad_norm": 0.39286065101623535,
      "learning_rate": 0.0001985011334844646,
      "loss": 0.8333,
      "step": 286
    },
    {
      "epoch": 0.02296,
      "grad_norm": 0.26241809129714966,
      "learning_rate": 0.00019849579943992534,
      "loss": 0.9767,
      "step": 287
    },
    {
      "epoch": 0.02304,
      "grad_norm": 0.2362273633480072,
      "learning_rate": 0.00019849046539538608,
      "loss": 0.7065,
      "step": 288
    },
    {
      "epoch": 0.02312,
      "grad_norm": 0.29883235692977905,
      "learning_rate": 0.0001984851313508468,
      "loss": 0.7847,
      "step": 289
    },
    {
      "epoch": 0.0232,
      "grad_norm": 0.24970698356628418,
      "learning_rate": 0.00019847979730630753,
      "loss": 0.8488,
      "step": 290
    },
    {
      "epoch": 0.02328,
      "grad_norm": 0.22815737128257751,
      "learning_rate": 0.00019847446326176825,
      "loss": 1.1403,
      "step": 291
    },
    {
      "epoch": 0.02336,
      "grad_norm": 0.2748304009437561,
      "learning_rate": 0.00019846912921722899,
      "loss": 0.9428,
      "step": 292
    },
    {
      "epoch": 0.02344,
      "grad_norm": 0.2540269196033478,
      "learning_rate": 0.0001984637951726897,
      "loss": 0.8516,
      "step": 293
    },
    {
      "epoch": 0.02352,
      "grad_norm": 0.31758591532707214,
      "learning_rate": 0.00019845846112815044,
      "loss": 0.6741,
      "step": 294
    },
    {
      "epoch": 0.0236,
      "grad_norm": 0.3244621157646179,
      "learning_rate": 0.00019845312708361118,
      "loss": 1.0927,
      "step": 295
    },
    {
      "epoch": 0.02368,
      "grad_norm": 0.29743629693984985,
      "learning_rate": 0.0001984477930390719,
      "loss": 0.8952,
      "step": 296
    },
    {
      "epoch": 0.02376,
      "grad_norm": 0.3026044964790344,
      "learning_rate": 0.00019844245899453263,
      "loss": 1.0069,
      "step": 297
    },
    {
      "epoch": 0.02384,
      "grad_norm": 0.2959839403629303,
      "learning_rate": 0.00019843712494999334,
      "loss": 0.9563,
      "step": 298
    },
    {
      "epoch": 0.02392,
      "grad_norm": 0.2735087275505066,
      "learning_rate": 0.00019843179090545408,
      "loss": 0.6968,
      "step": 299
    },
    {
      "epoch": 0.024,
      "grad_norm": 0.2940901219844818,
      "learning_rate": 0.0001984264568609148,
      "loss": 1.3378,
      "step": 300
    },
    {
      "epoch": 0.02408,
      "grad_norm": 0.28634849190711975,
      "learning_rate": 0.00019842112281637554,
      "loss": 0.7943,
      "step": 301
    },
    {
      "epoch": 0.02416,
      "grad_norm": 0.35312244296073914,
      "learning_rate": 0.00019841578877183628,
      "loss": 0.7309,
      "step": 302
    },
    {
      "epoch": 0.02424,
      "grad_norm": 0.29498785734176636,
      "learning_rate": 0.000198410454727297,
      "loss": 0.7634,
      "step": 303
    },
    {
      "epoch": 0.02432,
      "grad_norm": 0.2504027485847473,
      "learning_rate": 0.00019840512068275773,
      "loss": 0.7307,
      "step": 304
    },
    {
      "epoch": 0.0244,
      "grad_norm": 0.42191818356513977,
      "learning_rate": 0.00019839978663821844,
      "loss": 0.9976,
      "step": 305
    },
    {
      "epoch": 0.02448,
      "grad_norm": 0.30118539929389954,
      "learning_rate": 0.00019839445259367918,
      "loss": 0.8851,
      "step": 306
    },
    {
      "epoch": 0.02456,
      "grad_norm": 0.3181660771369934,
      "learning_rate": 0.0001983891185491399,
      "loss": 0.8568,
      "step": 307
    },
    {
      "epoch": 0.02464,
      "grad_norm": 0.2315492331981659,
      "learning_rate": 0.00019838378450460063,
      "loss": 0.5528,
      "step": 308
    },
    {
      "epoch": 0.02472,
      "grad_norm": 0.2885878086090088,
      "learning_rate": 0.00019837845046006137,
      "loss": 0.8162,
      "step": 309
    },
    {
      "epoch": 0.0248,
      "grad_norm": 0.2509196698665619,
      "learning_rate": 0.00019837311641552209,
      "loss": 0.8874,
      "step": 310
    },
    {
      "epoch": 0.02488,
      "grad_norm": 0.3164367079734802,
      "learning_rate": 0.00019836778237098283,
      "loss": 0.6635,
      "step": 311
    },
    {
      "epoch": 0.02496,
      "grad_norm": 0.24116061627864838,
      "learning_rate": 0.00019836244832644354,
      "loss": 1.0319,
      "step": 312
    },
    {
      "epoch": 0.02504,
      "grad_norm": 0.24352078139781952,
      "learning_rate": 0.00019835711428190428,
      "loss": 1.035,
      "step": 313
    },
    {
      "epoch": 0.02512,
      "grad_norm": 0.22340096533298492,
      "learning_rate": 0.000198351780237365,
      "loss": 1.0064,
      "step": 314
    },
    {
      "epoch": 0.0252,
      "grad_norm": 0.2509300410747528,
      "learning_rate": 0.00019834644619282573,
      "loss": 0.7041,
      "step": 315
    },
    {
      "epoch": 0.02528,
      "grad_norm": 0.2817932963371277,
      "learning_rate": 0.00019834111214828644,
      "loss": 0.9826,
      "step": 316
    },
    {
      "epoch": 0.02536,
      "grad_norm": 0.3189140558242798,
      "learning_rate": 0.00019833577810374718,
      "loss": 1.0271,
      "step": 317
    },
    {
      "epoch": 0.02544,
      "grad_norm": 0.28792980313301086,
      "learning_rate": 0.00019833044405920792,
      "loss": 0.8442,
      "step": 318
    },
    {
      "epoch": 0.02552,
      "grad_norm": 0.34747347235679626,
      "learning_rate": 0.00019832511001466864,
      "loss": 0.9482,
      "step": 319
    },
    {
      "epoch": 0.0256,
      "grad_norm": 0.3154391944408417,
      "learning_rate": 0.00019831977597012938,
      "loss": 0.7466,
      "step": 320
    },
    {
      "epoch": 0.02568,
      "grad_norm": 0.29450076818466187,
      "learning_rate": 0.0001983144419255901,
      "loss": 0.7669,
      "step": 321
    },
    {
      "epoch": 0.02576,
      "grad_norm": 0.25590574741363525,
      "learning_rate": 0.00019830910788105083,
      "loss": 0.8889,
      "step": 322
    },
    {
      "epoch": 0.02584,
      "grad_norm": 0.39409443736076355,
      "learning_rate": 0.00019830377383651154,
      "loss": 0.8906,
      "step": 323
    },
    {
      "epoch": 0.02592,
      "grad_norm": 0.2398754358291626,
      "learning_rate": 0.00019829843979197228,
      "loss": 0.4907,
      "step": 324
    },
    {
      "epoch": 0.026,
      "grad_norm": 0.3297881484031677,
      "learning_rate": 0.000198293105747433,
      "loss": 0.6673,
      "step": 325
    },
    {
      "epoch": 0.02608,
      "grad_norm": 0.30258187651634216,
      "learning_rate": 0.00019828777170289373,
      "loss": 0.9172,
      "step": 326
    },
    {
      "epoch": 0.02616,
      "grad_norm": 0.30953776836395264,
      "learning_rate": 0.00019828243765835445,
      "loss": 1.0262,
      "step": 327
    },
    {
      "epoch": 0.02624,
      "grad_norm": 0.2570149004459381,
      "learning_rate": 0.00019827710361381519,
      "loss": 1.1078,
      "step": 328
    },
    {
      "epoch": 0.02632,
      "grad_norm": 0.39412131905555725,
      "learning_rate": 0.0001982717695692759,
      "loss": 1.0475,
      "step": 329
    },
    {
      "epoch": 0.0264,
      "grad_norm": 0.25916507840156555,
      "learning_rate": 0.00019826643552473664,
      "loss": 0.6602,
      "step": 330
    },
    {
      "epoch": 0.02648,
      "grad_norm": 0.331570565700531,
      "learning_rate": 0.00019826110148019735,
      "loss": 1.271,
      "step": 331
    },
    {
      "epoch": 0.02656,
      "grad_norm": 0.2956366240978241,
      "learning_rate": 0.0001982557674356581,
      "loss": 0.8551,
      "step": 332
    },
    {
      "epoch": 0.02664,
      "grad_norm": 0.2490050494670868,
      "learning_rate": 0.0001982504333911188,
      "loss": 0.5991,
      "step": 333
    },
    {
      "epoch": 0.02672,
      "grad_norm": 0.30337774753570557,
      "learning_rate": 0.00019824509934657954,
      "loss": 0.7595,
      "step": 334
    },
    {
      "epoch": 0.0268,
      "grad_norm": 0.2872633635997772,
      "learning_rate": 0.00019823976530204028,
      "loss": 0.5109,
      "step": 335
    },
    {
      "epoch": 0.02688,
      "grad_norm": 0.28174930810928345,
      "learning_rate": 0.000198234431257501,
      "loss": 0.7832,
      "step": 336
    },
    {
      "epoch": 0.02696,
      "grad_norm": 0.2703130841255188,
      "learning_rate": 0.00019822909721296174,
      "loss": 0.799,
      "step": 337
    },
    {
      "epoch": 0.02704,
      "grad_norm": 0.3170360028743744,
      "learning_rate": 0.00019822376316842245,
      "loss": 0.7127,
      "step": 338
    },
    {
      "epoch": 0.02712,
      "grad_norm": 0.211055189371109,
      "learning_rate": 0.0001982184291238832,
      "loss": 0.9795,
      "step": 339
    },
    {
      "epoch": 0.0272,
      "grad_norm": 0.26884904503822327,
      "learning_rate": 0.0001982130950793439,
      "loss": 0.6967,
      "step": 340
    },
    {
      "epoch": 0.02728,
      "grad_norm": 0.31126779317855835,
      "learning_rate": 0.00019820776103480464,
      "loss": 1.0884,
      "step": 341
    },
    {
      "epoch": 0.02736,
      "grad_norm": 0.2682839334011078,
      "learning_rate": 0.00019820242699026538,
      "loss": 0.866,
      "step": 342
    },
    {
      "epoch": 0.02744,
      "grad_norm": 0.2753008306026459,
      "learning_rate": 0.0001981970929457261,
      "loss": 0.8649,
      "step": 343
    },
    {
      "epoch": 0.02752,
      "grad_norm": 0.36206376552581787,
      "learning_rate": 0.00019819175890118683,
      "loss": 0.649,
      "step": 344
    },
    {
      "epoch": 0.0276,
      "grad_norm": 0.26077884435653687,
      "learning_rate": 0.00019818642485664755,
      "loss": 0.8901,
      "step": 345
    },
    {
      "epoch": 0.02768,
      "grad_norm": 0.283355712890625,
      "learning_rate": 0.00019818109081210829,
      "loss": 0.9149,
      "step": 346
    },
    {
      "epoch": 0.02776,
      "grad_norm": 0.2801063060760498,
      "learning_rate": 0.000198175756767569,
      "loss": 1.0773,
      "step": 347
    },
    {
      "epoch": 0.02784,
      "grad_norm": 0.33972111344337463,
      "learning_rate": 0.00019817042272302974,
      "loss": 0.8676,
      "step": 348
    },
    {
      "epoch": 0.02792,
      "grad_norm": 0.26624250411987305,
      "learning_rate": 0.00019816508867849048,
      "loss": 0.6911,
      "step": 349
    },
    {
      "epoch": 0.028,
      "grad_norm": 0.2271084040403366,
      "learning_rate": 0.0001981597546339512,
      "loss": 0.6469,
      "step": 350
    },
    {
      "epoch": 0.02808,
      "grad_norm": 0.24091947078704834,
      "learning_rate": 0.00019815442058941193,
      "loss": 0.6621,
      "step": 351
    },
    {
      "epoch": 0.02816,
      "grad_norm": 0.32621532678604126,
      "learning_rate": 0.00019814908654487264,
      "loss": 0.8076,
      "step": 352
    },
    {
      "epoch": 0.02824,
      "grad_norm": 0.24230508506298065,
      "learning_rate": 0.00019814375250033338,
      "loss": 0.635,
      "step": 353
    },
    {
      "epoch": 0.02832,
      "grad_norm": 0.2966284453868866,
      "learning_rate": 0.0001981384184557941,
      "loss": 0.6853,
      "step": 354
    },
    {
      "epoch": 0.0284,
      "grad_norm": 0.2846633493900299,
      "learning_rate": 0.00019813308441125484,
      "loss": 0.9196,
      "step": 355
    },
    {
      "epoch": 0.02848,
      "grad_norm": 0.2758534848690033,
      "learning_rate": 0.00019812775036671558,
      "loss": 0.8358,
      "step": 356
    },
    {
      "epoch": 0.02856,
      "grad_norm": 0.32225653529167175,
      "learning_rate": 0.0001981224163221763,
      "loss": 0.8693,
      "step": 357
    },
    {
      "epoch": 0.02864,
      "grad_norm": 0.29888302087783813,
      "learning_rate": 0.00019811708227763703,
      "loss": 0.7573,
      "step": 358
    },
    {
      "epoch": 0.02872,
      "grad_norm": 0.32181796431541443,
      "learning_rate": 0.00019811174823309774,
      "loss": 0.8289,
      "step": 359
    },
    {
      "epoch": 0.0288,
      "grad_norm": 0.2324225753545761,
      "learning_rate": 0.00019810641418855848,
      "loss": 0.8849,
      "step": 360
    },
    {
      "epoch": 0.02888,
      "grad_norm": 0.21909783780574799,
      "learning_rate": 0.0001981010801440192,
      "loss": 0.6138,
      "step": 361
    },
    {
      "epoch": 0.02896,
      "grad_norm": 0.29753074049949646,
      "learning_rate": 0.00019809574609947993,
      "loss": 0.8963,
      "step": 362
    },
    {
      "epoch": 0.02904,
      "grad_norm": 0.27609315514564514,
      "learning_rate": 0.00019809041205494067,
      "loss": 0.7336,
      "step": 363
    },
    {
      "epoch": 0.02912,
      "grad_norm": 0.3048613667488098,
      "learning_rate": 0.0001980850780104014,
      "loss": 0.7047,
      "step": 364
    },
    {
      "epoch": 0.0292,
      "grad_norm": 0.21838144958019257,
      "learning_rate": 0.00019807974396586213,
      "loss": 0.9547,
      "step": 365
    },
    {
      "epoch": 0.02928,
      "grad_norm": 0.380260705947876,
      "learning_rate": 0.00019807440992132284,
      "loss": 0.9162,
      "step": 366
    },
    {
      "epoch": 0.02936,
      "grad_norm": 0.23012693226337433,
      "learning_rate": 0.00019806907587678358,
      "loss": 1.0193,
      "step": 367
    },
    {
      "epoch": 0.02944,
      "grad_norm": 0.25724849104881287,
      "learning_rate": 0.0001980637418322443,
      "loss": 0.8256,
      "step": 368
    },
    {
      "epoch": 0.02952,
      "grad_norm": 0.28527674078941345,
      "learning_rate": 0.00019805840778770503,
      "loss": 0.7302,
      "step": 369
    },
    {
      "epoch": 0.0296,
      "grad_norm": 0.3965768814086914,
      "learning_rate": 0.00019805307374316577,
      "loss": 0.7708,
      "step": 370
    },
    {
      "epoch": 0.02968,
      "grad_norm": 0.240766242146492,
      "learning_rate": 0.00019804773969862648,
      "loss": 0.4862,
      "step": 371
    },
    {
      "epoch": 0.02976,
      "grad_norm": 0.30788061022758484,
      "learning_rate": 0.00019804240565408722,
      "loss": 1.3409,
      "step": 372
    },
    {
      "epoch": 0.02984,
      "grad_norm": 0.4352185130119324,
      "learning_rate": 0.00019803707160954794,
      "loss": 0.8594,
      "step": 373
    },
    {
      "epoch": 0.02992,
      "grad_norm": 0.29392752051353455,
      "learning_rate": 0.00019803173756500868,
      "loss": 1.0072,
      "step": 374
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.25580084323883057,
      "learning_rate": 0.0001980264035204694,
      "loss": 0.5139,
      "step": 375
    },
    {
      "epoch": 0.03008,
      "grad_norm": 0.35721173882484436,
      "learning_rate": 0.00019802106947593013,
      "loss": 1.0228,
      "step": 376
    },
    {
      "epoch": 0.03016,
      "grad_norm": 0.2779865562915802,
      "learning_rate": 0.00019801573543139087,
      "loss": 0.7524,
      "step": 377
    },
    {
      "epoch": 0.03024,
      "grad_norm": 0.25455242395401,
      "learning_rate": 0.00019801040138685158,
      "loss": 0.7065,
      "step": 378
    },
    {
      "epoch": 0.03032,
      "grad_norm": 0.3144228756427765,
      "learning_rate": 0.00019800506734231232,
      "loss": 0.8463,
      "step": 379
    },
    {
      "epoch": 0.0304,
      "grad_norm": 0.3148828148841858,
      "learning_rate": 0.00019799973329777303,
      "loss": 0.6465,
      "step": 380
    },
    {
      "epoch": 0.03048,
      "grad_norm": 0.3950086534023285,
      "learning_rate": 0.00019799439925323377,
      "loss": 0.8899,
      "step": 381
    },
    {
      "epoch": 0.03056,
      "grad_norm": 0.26874682307243347,
      "learning_rate": 0.00019798906520869451,
      "loss": 0.8884,
      "step": 382
    },
    {
      "epoch": 0.03064,
      "grad_norm": 0.32626983523368835,
      "learning_rate": 0.00019798373116415523,
      "loss": 0.7264,
      "step": 383
    },
    {
      "epoch": 0.03072,
      "grad_norm": 0.2953917384147644,
      "learning_rate": 0.00019797839711961597,
      "loss": 0.8686,
      "step": 384
    },
    {
      "epoch": 0.0308,
      "grad_norm": 0.28392860293388367,
      "learning_rate": 0.00019797306307507668,
      "loss": 0.6766,
      "step": 385
    },
    {
      "epoch": 0.03088,
      "grad_norm": 0.29658156633377075,
      "learning_rate": 0.00019796772903053742,
      "loss": 1.0526,
      "step": 386
    },
    {
      "epoch": 0.03096,
      "grad_norm": 0.31592512130737305,
      "learning_rate": 0.00019796239498599813,
      "loss": 0.853,
      "step": 387
    },
    {
      "epoch": 0.03104,
      "grad_norm": 0.2732330858707428,
      "learning_rate": 0.00019795706094145887,
      "loss": 0.9097,
      "step": 388
    },
    {
      "epoch": 0.03112,
      "grad_norm": 0.3111000955104828,
      "learning_rate": 0.0001979517268969196,
      "loss": 1.0021,
      "step": 389
    },
    {
      "epoch": 0.0312,
      "grad_norm": 0.2751576900482178,
      "learning_rate": 0.00019794639285238032,
      "loss": 0.8421,
      "step": 390
    },
    {
      "epoch": 0.03128,
      "grad_norm": 0.26825839281082153,
      "learning_rate": 0.00019794105880784106,
      "loss": 1.0529,
      "step": 391
    },
    {
      "epoch": 0.03136,
      "grad_norm": 0.28721174597740173,
      "learning_rate": 0.00019793572476330178,
      "loss": 0.6349,
      "step": 392
    },
    {
      "epoch": 0.03144,
      "grad_norm": 0.3249708414077759,
      "learning_rate": 0.00019793039071876252,
      "loss": 0.8635,
      "step": 393
    },
    {
      "epoch": 0.03152,
      "grad_norm": 0.335625022649765,
      "learning_rate": 0.00019792505667422323,
      "loss": 0.8627,
      "step": 394
    },
    {
      "epoch": 0.0316,
      "grad_norm": 0.3930656909942627,
      "learning_rate": 0.00019791972262968397,
      "loss": 1.0442,
      "step": 395
    },
    {
      "epoch": 0.03168,
      "grad_norm": 0.22637997567653656,
      "learning_rate": 0.0001979143885851447,
      "loss": 1.0919,
      "step": 396
    },
    {
      "epoch": 0.03176,
      "grad_norm": 0.2771119177341461,
      "learning_rate": 0.00019790905454060542,
      "loss": 0.8121,
      "step": 397
    },
    {
      "epoch": 0.03184,
      "grad_norm": 0.305123895406723,
      "learning_rate": 0.00019790372049606616,
      "loss": 0.6705,
      "step": 398
    },
    {
      "epoch": 0.03192,
      "grad_norm": 0.26024681329727173,
      "learning_rate": 0.00019789838645152687,
      "loss": 0.9012,
      "step": 399
    },
    {
      "epoch": 0.032,
      "grad_norm": 0.29018473625183105,
      "learning_rate": 0.00019789305240698761,
      "loss": 0.9014,
      "step": 400
    },
    {
      "epoch": 0.03208,
      "grad_norm": 0.26010021567344666,
      "learning_rate": 0.00019788771836244833,
      "loss": 1.049,
      "step": 401
    },
    {
      "epoch": 0.03216,
      "grad_norm": 0.26597946882247925,
      "learning_rate": 0.00019788238431790907,
      "loss": 1.0076,
      "step": 402
    },
    {
      "epoch": 0.03224,
      "grad_norm": 0.3431779146194458,
      "learning_rate": 0.0001978770502733698,
      "loss": 1.1087,
      "step": 403
    },
    {
      "epoch": 0.03232,
      "grad_norm": 0.28986746072769165,
      "learning_rate": 0.00019787171622883052,
      "loss": 1.0456,
      "step": 404
    },
    {
      "epoch": 0.0324,
      "grad_norm": 0.33671852946281433,
      "learning_rate": 0.00019786638218429126,
      "loss": 0.9806,
      "step": 405
    },
    {
      "epoch": 0.03248,
      "grad_norm": 0.3310624659061432,
      "learning_rate": 0.00019786104813975197,
      "loss": 0.927,
      "step": 406
    },
    {
      "epoch": 0.03256,
      "grad_norm": 0.28437331318855286,
      "learning_rate": 0.0001978557140952127,
      "loss": 0.8484,
      "step": 407
    },
    {
      "epoch": 0.03264,
      "grad_norm": 0.27995508909225464,
      "learning_rate": 0.00019785038005067343,
      "loss": 0.8351,
      "step": 408
    },
    {
      "epoch": 0.03272,
      "grad_norm": 0.2574785351753235,
      "learning_rate": 0.00019784504600613416,
      "loss": 0.8395,
      "step": 409
    },
    {
      "epoch": 0.0328,
      "grad_norm": 0.3561564087867737,
      "learning_rate": 0.0001978397119615949,
      "loss": 0.9732,
      "step": 410
    },
    {
      "epoch": 0.03288,
      "grad_norm": 0.3150686025619507,
      "learning_rate": 0.00019783437791705562,
      "loss": 0.7362,
      "step": 411
    },
    {
      "epoch": 0.03296,
      "grad_norm": 0.268581360578537,
      "learning_rate": 0.00019782904387251636,
      "loss": 0.8507,
      "step": 412
    },
    {
      "epoch": 0.03304,
      "grad_norm": 0.384727418422699,
      "learning_rate": 0.00019782370982797707,
      "loss": 0.8902,
      "step": 413
    },
    {
      "epoch": 0.03312,
      "grad_norm": 0.3657502830028534,
      "learning_rate": 0.0001978183757834378,
      "loss": 0.9979,
      "step": 414
    },
    {
      "epoch": 0.0332,
      "grad_norm": 0.2874316871166229,
      "learning_rate": 0.00019781304173889852,
      "loss": 0.8005,
      "step": 415
    },
    {
      "epoch": 0.03328,
      "grad_norm": 0.2538062334060669,
      "learning_rate": 0.00019780770769435926,
      "loss": 1.0318,
      "step": 416
    },
    {
      "epoch": 0.03336,
      "grad_norm": 0.3084830939769745,
      "learning_rate": 0.00019780237364982,
      "loss": 0.7973,
      "step": 417
    },
    {
      "epoch": 0.03344,
      "grad_norm": 0.3037903606891632,
      "learning_rate": 0.00019779703960528072,
      "loss": 0.6965,
      "step": 418
    },
    {
      "epoch": 0.03352,
      "grad_norm": 0.21926453709602356,
      "learning_rate": 0.00019779170556074145,
      "loss": 0.6317,
      "step": 419
    },
    {
      "epoch": 0.0336,
      "grad_norm": 0.31076958775520325,
      "learning_rate": 0.00019778637151620217,
      "loss": 1.0414,
      "step": 420
    },
    {
      "epoch": 0.03368,
      "grad_norm": 0.2902679145336151,
      "learning_rate": 0.0001977810374716629,
      "loss": 1.0115,
      "step": 421
    },
    {
      "epoch": 0.03376,
      "grad_norm": 0.2525545656681061,
      "learning_rate": 0.00019777570342712362,
      "loss": 1.0124,
      "step": 422
    },
    {
      "epoch": 0.03384,
      "grad_norm": 0.26171180605888367,
      "learning_rate": 0.00019777036938258436,
      "loss": 0.7687,
      "step": 423
    },
    {
      "epoch": 0.03392,
      "grad_norm": 0.3137866258621216,
      "learning_rate": 0.0001977650353380451,
      "loss": 0.9301,
      "step": 424
    },
    {
      "epoch": 0.034,
      "grad_norm": 0.2726733982563019,
      "learning_rate": 0.0001977597012935058,
      "loss": 0.594,
      "step": 425
    },
    {
      "epoch": 0.03408,
      "grad_norm": 0.2942420244216919,
      "learning_rate": 0.00019775436724896655,
      "loss": 0.9158,
      "step": 426
    },
    {
      "epoch": 0.03416,
      "grad_norm": 0.3375759720802307,
      "learning_rate": 0.00019774903320442727,
      "loss": 0.8395,
      "step": 427
    },
    {
      "epoch": 0.03424,
      "grad_norm": 0.25463518500328064,
      "learning_rate": 0.000197743699159888,
      "loss": 1.1181,
      "step": 428
    },
    {
      "epoch": 0.03432,
      "grad_norm": 0.3702458441257477,
      "learning_rate": 0.00019773836511534872,
      "loss": 0.7319,
      "step": 429
    },
    {
      "epoch": 0.0344,
      "grad_norm": 0.2944793999195099,
      "learning_rate": 0.00019773303107080946,
      "loss": 0.7127,
      "step": 430
    },
    {
      "epoch": 0.03448,
      "grad_norm": 0.3644754886627197,
      "learning_rate": 0.0001977276970262702,
      "loss": 0.7101,
      "step": 431
    },
    {
      "epoch": 0.03456,
      "grad_norm": 0.32542914152145386,
      "learning_rate": 0.0001977223629817309,
      "loss": 0.6937,
      "step": 432
    },
    {
      "epoch": 0.03464,
      "grad_norm": 0.2864992320537567,
      "learning_rate": 0.00019771702893719165,
      "loss": 0.8163,
      "step": 433
    },
    {
      "epoch": 0.03472,
      "grad_norm": 0.33845585584640503,
      "learning_rate": 0.00019771169489265236,
      "loss": 0.8289,
      "step": 434
    },
    {
      "epoch": 0.0348,
      "grad_norm": 0.30510106682777405,
      "learning_rate": 0.0001977063608481131,
      "loss": 0.9731,
      "step": 435
    },
    {
      "epoch": 0.03488,
      "grad_norm": 0.28669407963752747,
      "learning_rate": 0.00019770102680357384,
      "loss": 0.6598,
      "step": 436
    },
    {
      "epoch": 0.03496,
      "grad_norm": 0.26526933908462524,
      "learning_rate": 0.00019769569275903456,
      "loss": 0.5437,
      "step": 437
    },
    {
      "epoch": 0.03504,
      "grad_norm": 0.2511802017688751,
      "learning_rate": 0.0001976903587144953,
      "loss": 0.9864,
      "step": 438
    },
    {
      "epoch": 0.03512,
      "grad_norm": 0.27109429240226746,
      "learning_rate": 0.000197685024669956,
      "loss": 1.0626,
      "step": 439
    },
    {
      "epoch": 0.0352,
      "grad_norm": 0.18711932003498077,
      "learning_rate": 0.00019767969062541675,
      "loss": 0.6776,
      "step": 440
    },
    {
      "epoch": 0.03528,
      "grad_norm": 0.4121774435043335,
      "learning_rate": 0.00019767435658087746,
      "loss": 0.8568,
      "step": 441
    },
    {
      "epoch": 0.03536,
      "grad_norm": 0.22506549954414368,
      "learning_rate": 0.0001976690225363382,
      "loss": 0.8907,
      "step": 442
    },
    {
      "epoch": 0.03544,
      "grad_norm": 0.3047863245010376,
      "learning_rate": 0.0001976636884917989,
      "loss": 0.9927,
      "step": 443
    },
    {
      "epoch": 0.03552,
      "grad_norm": 0.2755843997001648,
      "learning_rate": 0.00019765835444725965,
      "loss": 0.9179,
      "step": 444
    },
    {
      "epoch": 0.0356,
      "grad_norm": 0.254882276058197,
      "learning_rate": 0.00019765302040272037,
      "loss": 1.119,
      "step": 445
    },
    {
      "epoch": 0.03568,
      "grad_norm": 0.2719317078590393,
      "learning_rate": 0.0001976476863581811,
      "loss": 0.8915,
      "step": 446
    },
    {
      "epoch": 0.03576,
      "grad_norm": 0.2575340270996094,
      "learning_rate": 0.00019764235231364185,
      "loss": 1.1281,
      "step": 447
    },
    {
      "epoch": 0.03584,
      "grad_norm": 0.2957681119441986,
      "learning_rate": 0.00019763701826910256,
      "loss": 0.9199,
      "step": 448
    },
    {
      "epoch": 0.03592,
      "grad_norm": 0.23117922246456146,
      "learning_rate": 0.0001976316842245633,
      "loss": 0.98,
      "step": 449
    },
    {
      "epoch": 0.036,
      "grad_norm": 0.22972966730594635,
      "learning_rate": 0.000197626350180024,
      "loss": 0.5776,
      "step": 450
    },
    {
      "epoch": 0.03608,
      "grad_norm": 0.48990002274513245,
      "learning_rate": 0.00019762101613548475,
      "loss": 0.8305,
      "step": 451
    },
    {
      "epoch": 0.03616,
      "grad_norm": 0.24155834317207336,
      "learning_rate": 0.00019761568209094546,
      "loss": 1.3329,
      "step": 452
    },
    {
      "epoch": 0.03624,
      "grad_norm": 0.3269459009170532,
      "learning_rate": 0.0001976103480464062,
      "loss": 1.0392,
      "step": 453
    },
    {
      "epoch": 0.03632,
      "grad_norm": 0.27547070384025574,
      "learning_rate": 0.00019760501400186692,
      "loss": 1.0307,
      "step": 454
    },
    {
      "epoch": 0.0364,
      "grad_norm": 0.3333086371421814,
      "learning_rate": 0.00019759967995732766,
      "loss": 0.9511,
      "step": 455
    },
    {
      "epoch": 0.03648,
      "grad_norm": 0.43001699447631836,
      "learning_rate": 0.00019759434591278837,
      "loss": 0.8812,
      "step": 456
    },
    {
      "epoch": 0.03656,
      "grad_norm": 0.25698089599609375,
      "learning_rate": 0.0001975890118682491,
      "loss": 1.0267,
      "step": 457
    },
    {
      "epoch": 0.03664,
      "grad_norm": 0.3526802957057953,
      "learning_rate": 0.00019758367782370982,
      "loss": 0.837,
      "step": 458
    },
    {
      "epoch": 0.03672,
      "grad_norm": 0.2654472291469574,
      "learning_rate": 0.00019757834377917056,
      "loss": 0.865,
      "step": 459
    },
    {
      "epoch": 0.0368,
      "grad_norm": 0.2929026782512665,
      "learning_rate": 0.00019757300973463127,
      "loss": 0.7144,
      "step": 460
    },
    {
      "epoch": 0.03688,
      "grad_norm": 0.3361750841140747,
      "learning_rate": 0.00019756767569009201,
      "loss": 0.7686,
      "step": 461
    },
    {
      "epoch": 0.03696,
      "grad_norm": 0.3546120822429657,
      "learning_rate": 0.00019756234164555273,
      "loss": 1.1728,
      "step": 462
    },
    {
      "epoch": 0.03704,
      "grad_norm": 0.3082408905029297,
      "learning_rate": 0.00019755700760101347,
      "loss": 1.0022,
      "step": 463
    },
    {
      "epoch": 0.03712,
      "grad_norm": 0.23865625262260437,
      "learning_rate": 0.0001975516735564742,
      "loss": 0.5011,
      "step": 464
    },
    {
      "epoch": 0.0372,
      "grad_norm": 0.23607178032398224,
      "learning_rate": 0.00019754633951193492,
      "loss": 1.0282,
      "step": 465
    },
    {
      "epoch": 0.03728,
      "grad_norm": 0.3090813159942627,
      "learning_rate": 0.00019754100546739566,
      "loss": 0.4361,
      "step": 466
    },
    {
      "epoch": 0.03736,
      "grad_norm": 0.31180232763290405,
      "learning_rate": 0.00019753567142285637,
      "loss": 0.8906,
      "step": 467
    },
    {
      "epoch": 0.03744,
      "grad_norm": 0.3246537148952484,
      "learning_rate": 0.0001975303373783171,
      "loss": 0.7974,
      "step": 468
    },
    {
      "epoch": 0.03752,
      "grad_norm": 0.2964678406715393,
      "learning_rate": 0.00019752500333377782,
      "loss": 1.1547,
      "step": 469
    },
    {
      "epoch": 0.0376,
      "grad_norm": 0.3610510230064392,
      "learning_rate": 0.00019751966928923856,
      "loss": 0.8944,
      "step": 470
    },
    {
      "epoch": 0.03768,
      "grad_norm": 0.24892845749855042,
      "learning_rate": 0.0001975143352446993,
      "loss": 0.7746,
      "step": 471
    },
    {
      "epoch": 0.03776,
      "grad_norm": 0.32378295063972473,
      "learning_rate": 0.00019750900120016002,
      "loss": 1.0768,
      "step": 472
    },
    {
      "epoch": 0.03784,
      "grad_norm": 0.2741827368736267,
      "learning_rate": 0.00019750366715562076,
      "loss": 0.7214,
      "step": 473
    },
    {
      "epoch": 0.03792,
      "grad_norm": 0.30976131558418274,
      "learning_rate": 0.00019749833311108147,
      "loss": 0.6402,
      "step": 474
    },
    {
      "epoch": 0.038,
      "grad_norm": 0.36087778210639954,
      "learning_rate": 0.0001974929990665422,
      "loss": 0.8834,
      "step": 475
    },
    {
      "epoch": 0.03808,
      "grad_norm": 0.3343086242675781,
      "learning_rate": 0.00019748766502200292,
      "loss": 0.9125,
      "step": 476
    },
    {
      "epoch": 0.03816,
      "grad_norm": 0.29101139307022095,
      "learning_rate": 0.00019748233097746366,
      "loss": 0.9937,
      "step": 477
    },
    {
      "epoch": 0.03824,
      "grad_norm": 0.29496699571609497,
      "learning_rate": 0.0001974769969329244,
      "loss": 0.9533,
      "step": 478
    },
    {
      "epoch": 0.03832,
      "grad_norm": 0.2818840742111206,
      "learning_rate": 0.00019747166288838511,
      "loss": 1.0009,
      "step": 479
    },
    {
      "epoch": 0.0384,
      "grad_norm": 0.31838950514793396,
      "learning_rate": 0.00019746632884384585,
      "loss": 0.8058,
      "step": 480
    },
    {
      "epoch": 0.03848,
      "grad_norm": 0.24947649240493774,
      "learning_rate": 0.00019746099479930657,
      "loss": 0.4815,
      "step": 481
    },
    {
      "epoch": 0.03856,
      "grad_norm": 0.2912248373031616,
      "learning_rate": 0.0001974556607547673,
      "loss": 0.7646,
      "step": 482
    },
    {
      "epoch": 0.03864,
      "grad_norm": 0.35950997471809387,
      "learning_rate": 0.00019745032671022805,
      "loss": 0.7478,
      "step": 483
    },
    {
      "epoch": 0.03872,
      "grad_norm": 0.25763046741485596,
      "learning_rate": 0.00019744499266568876,
      "loss": 1.1851,
      "step": 484
    },
    {
      "epoch": 0.0388,
      "grad_norm": 0.2889343500137329,
      "learning_rate": 0.0001974396586211495,
      "loss": 0.8789,
      "step": 485
    },
    {
      "epoch": 0.03888,
      "grad_norm": 0.32133743166923523,
      "learning_rate": 0.0001974343245766102,
      "loss": 0.8084,
      "step": 486
    },
    {
      "epoch": 0.03896,
      "grad_norm": 0.3680367171764374,
      "learning_rate": 0.00019742899053207095,
      "loss": 0.8244,
      "step": 487
    },
    {
      "epoch": 0.03904,
      "grad_norm": 0.30755552649497986,
      "learning_rate": 0.00019742365648753166,
      "loss": 0.6749,
      "step": 488
    },
    {
      "epoch": 0.03912,
      "grad_norm": 0.2877115309238434,
      "learning_rate": 0.0001974183224429924,
      "loss": 0.4071,
      "step": 489
    },
    {
      "epoch": 0.0392,
      "grad_norm": 0.3683134913444519,
      "learning_rate": 0.00019741298839845314,
      "loss": 0.8964,
      "step": 490
    },
    {
      "epoch": 0.03928,
      "grad_norm": 0.42059043049812317,
      "learning_rate": 0.00019740765435391386,
      "loss": 0.9995,
      "step": 491
    },
    {
      "epoch": 0.03936,
      "grad_norm": 0.30903756618499756,
      "learning_rate": 0.0001974023203093746,
      "loss": 0.7493,
      "step": 492
    },
    {
      "epoch": 0.03944,
      "grad_norm": 0.3915401101112366,
      "learning_rate": 0.0001973969862648353,
      "loss": 0.9997,
      "step": 493
    },
    {
      "epoch": 0.03952,
      "grad_norm": 0.23639646172523499,
      "learning_rate": 0.00019739165222029605,
      "loss": 0.8902,
      "step": 494
    },
    {
      "epoch": 0.0396,
      "grad_norm": 0.22973957657814026,
      "learning_rate": 0.00019738631817575676,
      "loss": 0.5291,
      "step": 495
    },
    {
      "epoch": 0.03968,
      "grad_norm": 0.30967217683792114,
      "learning_rate": 0.0001973809841312175,
      "loss": 0.757,
      "step": 496
    },
    {
      "epoch": 0.03976,
      "grad_norm": 0.3431355953216553,
      "learning_rate": 0.00019737565008667824,
      "loss": 0.684,
      "step": 497
    },
    {
      "epoch": 0.03984,
      "grad_norm": 0.28261280059814453,
      "learning_rate": 0.00019737031604213895,
      "loss": 0.6367,
      "step": 498
    },
    {
      "epoch": 0.03992,
      "grad_norm": 0.3424554169178009,
      "learning_rate": 0.0001973649819975997,
      "loss": 0.7862,
      "step": 499
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.24605348706245422,
      "learning_rate": 0.0001973596479530604,
      "loss": 0.9606,
      "step": 500
    },
    {
      "epoch": 0.04008,
      "grad_norm": 0.28781041502952576,
      "learning_rate": 0.00019735431390852115,
      "loss": 0.6971,
      "step": 501
    },
    {
      "epoch": 0.04016,
      "grad_norm": 0.31988728046417236,
      "learning_rate": 0.00019734897986398186,
      "loss": 0.8463,
      "step": 502
    },
    {
      "epoch": 0.04024,
      "grad_norm": 0.27468341588974,
      "learning_rate": 0.0001973436458194426,
      "loss": 0.839,
      "step": 503
    },
    {
      "epoch": 0.04032,
      "grad_norm": 0.3678983747959137,
      "learning_rate": 0.00019733831177490334,
      "loss": 0.5082,
      "step": 504
    },
    {
      "epoch": 0.0404,
      "grad_norm": 0.25767311453819275,
      "learning_rate": 0.00019733297773036405,
      "loss": 0.8561,
      "step": 505
    },
    {
      "epoch": 0.04048,
      "grad_norm": 0.2551330029964447,
      "learning_rate": 0.0001973276436858248,
      "loss": 0.5825,
      "step": 506
    },
    {
      "epoch": 0.04056,
      "grad_norm": 0.2957054376602173,
      "learning_rate": 0.0001973223096412855,
      "loss": 0.7333,
      "step": 507
    },
    {
      "epoch": 0.04064,
      "grad_norm": 0.2753981649875641,
      "learning_rate": 0.00019731697559674624,
      "loss": 0.4813,
      "step": 508
    },
    {
      "epoch": 0.04072,
      "grad_norm": 0.31519803404808044,
      "learning_rate": 0.00019731164155220696,
      "loss": 0.8355,
      "step": 509
    },
    {
      "epoch": 0.0408,
      "grad_norm": 0.3027007579803467,
      "learning_rate": 0.0001973063075076677,
      "loss": 0.8977,
      "step": 510
    },
    {
      "epoch": 0.04088,
      "grad_norm": 0.35191917419433594,
      "learning_rate": 0.00019730097346312844,
      "loss": 1.1354,
      "step": 511
    },
    {
      "epoch": 0.04096,
      "grad_norm": 0.29042693972587585,
      "learning_rate": 0.00019729563941858915,
      "loss": 1.0481,
      "step": 512
    },
    {
      "epoch": 0.04104,
      "grad_norm": 0.38341936469078064,
      "learning_rate": 0.0001972903053740499,
      "loss": 0.6948,
      "step": 513
    },
    {
      "epoch": 0.04112,
      "grad_norm": 0.25207987427711487,
      "learning_rate": 0.0001972849713295106,
      "loss": 1.0508,
      "step": 514
    },
    {
      "epoch": 0.0412,
      "grad_norm": 0.23358957469463348,
      "learning_rate": 0.00019727963728497134,
      "loss": 0.5321,
      "step": 515
    },
    {
      "epoch": 0.04128,
      "grad_norm": 0.2724974453449249,
      "learning_rate": 0.00019727430324043205,
      "loss": 0.9436,
      "step": 516
    },
    {
      "epoch": 0.04136,
      "grad_norm": 0.2774757146835327,
      "learning_rate": 0.0001972689691958928,
      "loss": 0.5884,
      "step": 517
    },
    {
      "epoch": 0.04144,
      "grad_norm": 0.31556132435798645,
      "learning_rate": 0.00019726363515135353,
      "loss": 0.6529,
      "step": 518
    },
    {
      "epoch": 0.04152,
      "grad_norm": 0.357601523399353,
      "learning_rate": 0.00019725830110681425,
      "loss": 0.7542,
      "step": 519
    },
    {
      "epoch": 0.0416,
      "grad_norm": 0.33256039023399353,
      "learning_rate": 0.000197252967062275,
      "loss": 0.8658,
      "step": 520
    },
    {
      "epoch": 0.04168,
      "grad_norm": 0.330263614654541,
      "learning_rate": 0.0001972476330177357,
      "loss": 0.8211,
      "step": 521
    },
    {
      "epoch": 0.04176,
      "grad_norm": 0.3719229996204376,
      "learning_rate": 0.00019724229897319644,
      "loss": 1.0073,
      "step": 522
    },
    {
      "epoch": 0.04184,
      "grad_norm": 0.3867260217666626,
      "learning_rate": 0.00019723696492865715,
      "loss": 0.8914,
      "step": 523
    },
    {
      "epoch": 0.04192,
      "grad_norm": 0.29562127590179443,
      "learning_rate": 0.0001972316308841179,
      "loss": 0.776,
      "step": 524
    },
    {
      "epoch": 0.042,
      "grad_norm": 0.24899952113628387,
      "learning_rate": 0.00019722629683957863,
      "loss": 0.8266,
      "step": 525
    },
    {
      "epoch": 0.04208,
      "grad_norm": 0.29684561491012573,
      "learning_rate": 0.00019722096279503934,
      "loss": 1.0559,
      "step": 526
    },
    {
      "epoch": 0.04216,
      "grad_norm": 0.34812796115875244,
      "learning_rate": 0.00019721562875050008,
      "loss": 0.739,
      "step": 527
    },
    {
      "epoch": 0.04224,
      "grad_norm": 0.23414246737957,
      "learning_rate": 0.0001972102947059608,
      "loss": 1.077,
      "step": 528
    },
    {
      "epoch": 0.04232,
      "grad_norm": 0.26992571353912354,
      "learning_rate": 0.00019720496066142154,
      "loss": 1.3107,
      "step": 529
    },
    {
      "epoch": 0.0424,
      "grad_norm": 0.26353690028190613,
      "learning_rate": 0.00019719962661688228,
      "loss": 0.6091,
      "step": 530
    },
    {
      "epoch": 0.04248,
      "grad_norm": 0.34311771392822266,
      "learning_rate": 0.000197194292572343,
      "loss": 0.7903,
      "step": 531
    },
    {
      "epoch": 0.04256,
      "grad_norm": 0.296586811542511,
      "learning_rate": 0.00019718895852780373,
      "loss": 1.0365,
      "step": 532
    },
    {
      "epoch": 0.04264,
      "grad_norm": 0.5394906401634216,
      "learning_rate": 0.00019718362448326444,
      "loss": 1.0158,
      "step": 533
    },
    {
      "epoch": 0.04272,
      "grad_norm": 0.32577186822891235,
      "learning_rate": 0.00019717829043872518,
      "loss": 0.6805,
      "step": 534
    },
    {
      "epoch": 0.0428,
      "grad_norm": 0.28460922837257385,
      "learning_rate": 0.0001971729563941859,
      "loss": 0.7852,
      "step": 535
    },
    {
      "epoch": 0.04288,
      "grad_norm": 0.37260547280311584,
      "learning_rate": 0.00019716762234964663,
      "loss": 0.8643,
      "step": 536
    },
    {
      "epoch": 0.04296,
      "grad_norm": 0.2512436807155609,
      "learning_rate": 0.00019716228830510737,
      "loss": 0.8737,
      "step": 537
    },
    {
      "epoch": 0.04304,
      "grad_norm": 0.21847210824489594,
      "learning_rate": 0.0001971569542605681,
      "loss": 0.7433,
      "step": 538
    },
    {
      "epoch": 0.04312,
      "grad_norm": 0.36226218938827515,
      "learning_rate": 0.00019715162021602883,
      "loss": 0.7104,
      "step": 539
    },
    {
      "epoch": 0.0432,
      "grad_norm": 0.3574599325656891,
      "learning_rate": 0.00019714628617148954,
      "loss": 0.8507,
      "step": 540
    },
    {
      "epoch": 0.04328,
      "grad_norm": 0.30625277757644653,
      "learning_rate": 0.00019714095212695028,
      "loss": 0.7861,
      "step": 541
    },
    {
      "epoch": 0.04336,
      "grad_norm": 0.38856032490730286,
      "learning_rate": 0.000197135618082411,
      "loss": 1.1545,
      "step": 542
    },
    {
      "epoch": 0.04344,
      "grad_norm": 0.29698652029037476,
      "learning_rate": 0.00019713028403787173,
      "loss": 0.7776,
      "step": 543
    },
    {
      "epoch": 0.04352,
      "grad_norm": 0.2247639149427414,
      "learning_rate": 0.00019712494999333247,
      "loss": 0.6232,
      "step": 544
    },
    {
      "epoch": 0.0436,
      "grad_norm": 0.3448413014411926,
      "learning_rate": 0.00019711961594879319,
      "loss": 0.6605,
      "step": 545
    },
    {
      "epoch": 0.04368,
      "grad_norm": 0.23635274171829224,
      "learning_rate": 0.00019711428190425392,
      "loss": 0.7945,
      "step": 546
    },
    {
      "epoch": 0.04376,
      "grad_norm": 0.3731609284877777,
      "learning_rate": 0.00019710894785971464,
      "loss": 1.3468,
      "step": 547
    },
    {
      "epoch": 0.04384,
      "grad_norm": 0.24474145472049713,
      "learning_rate": 0.00019710361381517538,
      "loss": 0.7617,
      "step": 548
    },
    {
      "epoch": 0.04392,
      "grad_norm": 0.35280054807662964,
      "learning_rate": 0.0001970982797706361,
      "loss": 0.9556,
      "step": 549
    },
    {
      "epoch": 0.044,
      "grad_norm": 0.3202516734600067,
      "learning_rate": 0.00019709294572609683,
      "loss": 0.8597,
      "step": 550
    },
    {
      "epoch": 0.04408,
      "grad_norm": 0.3058292865753174,
      "learning_rate": 0.00019708761168155757,
      "loss": 0.9479,
      "step": 551
    },
    {
      "epoch": 0.04416,
      "grad_norm": 0.2970427870750427,
      "learning_rate": 0.00019708227763701828,
      "loss": 0.663,
      "step": 552
    },
    {
      "epoch": 0.04424,
      "grad_norm": 0.2604283392429352,
      "learning_rate": 0.00019707694359247902,
      "loss": 0.6104,
      "step": 553
    },
    {
      "epoch": 0.04432,
      "grad_norm": 0.3162519633769989,
      "learning_rate": 0.00019707160954793974,
      "loss": 0.8049,
      "step": 554
    },
    {
      "epoch": 0.0444,
      "grad_norm": 0.27182239294052124,
      "learning_rate": 0.00019706627550340048,
      "loss": 0.8708,
      "step": 555
    },
    {
      "epoch": 0.04448,
      "grad_norm": 0.36471638083457947,
      "learning_rate": 0.0001970609414588612,
      "loss": 1.0773,
      "step": 556
    },
    {
      "epoch": 0.04456,
      "grad_norm": 0.2322208136320114,
      "learning_rate": 0.00019705560741432193,
      "loss": 1.2053,
      "step": 557
    },
    {
      "epoch": 0.04464,
      "grad_norm": 0.29728952050209045,
      "learning_rate": 0.00019705027336978267,
      "loss": 1.0047,
      "step": 558
    },
    {
      "epoch": 0.04472,
      "grad_norm": 0.4071849584579468,
      "learning_rate": 0.00019704493932524338,
      "loss": 1.0971,
      "step": 559
    },
    {
      "epoch": 0.0448,
      "grad_norm": 0.3416717052459717,
      "learning_rate": 0.00019703960528070412,
      "loss": 0.6673,
      "step": 560
    },
    {
      "epoch": 0.04488,
      "grad_norm": 0.2948468327522278,
      "learning_rate": 0.00019703427123616483,
      "loss": 0.8878,
      "step": 561
    },
    {
      "epoch": 0.04496,
      "grad_norm": 0.33485865592956543,
      "learning_rate": 0.00019702893719162557,
      "loss": 0.6406,
      "step": 562
    },
    {
      "epoch": 0.04504,
      "grad_norm": 0.2763962745666504,
      "learning_rate": 0.00019702360314708629,
      "loss": 0.9931,
      "step": 563
    },
    {
      "epoch": 0.04512,
      "grad_norm": 0.31394338607788086,
      "learning_rate": 0.00019701826910254703,
      "loss": 0.8386,
      "step": 564
    },
    {
      "epoch": 0.0452,
      "grad_norm": 0.29015034437179565,
      "learning_rate": 0.00019701293505800777,
      "loss": 1.0896,
      "step": 565
    },
    {
      "epoch": 0.04528,
      "grad_norm": 0.2527603805065155,
      "learning_rate": 0.00019700760101346848,
      "loss": 0.8322,
      "step": 566
    },
    {
      "epoch": 0.04536,
      "grad_norm": 0.29228365421295166,
      "learning_rate": 0.00019700226696892922,
      "loss": 1.041,
      "step": 567
    },
    {
      "epoch": 0.04544,
      "grad_norm": 0.33065032958984375,
      "learning_rate": 0.00019699693292438993,
      "loss": 0.7537,
      "step": 568
    },
    {
      "epoch": 0.04552,
      "grad_norm": 0.26757627725601196,
      "learning_rate": 0.00019699159887985067,
      "loss": 0.6721,
      "step": 569
    },
    {
      "epoch": 0.0456,
      "grad_norm": 0.3486080765724182,
      "learning_rate": 0.00019698626483531138,
      "loss": 0.7898,
      "step": 570
    },
    {
      "epoch": 0.04568,
      "grad_norm": 0.27689841389656067,
      "learning_rate": 0.00019698093079077212,
      "loss": 0.9196,
      "step": 571
    },
    {
      "epoch": 0.04576,
      "grad_norm": 0.2031225711107254,
      "learning_rate": 0.00019697559674623284,
      "loss": 1.0608,
      "step": 572
    },
    {
      "epoch": 0.04584,
      "grad_norm": 0.2612805664539337,
      "learning_rate": 0.00019697026270169358,
      "loss": 0.6559,
      "step": 573
    },
    {
      "epoch": 0.04592,
      "grad_norm": 0.3636859655380249,
      "learning_rate": 0.0001969649286571543,
      "loss": 1.2199,
      "step": 574
    },
    {
      "epoch": 0.046,
      "grad_norm": 0.31121590733528137,
      "learning_rate": 0.00019695959461261503,
      "loss": 0.8328,
      "step": 575
    },
    {
      "epoch": 0.04608,
      "grad_norm": 0.2918001413345337,
      "learning_rate": 0.00019695426056807577,
      "loss": 0.6481,
      "step": 576
    },
    {
      "epoch": 0.04616,
      "grad_norm": 0.2725202441215515,
      "learning_rate": 0.00019694892652353648,
      "loss": 0.7049,
      "step": 577
    },
    {
      "epoch": 0.04624,
      "grad_norm": 0.3689356744289398,
      "learning_rate": 0.00019694359247899722,
      "loss": 0.7471,
      "step": 578
    },
    {
      "epoch": 0.04632,
      "grad_norm": 0.2742185592651367,
      "learning_rate": 0.00019693825843445793,
      "loss": 0.8096,
      "step": 579
    },
    {
      "epoch": 0.0464,
      "grad_norm": 0.2075733095407486,
      "learning_rate": 0.00019693292438991867,
      "loss": 0.8941,
      "step": 580
    },
    {
      "epoch": 0.04648,
      "grad_norm": 0.2955668270587921,
      "learning_rate": 0.00019692759034537939,
      "loss": 0.7096,
      "step": 581
    },
    {
      "epoch": 0.04656,
      "grad_norm": 0.26097673177719116,
      "learning_rate": 0.00019692225630084013,
      "loss": 0.728,
      "step": 582
    },
    {
      "epoch": 0.04664,
      "grad_norm": 0.3476664423942566,
      "learning_rate": 0.00019691692225630084,
      "loss": 0.8772,
      "step": 583
    },
    {
      "epoch": 0.04672,
      "grad_norm": 0.3249650299549103,
      "learning_rate": 0.00019691158821176158,
      "loss": 1.0471,
      "step": 584
    },
    {
      "epoch": 0.0468,
      "grad_norm": 0.26385802030563354,
      "learning_rate": 0.0001969062541672223,
      "loss": 0.7907,
      "step": 585
    },
    {
      "epoch": 0.04688,
      "grad_norm": 0.3069598376750946,
      "learning_rate": 0.00019690092012268303,
      "loss": 0.7554,
      "step": 586
    },
    {
      "epoch": 0.04696,
      "grad_norm": 0.30665990710258484,
      "learning_rate": 0.00019689558607814374,
      "loss": 0.7254,
      "step": 587
    },
    {
      "epoch": 0.04704,
      "grad_norm": 0.2612343728542328,
      "learning_rate": 0.00019689025203360448,
      "loss": 0.8933,
      "step": 588
    },
    {
      "epoch": 0.04712,
      "grad_norm": 0.4609564542770386,
      "learning_rate": 0.0001968849179890652,
      "loss": 0.9327,
      "step": 589
    },
    {
      "epoch": 0.0472,
      "grad_norm": 0.2814216613769531,
      "learning_rate": 0.00019687958394452594,
      "loss": 0.7072,
      "step": 590
    },
    {
      "epoch": 0.04728,
      "grad_norm": 0.27699583768844604,
      "learning_rate": 0.00019687424989998668,
      "loss": 0.8312,
      "step": 591
    },
    {
      "epoch": 0.04736,
      "grad_norm": 0.31404751539230347,
      "learning_rate": 0.0001968689158554474,
      "loss": 0.7652,
      "step": 592
    },
    {
      "epoch": 0.04744,
      "grad_norm": 0.3613555431365967,
      "learning_rate": 0.00019686358181090813,
      "loss": 1.0202,
      "step": 593
    },
    {
      "epoch": 0.04752,
      "grad_norm": 0.3347380757331848,
      "learning_rate": 0.00019685824776636884,
      "loss": 0.8238,
      "step": 594
    },
    {
      "epoch": 0.0476,
      "grad_norm": 0.35948652029037476,
      "learning_rate": 0.00019685291372182958,
      "loss": 1.0071,
      "step": 595
    },
    {
      "epoch": 0.04768,
      "grad_norm": 0.30835679173469543,
      "learning_rate": 0.0001968475796772903,
      "loss": 0.7584,
      "step": 596
    },
    {
      "epoch": 0.04776,
      "grad_norm": 0.2614758610725403,
      "learning_rate": 0.00019684224563275103,
      "loss": 0.7429,
      "step": 597
    },
    {
      "epoch": 0.04784,
      "grad_norm": 0.2575879395008087,
      "learning_rate": 0.00019683691158821177,
      "loss": 0.9067,
      "step": 598
    },
    {
      "epoch": 0.04792,
      "grad_norm": 0.27078908681869507,
      "learning_rate": 0.00019683157754367249,
      "loss": 1.1036,
      "step": 599
    },
    {
      "epoch": 0.048,
      "grad_norm": 0.2716905176639557,
      "learning_rate": 0.00019682624349913323,
      "loss": 1.0187,
      "step": 600
    },
    {
      "epoch": 0.04808,
      "grad_norm": 0.2787139117717743,
      "learning_rate": 0.00019682090945459394,
      "loss": 0.6628,
      "step": 601
    },
    {
      "epoch": 0.04816,
      "grad_norm": 0.2522819936275482,
      "learning_rate": 0.00019681557541005468,
      "loss": 0.9324,
      "step": 602
    },
    {
      "epoch": 0.04824,
      "grad_norm": 0.2623967230319977,
      "learning_rate": 0.0001968102413655154,
      "loss": 0.6203,
      "step": 603
    },
    {
      "epoch": 0.04832,
      "grad_norm": 0.21083903312683105,
      "learning_rate": 0.00019680490732097613,
      "loss": 0.6923,
      "step": 604
    },
    {
      "epoch": 0.0484,
      "grad_norm": 0.281755656003952,
      "learning_rate": 0.00019679957327643687,
      "loss": 0.8787,
      "step": 605
    },
    {
      "epoch": 0.04848,
      "grad_norm": 0.30948686599731445,
      "learning_rate": 0.00019679423923189758,
      "loss": 1.0202,
      "step": 606
    },
    {
      "epoch": 0.04856,
      "grad_norm": 0.34862855076789856,
      "learning_rate": 0.00019678890518735832,
      "loss": 1.2335,
      "step": 607
    },
    {
      "epoch": 0.04864,
      "grad_norm": 0.2933863401412964,
      "learning_rate": 0.00019678357114281904,
      "loss": 0.7599,
      "step": 608
    },
    {
      "epoch": 0.04872,
      "grad_norm": 0.3411198556423187,
      "learning_rate": 0.00019677823709827978,
      "loss": 0.8549,
      "step": 609
    },
    {
      "epoch": 0.0488,
      "grad_norm": 0.2654409110546112,
      "learning_rate": 0.0001967729030537405,
      "loss": 0.9722,
      "step": 610
    },
    {
      "epoch": 0.04888,
      "grad_norm": 0.26622340083122253,
      "learning_rate": 0.00019676756900920123,
      "loss": 0.9003,
      "step": 611
    },
    {
      "epoch": 0.04896,
      "grad_norm": 0.3293682038784027,
      "learning_rate": 0.00019676223496466197,
      "loss": 0.7911,
      "step": 612
    },
    {
      "epoch": 0.04904,
      "grad_norm": 0.28717517852783203,
      "learning_rate": 0.00019675690092012268,
      "loss": 0.7257,
      "step": 613
    },
    {
      "epoch": 0.04912,
      "grad_norm": 0.3388684093952179,
      "learning_rate": 0.00019675156687558342,
      "loss": 1.0634,
      "step": 614
    },
    {
      "epoch": 0.0492,
      "grad_norm": 0.3115254044532776,
      "learning_rate": 0.00019674623283104413,
      "loss": 0.7756,
      "step": 615
    },
    {
      "epoch": 0.04928,
      "grad_norm": 0.25061407685279846,
      "learning_rate": 0.00019674089878650487,
      "loss": 0.6834,
      "step": 616
    },
    {
      "epoch": 0.04936,
      "grad_norm": 0.32733049988746643,
      "learning_rate": 0.0001967355647419656,
      "loss": 0.6111,
      "step": 617
    },
    {
      "epoch": 0.04944,
      "grad_norm": 0.31266868114471436,
      "learning_rate": 0.00019673023069742633,
      "loss": 0.73,
      "step": 618
    },
    {
      "epoch": 0.04952,
      "grad_norm": 0.404865026473999,
      "learning_rate": 0.00019672489665288707,
      "loss": 0.8962,
      "step": 619
    },
    {
      "epoch": 0.0496,
      "grad_norm": 0.3423493802547455,
      "learning_rate": 0.00019671956260834778,
      "loss": 1.1199,
      "step": 620
    },
    {
      "epoch": 0.04968,
      "grad_norm": 0.30813971161842346,
      "learning_rate": 0.00019671422856380852,
      "loss": 0.8078,
      "step": 621
    },
    {
      "epoch": 0.04976,
      "grad_norm": 0.29591017961502075,
      "learning_rate": 0.00019670889451926923,
      "loss": 1.0411,
      "step": 622
    },
    {
      "epoch": 0.04984,
      "grad_norm": 0.39084649085998535,
      "learning_rate": 0.00019670356047472997,
      "loss": 0.89,
      "step": 623
    },
    {
      "epoch": 0.04992,
      "grad_norm": 0.3345213830471039,
      "learning_rate": 0.00019669822643019068,
      "loss": 0.9033,
      "step": 624
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.36612439155578613,
      "learning_rate": 0.00019669289238565142,
      "loss": 0.9523,
      "step": 625
    },
    {
      "epoch": 0.05008,
      "grad_norm": 0.342512309551239,
      "learning_rate": 0.00019668755834111216,
      "loss": 0.7906,
      "step": 626
    },
    {
      "epoch": 0.05016,
      "grad_norm": 0.24266785383224487,
      "learning_rate": 0.00019668222429657288,
      "loss": 0.8677,
      "step": 627
    },
    {
      "epoch": 0.05024,
      "grad_norm": 0.27264004945755005,
      "learning_rate": 0.00019667689025203362,
      "loss": 0.5171,
      "step": 628
    },
    {
      "epoch": 0.05032,
      "grad_norm": 0.3499956727027893,
      "learning_rate": 0.00019667155620749433,
      "loss": 0.9773,
      "step": 629
    },
    {
      "epoch": 0.0504,
      "grad_norm": 0.23504014313220978,
      "learning_rate": 0.00019666622216295507,
      "loss": 0.7988,
      "step": 630
    },
    {
      "epoch": 0.05048,
      "grad_norm": 0.31694820523262024,
      "learning_rate": 0.0001966608881184158,
      "loss": 0.6764,
      "step": 631
    },
    {
      "epoch": 0.05056,
      "grad_norm": 0.38639116287231445,
      "learning_rate": 0.00019665555407387652,
      "loss": 1.0554,
      "step": 632
    },
    {
      "epoch": 0.05064,
      "grad_norm": 0.3504258096218109,
      "learning_rate": 0.00019665022002933726,
      "loss": 0.9929,
      "step": 633
    },
    {
      "epoch": 0.05072,
      "grad_norm": 0.2796080410480499,
      "learning_rate": 0.00019664488598479797,
      "loss": 0.8095,
      "step": 634
    },
    {
      "epoch": 0.0508,
      "grad_norm": 0.32363608479499817,
      "learning_rate": 0.00019663955194025871,
      "loss": 0.6804,
      "step": 635
    },
    {
      "epoch": 0.05088,
      "grad_norm": 0.3470212519168854,
      "learning_rate": 0.00019663421789571943,
      "loss": 0.8592,
      "step": 636
    },
    {
      "epoch": 0.05096,
      "grad_norm": 0.3347967565059662,
      "learning_rate": 0.00019662888385118017,
      "loss": 0.8764,
      "step": 637
    },
    {
      "epoch": 0.05104,
      "grad_norm": 0.29883959889411926,
      "learning_rate": 0.0001966235498066409,
      "loss": 0.6827,
      "step": 638
    },
    {
      "epoch": 0.05112,
      "grad_norm": 0.3682984411716461,
      "learning_rate": 0.00019661821576210162,
      "loss": 0.6874,
      "step": 639
    },
    {
      "epoch": 0.0512,
      "grad_norm": 0.2635776996612549,
      "learning_rate": 0.00019661288171756236,
      "loss": 0.652,
      "step": 640
    },
    {
      "epoch": 0.05128,
      "grad_norm": 0.3137427270412445,
      "learning_rate": 0.00019660754767302307,
      "loss": 0.549,
      "step": 641
    },
    {
      "epoch": 0.05136,
      "grad_norm": 0.32648152112960815,
      "learning_rate": 0.0001966022136284838,
      "loss": 0.744,
      "step": 642
    },
    {
      "epoch": 0.05144,
      "grad_norm": 0.2811565697193146,
      "learning_rate": 0.00019659687958394452,
      "loss": 1.1212,
      "step": 643
    },
    {
      "epoch": 0.05152,
      "grad_norm": 0.34531205892562866,
      "learning_rate": 0.00019659154553940526,
      "loss": 0.9205,
      "step": 644
    },
    {
      "epoch": 0.0516,
      "grad_norm": 0.3265872895717621,
      "learning_rate": 0.000196586211494866,
      "loss": 0.8116,
      "step": 645
    },
    {
      "epoch": 0.05168,
      "grad_norm": 0.268000066280365,
      "learning_rate": 0.00019658087745032672,
      "loss": 0.7416,
      "step": 646
    },
    {
      "epoch": 0.05176,
      "grad_norm": 0.2670132517814636,
      "learning_rate": 0.00019657554340578746,
      "loss": 0.5796,
      "step": 647
    },
    {
      "epoch": 0.05184,
      "grad_norm": 0.23645614087581635,
      "learning_rate": 0.00019657020936124817,
      "loss": 0.4747,
      "step": 648
    },
    {
      "epoch": 0.05192,
      "grad_norm": 0.2422882318496704,
      "learning_rate": 0.0001965648753167089,
      "loss": 0.8545,
      "step": 649
    },
    {
      "epoch": 0.052,
      "grad_norm": 0.3244021534919739,
      "learning_rate": 0.00019655954127216962,
      "loss": 0.499,
      "step": 650
    },
    {
      "epoch": 0.05208,
      "grad_norm": 0.30075955390930176,
      "learning_rate": 0.00019655420722763036,
      "loss": 0.7818,
      "step": 651
    },
    {
      "epoch": 0.05216,
      "grad_norm": 0.344050794839859,
      "learning_rate": 0.0001965488731830911,
      "loss": 0.9372,
      "step": 652
    },
    {
      "epoch": 0.05224,
      "grad_norm": 0.3590013086795807,
      "learning_rate": 0.00019654353913855181,
      "loss": 0.7468,
      "step": 653
    },
    {
      "epoch": 0.05232,
      "grad_norm": 0.27477312088012695,
      "learning_rate": 0.00019653820509401255,
      "loss": 0.6581,
      "step": 654
    },
    {
      "epoch": 0.0524,
      "grad_norm": 0.38335222005844116,
      "learning_rate": 0.00019653287104947327,
      "loss": 1.0652,
      "step": 655
    },
    {
      "epoch": 0.05248,
      "grad_norm": 0.38096246123313904,
      "learning_rate": 0.000196527537004934,
      "loss": 0.8873,
      "step": 656
    },
    {
      "epoch": 0.05256,
      "grad_norm": 0.28139567375183105,
      "learning_rate": 0.00019652220296039472,
      "loss": 0.7323,
      "step": 657
    },
    {
      "epoch": 0.05264,
      "grad_norm": 0.22791747748851776,
      "learning_rate": 0.00019651686891585546,
      "loss": 0.5432,
      "step": 658
    },
    {
      "epoch": 0.05272,
      "grad_norm": 0.2915312349796295,
      "learning_rate": 0.0001965115348713162,
      "loss": 0.7164,
      "step": 659
    },
    {
      "epoch": 0.0528,
      "grad_norm": 0.2872900664806366,
      "learning_rate": 0.0001965062008267769,
      "loss": 0.676,
      "step": 660
    },
    {
      "epoch": 0.05288,
      "grad_norm": 0.25780779123306274,
      "learning_rate": 0.00019650086678223765,
      "loss": 0.8649,
      "step": 661
    },
    {
      "epoch": 0.05296,
      "grad_norm": 0.2387489229440689,
      "learning_rate": 0.00019649553273769836,
      "loss": 0.4996,
      "step": 662
    },
    {
      "epoch": 0.05304,
      "grad_norm": 0.3133413791656494,
      "learning_rate": 0.0001964901986931591,
      "loss": 1.1195,
      "step": 663
    },
    {
      "epoch": 0.05312,
      "grad_norm": 0.3201524615287781,
      "learning_rate": 0.00019648486464861982,
      "loss": 1.0674,
      "step": 664
    },
    {
      "epoch": 0.0532,
      "grad_norm": 0.4240352213382721,
      "learning_rate": 0.00019647953060408056,
      "loss": 0.9084,
      "step": 665
    },
    {
      "epoch": 0.05328,
      "grad_norm": 0.2354254275560379,
      "learning_rate": 0.0001964741965595413,
      "loss": 0.8983,
      "step": 666
    },
    {
      "epoch": 0.05336,
      "grad_norm": 0.3297138512134552,
      "learning_rate": 0.000196468862515002,
      "loss": 0.8512,
      "step": 667
    },
    {
      "epoch": 0.05344,
      "grad_norm": 0.3466329872608185,
      "learning_rate": 0.00019646352847046275,
      "loss": 0.9304,
      "step": 668
    },
    {
      "epoch": 0.05352,
      "grad_norm": 0.3541663885116577,
      "learning_rate": 0.00019645819442592346,
      "loss": 0.9409,
      "step": 669
    },
    {
      "epoch": 0.0536,
      "grad_norm": 0.4205477237701416,
      "learning_rate": 0.0001964528603813842,
      "loss": 0.8097,
      "step": 670
    },
    {
      "epoch": 0.05368,
      "grad_norm": 0.29895979166030884,
      "learning_rate": 0.00019644752633684492,
      "loss": 1.0057,
      "step": 671
    },
    {
      "epoch": 0.05376,
      "grad_norm": 0.37764590978622437,
      "learning_rate": 0.00019644219229230566,
      "loss": 1.0587,
      "step": 672
    },
    {
      "epoch": 0.05384,
      "grad_norm": 0.42428815364837646,
      "learning_rate": 0.0001964368582477664,
      "loss": 0.902,
      "step": 673
    },
    {
      "epoch": 0.05392,
      "grad_norm": 0.29955190420150757,
      "learning_rate": 0.0001964315242032271,
      "loss": 1.1527,
      "step": 674
    },
    {
      "epoch": 0.054,
      "grad_norm": 0.2599402070045471,
      "learning_rate": 0.00019642619015868785,
      "loss": 0.7865,
      "step": 675
    },
    {
      "epoch": 0.05408,
      "grad_norm": 0.2874826490879059,
      "learning_rate": 0.00019642085611414856,
      "loss": 1.1317,
      "step": 676
    },
    {
      "epoch": 0.05416,
      "grad_norm": 0.3282296359539032,
      "learning_rate": 0.0001964155220696093,
      "loss": 0.6673,
      "step": 677
    },
    {
      "epoch": 0.05424,
      "grad_norm": 0.35058727860450745,
      "learning_rate": 0.00019641018802507,
      "loss": 0.9236,
      "step": 678
    },
    {
      "epoch": 0.05432,
      "grad_norm": 0.234769806265831,
      "learning_rate": 0.00019640485398053075,
      "loss": 0.9626,
      "step": 679
    },
    {
      "epoch": 0.0544,
      "grad_norm": 0.253185510635376,
      "learning_rate": 0.0001963995199359915,
      "loss": 1.0395,
      "step": 680
    },
    {
      "epoch": 0.05448,
      "grad_norm": 0.36443883180618286,
      "learning_rate": 0.0001963941858914522,
      "loss": 0.8542,
      "step": 681
    },
    {
      "epoch": 0.05456,
      "grad_norm": 0.3381544351577759,
      "learning_rate": 0.00019638885184691295,
      "loss": 0.908,
      "step": 682
    },
    {
      "epoch": 0.05464,
      "grad_norm": 0.29771023988723755,
      "learning_rate": 0.00019638351780237366,
      "loss": 1.0117,
      "step": 683
    },
    {
      "epoch": 0.05472,
      "grad_norm": 0.33148738741874695,
      "learning_rate": 0.0001963781837578344,
      "loss": 0.805,
      "step": 684
    },
    {
      "epoch": 0.0548,
      "grad_norm": 0.3282971978187561,
      "learning_rate": 0.00019637284971329514,
      "loss": 0.7075,
      "step": 685
    },
    {
      "epoch": 0.05488,
      "grad_norm": 0.3365118205547333,
      "learning_rate": 0.00019636751566875585,
      "loss": 0.7457,
      "step": 686
    },
    {
      "epoch": 0.05496,
      "grad_norm": 0.3566480576992035,
      "learning_rate": 0.0001963621816242166,
      "loss": 1.071,
      "step": 687
    },
    {
      "epoch": 0.05504,
      "grad_norm": 0.363472044467926,
      "learning_rate": 0.0001963568475796773,
      "loss": 0.8863,
      "step": 688
    },
    {
      "epoch": 0.05512,
      "grad_norm": 0.41401708126068115,
      "learning_rate": 0.00019635151353513804,
      "loss": 0.887,
      "step": 689
    },
    {
      "epoch": 0.0552,
      "grad_norm": 0.32454437017440796,
      "learning_rate": 0.00019634617949059876,
      "loss": 1.0894,
      "step": 690
    },
    {
      "epoch": 0.05528,
      "grad_norm": 0.2538142204284668,
      "learning_rate": 0.0001963408454460595,
      "loss": 0.5932,
      "step": 691
    },
    {
      "epoch": 0.05536,
      "grad_norm": 0.259598970413208,
      "learning_rate": 0.00019633551140152024,
      "loss": 1.0739,
      "step": 692
    },
    {
      "epoch": 0.05544,
      "grad_norm": 0.3188175559043884,
      "learning_rate": 0.00019633017735698095,
      "loss": 0.8419,
      "step": 693
    },
    {
      "epoch": 0.05552,
      "grad_norm": 0.2511884868144989,
      "learning_rate": 0.0001963248433124417,
      "loss": 1.1247,
      "step": 694
    },
    {
      "epoch": 0.0556,
      "grad_norm": 0.3788547217845917,
      "learning_rate": 0.0001963195092679024,
      "loss": 0.7099,
      "step": 695
    },
    {
      "epoch": 0.05568,
      "grad_norm": 0.30247214436531067,
      "learning_rate": 0.00019631417522336314,
      "loss": 0.9934,
      "step": 696
    },
    {
      "epoch": 0.05576,
      "grad_norm": 0.27575480937957764,
      "learning_rate": 0.00019630884117882385,
      "loss": 0.7228,
      "step": 697
    },
    {
      "epoch": 0.05584,
      "grad_norm": 0.3215397000312805,
      "learning_rate": 0.0001963035071342846,
      "loss": 0.8971,
      "step": 698
    },
    {
      "epoch": 0.05592,
      "grad_norm": 0.29963982105255127,
      "learning_rate": 0.0001962981730897453,
      "loss": 0.8306,
      "step": 699
    },
    {
      "epoch": 0.056,
      "grad_norm": 0.2506188750267029,
      "learning_rate": 0.00019629283904520605,
      "loss": 0.5743,
      "step": 700
    },
    {
      "epoch": 0.05608,
      "grad_norm": 0.3051234185695648,
      "learning_rate": 0.00019628750500066676,
      "loss": 0.7013,
      "step": 701
    },
    {
      "epoch": 0.05616,
      "grad_norm": 0.24739250540733337,
      "learning_rate": 0.0001962821709561275,
      "loss": 1.3158,
      "step": 702
    },
    {
      "epoch": 0.05624,
      "grad_norm": 0.2966427505016327,
      "learning_rate": 0.00019627683691158824,
      "loss": 0.6081,
      "step": 703
    },
    {
      "epoch": 0.05632,
      "grad_norm": 0.2534819543361664,
      "learning_rate": 0.00019627150286704895,
      "loss": 0.8044,
      "step": 704
    },
    {
      "epoch": 0.0564,
      "grad_norm": 0.3123810291290283,
      "learning_rate": 0.0001962661688225097,
      "loss": 0.8254,
      "step": 705
    },
    {
      "epoch": 0.05648,
      "grad_norm": 0.3019467890262604,
      "learning_rate": 0.0001962608347779704,
      "loss": 0.8441,
      "step": 706
    },
    {
      "epoch": 0.05656,
      "grad_norm": 0.29894426465034485,
      "learning_rate": 0.00019625550073343114,
      "loss": 0.8572,
      "step": 707
    },
    {
      "epoch": 0.05664,
      "grad_norm": 0.31297174096107483,
      "learning_rate": 0.00019625016668889186,
      "loss": 0.8558,
      "step": 708
    },
    {
      "epoch": 0.05672,
      "grad_norm": 0.3475687503814697,
      "learning_rate": 0.0001962448326443526,
      "loss": 0.8589,
      "step": 709
    },
    {
      "epoch": 0.0568,
      "grad_norm": 0.24501380324363708,
      "learning_rate": 0.0001962394985998133,
      "loss": 0.9379,
      "step": 710
    },
    {
      "epoch": 0.05688,
      "grad_norm": 0.29038622975349426,
      "learning_rate": 0.00019623416455527405,
      "loss": 0.9664,
      "step": 711
    },
    {
      "epoch": 0.05696,
      "grad_norm": 0.36430105566978455,
      "learning_rate": 0.00019622883051073476,
      "loss": 0.9429,
      "step": 712
    },
    {
      "epoch": 0.05704,
      "grad_norm": 0.3331104815006256,
      "learning_rate": 0.0001962234964661955,
      "loss": 0.8326,
      "step": 713
    },
    {
      "epoch": 0.05712,
      "grad_norm": 0.29207634925842285,
      "learning_rate": 0.00019621816242165621,
      "loss": 1.0255,
      "step": 714
    },
    {
      "epoch": 0.0572,
      "grad_norm": 0.3125233054161072,
      "learning_rate": 0.00019621282837711695,
      "loss": 0.8211,
      "step": 715
    },
    {
      "epoch": 0.05728,
      "grad_norm": 0.3164995014667511,
      "learning_rate": 0.00019620749433257767,
      "loss": 0.8452,
      "step": 716
    },
    {
      "epoch": 0.05736,
      "grad_norm": 0.3931881785392761,
      "learning_rate": 0.0001962021602880384,
      "loss": 0.9233,
      "step": 717
    },
    {
      "epoch": 0.05744,
      "grad_norm": 0.22460941970348358,
      "learning_rate": 0.00019619682624349912,
      "loss": 0.5734,
      "step": 718
    },
    {
      "epoch": 0.05752,
      "grad_norm": 0.24878089129924774,
      "learning_rate": 0.00019619149219895986,
      "loss": 0.7654,
      "step": 719
    },
    {
      "epoch": 0.0576,
      "grad_norm": 0.33687591552734375,
      "learning_rate": 0.0001961861581544206,
      "loss": 0.6451,
      "step": 720
    },
    {
      "epoch": 0.05768,
      "grad_norm": 0.22628013789653778,
      "learning_rate": 0.0001961808241098813,
      "loss": 1.0117,
      "step": 721
    },
    {
      "epoch": 0.05776,
      "grad_norm": 0.29224610328674316,
      "learning_rate": 0.00019617549006534205,
      "loss": 0.7259,
      "step": 722
    },
    {
      "epoch": 0.05784,
      "grad_norm": 0.3672257959842682,
      "learning_rate": 0.00019617015602080276,
      "loss": 0.8697,
      "step": 723
    },
    {
      "epoch": 0.05792,
      "grad_norm": 0.27227887511253357,
      "learning_rate": 0.0001961648219762635,
      "loss": 1.1392,
      "step": 724
    },
    {
      "epoch": 0.058,
      "grad_norm": 0.31514695286750793,
      "learning_rate": 0.00019615948793172422,
      "loss": 0.8968,
      "step": 725
    },
    {
      "epoch": 0.05808,
      "grad_norm": 0.4255938231945038,
      "learning_rate": 0.00019615415388718496,
      "loss": 0.8058,
      "step": 726
    },
    {
      "epoch": 0.05816,
      "grad_norm": 0.321512371301651,
      "learning_rate": 0.0001961488198426457,
      "loss": 0.8524,
      "step": 727
    },
    {
      "epoch": 0.05824,
      "grad_norm": 0.2760055363178253,
      "learning_rate": 0.0001961434857981064,
      "loss": 1.0572,
      "step": 728
    },
    {
      "epoch": 0.05832,
      "grad_norm": 0.4311108887195587,
      "learning_rate": 0.00019613815175356715,
      "loss": 1.0085,
      "step": 729
    },
    {
      "epoch": 0.0584,
      "grad_norm": 0.27798283100128174,
      "learning_rate": 0.00019613281770902786,
      "loss": 0.7491,
      "step": 730
    },
    {
      "epoch": 0.05848,
      "grad_norm": 0.31613776087760925,
      "learning_rate": 0.0001961274836644886,
      "loss": 0.907,
      "step": 731
    },
    {
      "epoch": 0.05856,
      "grad_norm": 0.3530713617801666,
      "learning_rate": 0.00019612214961994934,
      "loss": 0.7324,
      "step": 732
    },
    {
      "epoch": 0.05864,
      "grad_norm": 0.29115137457847595,
      "learning_rate": 0.00019611681557541005,
      "loss": 1.2366,
      "step": 733
    },
    {
      "epoch": 0.05872,
      "grad_norm": 0.37569600343704224,
      "learning_rate": 0.0001961114815308708,
      "loss": 0.9017,
      "step": 734
    },
    {
      "epoch": 0.0588,
      "grad_norm": 0.3296580910682678,
      "learning_rate": 0.0001961061474863315,
      "loss": 1.1171,
      "step": 735
    },
    {
      "epoch": 0.05888,
      "grad_norm": 0.41516536474227905,
      "learning_rate": 0.00019610081344179225,
      "loss": 0.8714,
      "step": 736
    },
    {
      "epoch": 0.05896,
      "grad_norm": 0.32866060733795166,
      "learning_rate": 0.00019609547939725296,
      "loss": 0.7518,
      "step": 737
    },
    {
      "epoch": 0.05904,
      "grad_norm": 0.2749820947647095,
      "learning_rate": 0.0001960901453527137,
      "loss": 0.6199,
      "step": 738
    },
    {
      "epoch": 0.05912,
      "grad_norm": 0.330036461353302,
      "learning_rate": 0.00019608481130817444,
      "loss": 0.7492,
      "step": 739
    },
    {
      "epoch": 0.0592,
      "grad_norm": 0.4070819914340973,
      "learning_rate": 0.00019607947726363515,
      "loss": 1.0569,
      "step": 740
    },
    {
      "epoch": 0.05928,
      "grad_norm": 0.4159715175628662,
      "learning_rate": 0.0001960741432190959,
      "loss": 1.2307,
      "step": 741
    },
    {
      "epoch": 0.05936,
      "grad_norm": 0.32992953062057495,
      "learning_rate": 0.0001960688091745566,
      "loss": 0.8008,
      "step": 742
    },
    {
      "epoch": 0.05944,
      "grad_norm": 0.31594496965408325,
      "learning_rate": 0.00019606347513001734,
      "loss": 0.6936,
      "step": 743
    },
    {
      "epoch": 0.05952,
      "grad_norm": 0.30725812911987305,
      "learning_rate": 0.00019605814108547806,
      "loss": 1.1718,
      "step": 744
    },
    {
      "epoch": 0.0596,
      "grad_norm": 0.2678476572036743,
      "learning_rate": 0.0001960528070409388,
      "loss": 0.7862,
      "step": 745
    },
    {
      "epoch": 0.05968,
      "grad_norm": 0.24479976296424866,
      "learning_rate": 0.00019604747299639954,
      "loss": 1.1429,
      "step": 746
    },
    {
      "epoch": 0.05976,
      "grad_norm": 0.29928427934646606,
      "learning_rate": 0.00019604213895186025,
      "loss": 0.9732,
      "step": 747
    },
    {
      "epoch": 0.05984,
      "grad_norm": 0.25021693110466003,
      "learning_rate": 0.000196036804907321,
      "loss": 0.7059,
      "step": 748
    },
    {
      "epoch": 0.05992,
      "grad_norm": 0.23818005621433258,
      "learning_rate": 0.0001960314708627817,
      "loss": 0.7056,
      "step": 749
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.3053324818611145,
      "learning_rate": 0.00019602613681824244,
      "loss": 0.6551,
      "step": 750
    },
    {
      "epoch": 0.06008,
      "grad_norm": 0.34546205401420593,
      "learning_rate": 0.00019602080277370315,
      "loss": 1.025,
      "step": 751
    },
    {
      "epoch": 0.06016,
      "grad_norm": 0.2370128035545349,
      "learning_rate": 0.0001960154687291639,
      "loss": 0.8461,
      "step": 752
    },
    {
      "epoch": 0.06024,
      "grad_norm": 0.3579919636249542,
      "learning_rate": 0.00019601013468462463,
      "loss": 0.8807,
      "step": 753
    },
    {
      "epoch": 0.06032,
      "grad_norm": 0.28689342737197876,
      "learning_rate": 0.00019600480064008535,
      "loss": 0.5812,
      "step": 754
    },
    {
      "epoch": 0.0604,
      "grad_norm": 0.32603153586387634,
      "learning_rate": 0.0001959994665955461,
      "loss": 0.8551,
      "step": 755
    },
    {
      "epoch": 0.06048,
      "grad_norm": 0.29457902908325195,
      "learning_rate": 0.0001959941325510068,
      "loss": 0.7824,
      "step": 756
    },
    {
      "epoch": 0.06056,
      "grad_norm": 0.3565192222595215,
      "learning_rate": 0.00019598879850646754,
      "loss": 0.9034,
      "step": 757
    },
    {
      "epoch": 0.06064,
      "grad_norm": 0.2847635746002197,
      "learning_rate": 0.00019598346446192825,
      "loss": 0.6552,
      "step": 758
    },
    {
      "epoch": 0.06072,
      "grad_norm": 0.3421037197113037,
      "learning_rate": 0.000195978130417389,
      "loss": 0.9128,
      "step": 759
    },
    {
      "epoch": 0.0608,
      "grad_norm": 0.3086179792881012,
      "learning_rate": 0.00019597279637284973,
      "loss": 0.5817,
      "step": 760
    },
    {
      "epoch": 0.06088,
      "grad_norm": 0.37101635336875916,
      "learning_rate": 0.00019596746232831044,
      "loss": 0.9811,
      "step": 761
    },
    {
      "epoch": 0.06096,
      "grad_norm": 0.2913303077220917,
      "learning_rate": 0.00019596212828377118,
      "loss": 0.7347,
      "step": 762
    },
    {
      "epoch": 0.06104,
      "grad_norm": 0.3977023661136627,
      "learning_rate": 0.0001959567942392319,
      "loss": 1.2106,
      "step": 763
    },
    {
      "epoch": 0.06112,
      "grad_norm": 0.35127779841423035,
      "learning_rate": 0.00019595146019469264,
      "loss": 1.0177,
      "step": 764
    },
    {
      "epoch": 0.0612,
      "grad_norm": 0.27483823895454407,
      "learning_rate": 0.00019594612615015335,
      "loss": 0.7559,
      "step": 765
    },
    {
      "epoch": 0.06128,
      "grad_norm": 0.34612056612968445,
      "learning_rate": 0.0001959407921056141,
      "loss": 0.8924,
      "step": 766
    },
    {
      "epoch": 0.06136,
      "grad_norm": 0.3602531850337982,
      "learning_rate": 0.00019593545806107483,
      "loss": 0.6786,
      "step": 767
    },
    {
      "epoch": 0.06144,
      "grad_norm": 0.3364613652229309,
      "learning_rate": 0.00019593012401653554,
      "loss": 0.9455,
      "step": 768
    },
    {
      "epoch": 0.06152,
      "grad_norm": 0.2288435697555542,
      "learning_rate": 0.00019592478997199628,
      "loss": 0.7913,
      "step": 769
    },
    {
      "epoch": 0.0616,
      "grad_norm": 0.30266833305358887,
      "learning_rate": 0.000195919455927457,
      "loss": 0.9484,
      "step": 770
    },
    {
      "epoch": 0.06168,
      "grad_norm": 0.3706401288509369,
      "learning_rate": 0.00019591412188291773,
      "loss": 1.0148,
      "step": 771
    },
    {
      "epoch": 0.06176,
      "grad_norm": 0.30403974652290344,
      "learning_rate": 0.00019590878783837845,
      "loss": 0.933,
      "step": 772
    },
    {
      "epoch": 0.06184,
      "grad_norm": 0.27373209595680237,
      "learning_rate": 0.0001959034537938392,
      "loss": 1.0803,
      "step": 773
    },
    {
      "epoch": 0.06192,
      "grad_norm": 0.38919365406036377,
      "learning_rate": 0.00019589811974929993,
      "loss": 0.7618,
      "step": 774
    },
    {
      "epoch": 0.062,
      "grad_norm": 0.36829423904418945,
      "learning_rate": 0.00019589278570476064,
      "loss": 0.8713,
      "step": 775
    },
    {
      "epoch": 0.06208,
      "grad_norm": 0.36366909742355347,
      "learning_rate": 0.00019588745166022138,
      "loss": 0.7437,
      "step": 776
    },
    {
      "epoch": 0.06216,
      "grad_norm": 0.25686705112457275,
      "learning_rate": 0.0001958821176156821,
      "loss": 0.9857,
      "step": 777
    },
    {
      "epoch": 0.06224,
      "grad_norm": 0.3209403455257416,
      "learning_rate": 0.00019587678357114283,
      "loss": 0.6553,
      "step": 778
    },
    {
      "epoch": 0.06232,
      "grad_norm": 0.23688103258609772,
      "learning_rate": 0.00019587144952660357,
      "loss": 0.7719,
      "step": 779
    },
    {
      "epoch": 0.0624,
      "grad_norm": 0.3778878450393677,
      "learning_rate": 0.00019586611548206428,
      "loss": 0.69,
      "step": 780
    },
    {
      "epoch": 0.06248,
      "grad_norm": 0.4223237931728363,
      "learning_rate": 0.00019586078143752502,
      "loss": 0.9735,
      "step": 781
    },
    {
      "epoch": 0.06256,
      "grad_norm": 0.35091766715049744,
      "learning_rate": 0.00019585544739298574,
      "loss": 1.011,
      "step": 782
    },
    {
      "epoch": 0.06264,
      "grad_norm": 0.2681853473186493,
      "learning_rate": 0.00019585011334844648,
      "loss": 1.232,
      "step": 783
    },
    {
      "epoch": 0.06272,
      "grad_norm": 0.2836828827857971,
      "learning_rate": 0.0001958447793039072,
      "loss": 0.7435,
      "step": 784
    },
    {
      "epoch": 0.0628,
      "grad_norm": 0.4024409055709839,
      "learning_rate": 0.00019583944525936793,
      "loss": 1.0904,
      "step": 785
    },
    {
      "epoch": 0.06288,
      "grad_norm": 0.30784711241722107,
      "learning_rate": 0.00019583411121482867,
      "loss": 0.5191,
      "step": 786
    },
    {
      "epoch": 0.06296,
      "grad_norm": 0.3621881306171417,
      "learning_rate": 0.00019582877717028938,
      "loss": 1.1947,
      "step": 787
    },
    {
      "epoch": 0.06304,
      "grad_norm": 0.3369022607803345,
      "learning_rate": 0.00019582344312575012,
      "loss": 0.7119,
      "step": 788
    },
    {
      "epoch": 0.06312,
      "grad_norm": 0.3109821379184723,
      "learning_rate": 0.00019581810908121083,
      "loss": 0.5182,
      "step": 789
    },
    {
      "epoch": 0.0632,
      "grad_norm": 0.413666307926178,
      "learning_rate": 0.00019581277503667157,
      "loss": 0.7872,
      "step": 790
    },
    {
      "epoch": 0.06328,
      "grad_norm": 0.3413247764110565,
      "learning_rate": 0.0001958074409921323,
      "loss": 0.7437,
      "step": 791
    },
    {
      "epoch": 0.06336,
      "grad_norm": 0.28201600909233093,
      "learning_rate": 0.00019580210694759303,
      "loss": 0.7368,
      "step": 792
    },
    {
      "epoch": 0.06344,
      "grad_norm": 0.29127612709999084,
      "learning_rate": 0.00019579677290305377,
      "loss": 0.8946,
      "step": 793
    },
    {
      "epoch": 0.06352,
      "grad_norm": 0.2710511088371277,
      "learning_rate": 0.00019579143885851448,
      "loss": 0.6595,
      "step": 794
    },
    {
      "epoch": 0.0636,
      "grad_norm": 0.35737118124961853,
      "learning_rate": 0.00019578610481397522,
      "loss": 1.1283,
      "step": 795
    },
    {
      "epoch": 0.06368,
      "grad_norm": 0.31497442722320557,
      "learning_rate": 0.00019578077076943593,
      "loss": 1.1427,
      "step": 796
    },
    {
      "epoch": 0.06376,
      "grad_norm": 0.26442116498947144,
      "learning_rate": 0.00019577543672489667,
      "loss": 0.9695,
      "step": 797
    },
    {
      "epoch": 0.06384,
      "grad_norm": 0.24405249953269958,
      "learning_rate": 0.00019577010268035739,
      "loss": 0.8895,
      "step": 798
    },
    {
      "epoch": 0.06392,
      "grad_norm": 0.3018837571144104,
      "learning_rate": 0.00019576476863581812,
      "loss": 0.8018,
      "step": 799
    },
    {
      "epoch": 0.064,
      "grad_norm": 0.3020854890346527,
      "learning_rate": 0.00019575943459127886,
      "loss": 0.9925,
      "step": 800
    },
    {
      "epoch": 0.06408,
      "grad_norm": 0.3293871283531189,
      "learning_rate": 0.00019575410054673958,
      "loss": 1.1393,
      "step": 801
    },
    {
      "epoch": 0.06416,
      "grad_norm": 0.3345782458782196,
      "learning_rate": 0.00019574876650220032,
      "loss": 0.7603,
      "step": 802
    },
    {
      "epoch": 0.06424,
      "grad_norm": 0.3227740526199341,
      "learning_rate": 0.00019574343245766103,
      "loss": 0.6923,
      "step": 803
    },
    {
      "epoch": 0.06432,
      "grad_norm": 0.35938769578933716,
      "learning_rate": 0.00019573809841312177,
      "loss": 0.8467,
      "step": 804
    },
    {
      "epoch": 0.0644,
      "grad_norm": 0.30911383032798767,
      "learning_rate": 0.00019573276436858248,
      "loss": 0.673,
      "step": 805
    },
    {
      "epoch": 0.06448,
      "grad_norm": 0.3795321583747864,
      "learning_rate": 0.00019572743032404322,
      "loss": 1.0204,
      "step": 806
    },
    {
      "epoch": 0.06456,
      "grad_norm": 0.2804672122001648,
      "learning_rate": 0.00019572209627950396,
      "loss": 0.9952,
      "step": 807
    },
    {
      "epoch": 0.06464,
      "grad_norm": 0.4376218616962433,
      "learning_rate": 0.00019571676223496468,
      "loss": 0.9255,
      "step": 808
    },
    {
      "epoch": 0.06472,
      "grad_norm": 0.2774462103843689,
      "learning_rate": 0.00019571142819042542,
      "loss": 0.7869,
      "step": 809
    },
    {
      "epoch": 0.0648,
      "grad_norm": 0.2379472404718399,
      "learning_rate": 0.00019570609414588613,
      "loss": 0.6821,
      "step": 810
    },
    {
      "epoch": 0.06488,
      "grad_norm": 0.34568294882774353,
      "learning_rate": 0.00019570076010134687,
      "loss": 0.8151,
      "step": 811
    },
    {
      "epoch": 0.06496,
      "grad_norm": 0.3315497636795044,
      "learning_rate": 0.00019569542605680758,
      "loss": 0.6803,
      "step": 812
    },
    {
      "epoch": 0.06504,
      "grad_norm": 0.3962152600288391,
      "learning_rate": 0.00019569009201226832,
      "loss": 0.9299,
      "step": 813
    },
    {
      "epoch": 0.06512,
      "grad_norm": 0.3115267753601074,
      "learning_rate": 0.00019568475796772906,
      "loss": 0.8439,
      "step": 814
    },
    {
      "epoch": 0.0652,
      "grad_norm": 0.35732224583625793,
      "learning_rate": 0.00019567942392318977,
      "loss": 1.0073,
      "step": 815
    },
    {
      "epoch": 0.06528,
      "grad_norm": 0.23309479653835297,
      "learning_rate": 0.0001956740898786505,
      "loss": 0.6339,
      "step": 816
    },
    {
      "epoch": 0.06536,
      "grad_norm": 0.29144254326820374,
      "learning_rate": 0.00019566875583411123,
      "loss": 1.1526,
      "step": 817
    },
    {
      "epoch": 0.06544,
      "grad_norm": 0.28965961933135986,
      "learning_rate": 0.00019566342178957197,
      "loss": 0.9056,
      "step": 818
    },
    {
      "epoch": 0.06552,
      "grad_norm": 0.4025059938430786,
      "learning_rate": 0.00019565808774503268,
      "loss": 1.0062,
      "step": 819
    },
    {
      "epoch": 0.0656,
      "grad_norm": 0.27754512429237366,
      "learning_rate": 0.00019565275370049342,
      "loss": 0.8222,
      "step": 820
    },
    {
      "epoch": 0.06568,
      "grad_norm": 0.28103476762771606,
      "learning_rate": 0.00019564741965595416,
      "loss": 0.7566,
      "step": 821
    },
    {
      "epoch": 0.06576,
      "grad_norm": 0.39739638566970825,
      "learning_rate": 0.00019564208561141487,
      "loss": 0.9702,
      "step": 822
    },
    {
      "epoch": 0.06584,
      "grad_norm": 0.31866490840911865,
      "learning_rate": 0.0001956367515668756,
      "loss": 0.8304,
      "step": 823
    },
    {
      "epoch": 0.06592,
      "grad_norm": 0.31655749678611755,
      "learning_rate": 0.00019563141752233632,
      "loss": 0.9219,
      "step": 824
    },
    {
      "epoch": 0.066,
      "grad_norm": 0.3910304307937622,
      "learning_rate": 0.00019562608347779706,
      "loss": 0.8302,
      "step": 825
    },
    {
      "epoch": 0.06608,
      "grad_norm": 0.3651300072669983,
      "learning_rate": 0.00019562074943325778,
      "loss": 0.7325,
      "step": 826
    },
    {
      "epoch": 0.06616,
      "grad_norm": 0.24276383221149445,
      "learning_rate": 0.00019561541538871852,
      "loss": 1.124,
      "step": 827
    },
    {
      "epoch": 0.06624,
      "grad_norm": 0.4077399671077728,
      "learning_rate": 0.00019561008134417923,
      "loss": 0.9066,
      "step": 828
    },
    {
      "epoch": 0.06632,
      "grad_norm": 0.4339742958545685,
      "learning_rate": 0.00019560474729963997,
      "loss": 0.7391,
      "step": 829
    },
    {
      "epoch": 0.0664,
      "grad_norm": 0.2902190685272217,
      "learning_rate": 0.00019559941325510068,
      "loss": 0.5785,
      "step": 830
    },
    {
      "epoch": 0.06648,
      "grad_norm": 0.32743868231773376,
      "learning_rate": 0.00019559407921056142,
      "loss": 0.6872,
      "step": 831
    },
    {
      "epoch": 0.06656,
      "grad_norm": 0.2903440296649933,
      "learning_rate": 0.00019558874516602216,
      "loss": 0.7572,
      "step": 832
    },
    {
      "epoch": 0.06664,
      "grad_norm": 0.3350597321987152,
      "learning_rate": 0.00019558341112148287,
      "loss": 0.7651,
      "step": 833
    },
    {
      "epoch": 0.06672,
      "grad_norm": 0.33639615774154663,
      "learning_rate": 0.0001955780770769436,
      "loss": 0.6824,
      "step": 834
    },
    {
      "epoch": 0.0668,
      "grad_norm": 0.3631074130535126,
      "learning_rate": 0.00019557274303240433,
      "loss": 0.5586,
      "step": 835
    },
    {
      "epoch": 0.06688,
      "grad_norm": 0.24138274788856506,
      "learning_rate": 0.00019556740898786507,
      "loss": 0.7238,
      "step": 836
    },
    {
      "epoch": 0.06696,
      "grad_norm": 0.320635050535202,
      "learning_rate": 0.00019556207494332578,
      "loss": 0.7941,
      "step": 837
    },
    {
      "epoch": 0.06704,
      "grad_norm": 0.3753408193588257,
      "learning_rate": 0.00019555674089878652,
      "loss": 0.7253,
      "step": 838
    },
    {
      "epoch": 0.06712,
      "grad_norm": 0.3586277961730957,
      "learning_rate": 0.00019555140685424723,
      "loss": 0.8192,
      "step": 839
    },
    {
      "epoch": 0.0672,
      "grad_norm": 0.3433454632759094,
      "learning_rate": 0.00019554607280970797,
      "loss": 1.0102,
      "step": 840
    },
    {
      "epoch": 0.06728,
      "grad_norm": 0.3994324803352356,
      "learning_rate": 0.00019554073876516868,
      "loss": 0.9519,
      "step": 841
    },
    {
      "epoch": 0.06736,
      "grad_norm": 0.2579585015773773,
      "learning_rate": 0.00019553540472062942,
      "loss": 1.0032,
      "step": 842
    },
    {
      "epoch": 0.06744,
      "grad_norm": 0.29216691851615906,
      "learning_rate": 0.00019553007067609014,
      "loss": 0.7893,
      "step": 843
    },
    {
      "epoch": 0.06752,
      "grad_norm": 0.32149600982666016,
      "learning_rate": 0.00019552473663155088,
      "loss": 0.746,
      "step": 844
    },
    {
      "epoch": 0.0676,
      "grad_norm": 0.3043522834777832,
      "learning_rate": 0.0001955194025870116,
      "loss": 0.7503,
      "step": 845
    },
    {
      "epoch": 0.06768,
      "grad_norm": 0.3618801534175873,
      "learning_rate": 0.00019551406854247233,
      "loss": 0.9521,
      "step": 846
    },
    {
      "epoch": 0.06776,
      "grad_norm": 0.27705857157707214,
      "learning_rate": 0.00019550873449793307,
      "loss": 0.5872,
      "step": 847
    },
    {
      "epoch": 0.06784,
      "grad_norm": 0.27931085228919983,
      "learning_rate": 0.00019550340045339378,
      "loss": 0.8197,
      "step": 848
    },
    {
      "epoch": 0.06792,
      "grad_norm": 0.3615460991859436,
      "learning_rate": 0.00019549806640885452,
      "loss": 0.8336,
      "step": 849
    },
    {
      "epoch": 0.068,
      "grad_norm": 0.32070353627204895,
      "learning_rate": 0.00019549273236431523,
      "loss": 0.7306,
      "step": 850
    },
    {
      "epoch": 0.06808,
      "grad_norm": 0.3456096351146698,
      "learning_rate": 0.00019548739831977597,
      "loss": 0.6774,
      "step": 851
    },
    {
      "epoch": 0.06816,
      "grad_norm": 0.3345910906791687,
      "learning_rate": 0.00019548206427523669,
      "loss": 0.8229,
      "step": 852
    },
    {
      "epoch": 0.06824,
      "grad_norm": 0.30895861983299255,
      "learning_rate": 0.00019547673023069743,
      "loss": 0.8383,
      "step": 853
    },
    {
      "epoch": 0.06832,
      "grad_norm": 0.3148536682128906,
      "learning_rate": 0.00019547139618615817,
      "loss": 0.8065,
      "step": 854
    },
    {
      "epoch": 0.0684,
      "grad_norm": 0.27507492899894714,
      "learning_rate": 0.00019546606214161888,
      "loss": 0.9143,
      "step": 855
    },
    {
      "epoch": 0.06848,
      "grad_norm": 0.32669392228126526,
      "learning_rate": 0.00019546072809707962,
      "loss": 0.7975,
      "step": 856
    },
    {
      "epoch": 0.06856,
      "grad_norm": 0.41732120513916016,
      "learning_rate": 0.00019545539405254033,
      "loss": 1.1031,
      "step": 857
    },
    {
      "epoch": 0.06864,
      "grad_norm": 0.3240678906440735,
      "learning_rate": 0.00019545006000800107,
      "loss": 1.0513,
      "step": 858
    },
    {
      "epoch": 0.06872,
      "grad_norm": 0.344908744096756,
      "learning_rate": 0.00019544472596346178,
      "loss": 0.7849,
      "step": 859
    },
    {
      "epoch": 0.0688,
      "grad_norm": 0.4031095504760742,
      "learning_rate": 0.00019543939191892252,
      "loss": 0.9749,
      "step": 860
    },
    {
      "epoch": 0.06888,
      "grad_norm": 0.2790876626968384,
      "learning_rate": 0.00019543405787438326,
      "loss": 0.7635,
      "step": 861
    },
    {
      "epoch": 0.06896,
      "grad_norm": 0.2807772159576416,
      "learning_rate": 0.00019542872382984398,
      "loss": 0.6767,
      "step": 862
    },
    {
      "epoch": 0.06904,
      "grad_norm": 0.3588319718837738,
      "learning_rate": 0.00019542338978530472,
      "loss": 0.9542,
      "step": 863
    },
    {
      "epoch": 0.06912,
      "grad_norm": 0.35662269592285156,
      "learning_rate": 0.00019541805574076543,
      "loss": 0.8983,
      "step": 864
    },
    {
      "epoch": 0.0692,
      "grad_norm": 0.31576478481292725,
      "learning_rate": 0.00019541272169622617,
      "loss": 0.8551,
      "step": 865
    },
    {
      "epoch": 0.06928,
      "grad_norm": 0.3962094783782959,
      "learning_rate": 0.00019540738765168688,
      "loss": 0.8524,
      "step": 866
    },
    {
      "epoch": 0.06936,
      "grad_norm": 0.34495463967323303,
      "learning_rate": 0.00019540205360714762,
      "loss": 0.9269,
      "step": 867
    },
    {
      "epoch": 0.06944,
      "grad_norm": 0.4282080829143524,
      "learning_rate": 0.00019539671956260836,
      "loss": 0.6585,
      "step": 868
    },
    {
      "epoch": 0.06952,
      "grad_norm": 0.3116380274295807,
      "learning_rate": 0.00019539138551806907,
      "loss": 0.664,
      "step": 869
    },
    {
      "epoch": 0.0696,
      "grad_norm": 0.3081386089324951,
      "learning_rate": 0.00019538605147352981,
      "loss": 0.6615,
      "step": 870
    },
    {
      "epoch": 0.06968,
      "grad_norm": 0.3606102764606476,
      "learning_rate": 0.00019538071742899053,
      "loss": 0.7844,
      "step": 871
    },
    {
      "epoch": 0.06976,
      "grad_norm": 0.3031129837036133,
      "learning_rate": 0.00019537538338445127,
      "loss": 0.7881,
      "step": 872
    },
    {
      "epoch": 0.06984,
      "grad_norm": 0.29537704586982727,
      "learning_rate": 0.00019537004933991198,
      "loss": 0.9866,
      "step": 873
    },
    {
      "epoch": 0.06992,
      "grad_norm": 0.30703669786453247,
      "learning_rate": 0.00019536471529537272,
      "loss": 0.9317,
      "step": 874
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.36557745933532715,
      "learning_rate": 0.00019535938125083346,
      "loss": 0.6753,
      "step": 875
    },
    {
      "epoch": 0.07008,
      "grad_norm": 0.26698675751686096,
      "learning_rate": 0.00019535404720629417,
      "loss": 0.7466,
      "step": 876
    },
    {
      "epoch": 0.07016,
      "grad_norm": 0.25184711813926697,
      "learning_rate": 0.0001953487131617549,
      "loss": 0.917,
      "step": 877
    },
    {
      "epoch": 0.07024,
      "grad_norm": 0.2755759358406067,
      "learning_rate": 0.00019534337911721562,
      "loss": 0.8651,
      "step": 878
    },
    {
      "epoch": 0.07032,
      "grad_norm": 0.2591889500617981,
      "learning_rate": 0.00019533804507267636,
      "loss": 0.7364,
      "step": 879
    },
    {
      "epoch": 0.0704,
      "grad_norm": 0.3010707199573517,
      "learning_rate": 0.0001953327110281371,
      "loss": 0.8713,
      "step": 880
    },
    {
      "epoch": 0.07048,
      "grad_norm": 0.35126227140426636,
      "learning_rate": 0.00019532737698359782,
      "loss": 0.9076,
      "step": 881
    },
    {
      "epoch": 0.07056,
      "grad_norm": 0.4193049371242523,
      "learning_rate": 0.00019532204293905856,
      "loss": 0.9907,
      "step": 882
    },
    {
      "epoch": 0.07064,
      "grad_norm": 0.2954830229282379,
      "learning_rate": 0.00019531670889451927,
      "loss": 0.7811,
      "step": 883
    },
    {
      "epoch": 0.07072,
      "grad_norm": 0.26906198263168335,
      "learning_rate": 0.00019531137484998,
      "loss": 0.5915,
      "step": 884
    },
    {
      "epoch": 0.0708,
      "grad_norm": 0.26460111141204834,
      "learning_rate": 0.00019530604080544072,
      "loss": 0.9852,
      "step": 885
    },
    {
      "epoch": 0.07088,
      "grad_norm": 0.2724912762641907,
      "learning_rate": 0.00019530070676090146,
      "loss": 0.5304,
      "step": 886
    },
    {
      "epoch": 0.07096,
      "grad_norm": 0.3006458580493927,
      "learning_rate": 0.0001952953727163622,
      "loss": 0.7867,
      "step": 887
    },
    {
      "epoch": 0.07104,
      "grad_norm": 0.2911909520626068,
      "learning_rate": 0.00019529003867182291,
      "loss": 0.7686,
      "step": 888
    },
    {
      "epoch": 0.07112,
      "grad_norm": 0.3172069489955902,
      "learning_rate": 0.00019528470462728365,
      "loss": 0.9282,
      "step": 889
    },
    {
      "epoch": 0.0712,
      "grad_norm": 0.2857869267463684,
      "learning_rate": 0.00019527937058274437,
      "loss": 0.897,
      "step": 890
    },
    {
      "epoch": 0.07128,
      "grad_norm": 0.28200116753578186,
      "learning_rate": 0.0001952740365382051,
      "loss": 0.6325,
      "step": 891
    },
    {
      "epoch": 0.07136,
      "grad_norm": 0.258497953414917,
      "learning_rate": 0.00019526870249366582,
      "loss": 0.6675,
      "step": 892
    },
    {
      "epoch": 0.07144,
      "grad_norm": 0.314640074968338,
      "learning_rate": 0.00019526336844912656,
      "loss": 0.867,
      "step": 893
    },
    {
      "epoch": 0.07152,
      "grad_norm": 0.3914903700351715,
      "learning_rate": 0.0001952580344045873,
      "loss": 0.9713,
      "step": 894
    },
    {
      "epoch": 0.0716,
      "grad_norm": 0.3271937072277069,
      "learning_rate": 0.000195252700360048,
      "loss": 0.6876,
      "step": 895
    },
    {
      "epoch": 0.07168,
      "grad_norm": 0.3929685950279236,
      "learning_rate": 0.00019524736631550875,
      "loss": 0.6749,
      "step": 896
    },
    {
      "epoch": 0.07176,
      "grad_norm": 0.34106171131134033,
      "learning_rate": 0.00019524203227096946,
      "loss": 0.8377,
      "step": 897
    },
    {
      "epoch": 0.07184,
      "grad_norm": 0.4203486442565918,
      "learning_rate": 0.0001952366982264302,
      "loss": 0.6871,
      "step": 898
    },
    {
      "epoch": 0.07192,
      "grad_norm": 0.36429980397224426,
      "learning_rate": 0.00019523136418189092,
      "loss": 0.7768,
      "step": 899
    },
    {
      "epoch": 0.072,
      "grad_norm": 0.3329797089099884,
      "learning_rate": 0.00019522603013735166,
      "loss": 0.729,
      "step": 900
    },
    {
      "epoch": 0.07208,
      "grad_norm": 0.3682601749897003,
      "learning_rate": 0.0001952206960928124,
      "loss": 0.7117,
      "step": 901
    },
    {
      "epoch": 0.07216,
      "grad_norm": 0.29614970088005066,
      "learning_rate": 0.0001952153620482731,
      "loss": 0.9881,
      "step": 902
    },
    {
      "epoch": 0.07224,
      "grad_norm": 0.3278747498989105,
      "learning_rate": 0.00019521002800373385,
      "loss": 0.7626,
      "step": 903
    },
    {
      "epoch": 0.07232,
      "grad_norm": 0.26551422476768494,
      "learning_rate": 0.00019520469395919456,
      "loss": 0.4926,
      "step": 904
    },
    {
      "epoch": 0.0724,
      "grad_norm": 0.24796225130558014,
      "learning_rate": 0.0001951993599146553,
      "loss": 0.5846,
      "step": 905
    },
    {
      "epoch": 0.07248,
      "grad_norm": 0.30930614471435547,
      "learning_rate": 0.00019519402587011601,
      "loss": 0.8788,
      "step": 906
    },
    {
      "epoch": 0.07256,
      "grad_norm": 0.2854447364807129,
      "learning_rate": 0.00019518869182557675,
      "loss": 1.1033,
      "step": 907
    },
    {
      "epoch": 0.07264,
      "grad_norm": 0.3859284818172455,
      "learning_rate": 0.0001951833577810375,
      "loss": 0.8833,
      "step": 908
    },
    {
      "epoch": 0.07272,
      "grad_norm": 0.32928773760795593,
      "learning_rate": 0.0001951780237364982,
      "loss": 0.7495,
      "step": 909
    },
    {
      "epoch": 0.0728,
      "grad_norm": 0.35596340894699097,
      "learning_rate": 0.00019517268969195895,
      "loss": 0.8659,
      "step": 910
    },
    {
      "epoch": 0.07288,
      "grad_norm": 0.34995758533477783,
      "learning_rate": 0.00019516735564741966,
      "loss": 0.8836,
      "step": 911
    },
    {
      "epoch": 0.07296,
      "grad_norm": 0.31057068705558777,
      "learning_rate": 0.0001951620216028804,
      "loss": 0.8351,
      "step": 912
    },
    {
      "epoch": 0.07304,
      "grad_norm": 0.31553763151168823,
      "learning_rate": 0.0001951566875583411,
      "loss": 0.8255,
      "step": 913
    },
    {
      "epoch": 0.07312,
      "grad_norm": 0.3464760482311249,
      "learning_rate": 0.00019515135351380185,
      "loss": 0.8521,
      "step": 914
    },
    {
      "epoch": 0.0732,
      "grad_norm": 0.3444039821624756,
      "learning_rate": 0.0001951460194692626,
      "loss": 0.8856,
      "step": 915
    },
    {
      "epoch": 0.07328,
      "grad_norm": 0.32458600401878357,
      "learning_rate": 0.0001951406854247233,
      "loss": 0.8176,
      "step": 916
    },
    {
      "epoch": 0.07336,
      "grad_norm": 0.3438481390476227,
      "learning_rate": 0.00019513535138018404,
      "loss": 0.78,
      "step": 917
    },
    {
      "epoch": 0.07344,
      "grad_norm": 0.3037559390068054,
      "learning_rate": 0.00019513001733564476,
      "loss": 0.8716,
      "step": 918
    },
    {
      "epoch": 0.07352,
      "grad_norm": 0.28338393568992615,
      "learning_rate": 0.0001951246832911055,
      "loss": 0.6104,
      "step": 919
    },
    {
      "epoch": 0.0736,
      "grad_norm": 0.27270129323005676,
      "learning_rate": 0.0001951193492465662,
      "loss": 1.1078,
      "step": 920
    },
    {
      "epoch": 0.07368,
      "grad_norm": 0.23863819241523743,
      "learning_rate": 0.00019511401520202695,
      "loss": 1.0322,
      "step": 921
    },
    {
      "epoch": 0.07376,
      "grad_norm": 0.27350175380706787,
      "learning_rate": 0.0001951086811574877,
      "loss": 1.101,
      "step": 922
    },
    {
      "epoch": 0.07384,
      "grad_norm": 0.485634982585907,
      "learning_rate": 0.0001951033471129484,
      "loss": 0.8249,
      "step": 923
    },
    {
      "epoch": 0.07392,
      "grad_norm": 0.3507641851902008,
      "learning_rate": 0.00019509801306840914,
      "loss": 0.7509,
      "step": 924
    },
    {
      "epoch": 0.074,
      "grad_norm": 0.31059643626213074,
      "learning_rate": 0.00019509267902386986,
      "loss": 1.0447,
      "step": 925
    },
    {
      "epoch": 0.07408,
      "grad_norm": 0.3494219481945038,
      "learning_rate": 0.0001950873449793306,
      "loss": 0.9274,
      "step": 926
    },
    {
      "epoch": 0.07416,
      "grad_norm": 0.2605847418308258,
      "learning_rate": 0.0001950820109347913,
      "loss": 0.5532,
      "step": 927
    },
    {
      "epoch": 0.07424,
      "grad_norm": 0.3259684443473816,
      "learning_rate": 0.00019507667689025205,
      "loss": 0.8542,
      "step": 928
    },
    {
      "epoch": 0.07432,
      "grad_norm": 0.2694651782512665,
      "learning_rate": 0.0001950713428457128,
      "loss": 0.9603,
      "step": 929
    },
    {
      "epoch": 0.0744,
      "grad_norm": 0.37687963247299194,
      "learning_rate": 0.0001950660088011735,
      "loss": 0.7414,
      "step": 930
    },
    {
      "epoch": 0.07448,
      "grad_norm": 0.264017790555954,
      "learning_rate": 0.00019506067475663424,
      "loss": 0.8216,
      "step": 931
    },
    {
      "epoch": 0.07456,
      "grad_norm": 0.38402116298675537,
      "learning_rate": 0.00019505534071209495,
      "loss": 0.799,
      "step": 932
    },
    {
      "epoch": 0.07464,
      "grad_norm": 0.33160272240638733,
      "learning_rate": 0.0001950500066675557,
      "loss": 1.1251,
      "step": 933
    },
    {
      "epoch": 0.07472,
      "grad_norm": 0.37298592925071716,
      "learning_rate": 0.00019504467262301643,
      "loss": 1.1918,
      "step": 934
    },
    {
      "epoch": 0.0748,
      "grad_norm": 0.3891923129558563,
      "learning_rate": 0.00019503933857847715,
      "loss": 0.6381,
      "step": 935
    },
    {
      "epoch": 0.07488,
      "grad_norm": 0.304248183965683,
      "learning_rate": 0.00019503400453393788,
      "loss": 0.7307,
      "step": 936
    },
    {
      "epoch": 0.07496,
      "grad_norm": 0.2540450394153595,
      "learning_rate": 0.0001950286704893986,
      "loss": 0.6378,
      "step": 937
    },
    {
      "epoch": 0.07504,
      "grad_norm": 0.49947670102119446,
      "learning_rate": 0.00019502333644485934,
      "loss": 1.0155,
      "step": 938
    },
    {
      "epoch": 0.07512,
      "grad_norm": 0.32766950130462646,
      "learning_rate": 0.00019501800240032005,
      "loss": 0.7979,
      "step": 939
    },
    {
      "epoch": 0.0752,
      "grad_norm": 0.3638409972190857,
      "learning_rate": 0.0001950126683557808,
      "loss": 0.8263,
      "step": 940
    },
    {
      "epoch": 0.07528,
      "grad_norm": 0.22750991582870483,
      "learning_rate": 0.00019500733431124153,
      "loss": 0.9133,
      "step": 941
    },
    {
      "epoch": 0.07536,
      "grad_norm": 0.34453174471855164,
      "learning_rate": 0.00019500200026670224,
      "loss": 0.8834,
      "step": 942
    },
    {
      "epoch": 0.07544,
      "grad_norm": 0.2901095747947693,
      "learning_rate": 0.00019499666622216298,
      "loss": 0.8608,
      "step": 943
    },
    {
      "epoch": 0.07552,
      "grad_norm": 0.32871150970458984,
      "learning_rate": 0.0001949913321776237,
      "loss": 0.7478,
      "step": 944
    },
    {
      "epoch": 0.0756,
      "grad_norm": 0.3238505423069,
      "learning_rate": 0.00019498599813308444,
      "loss": 0.8678,
      "step": 945
    },
    {
      "epoch": 0.07568,
      "grad_norm": 0.27718088030815125,
      "learning_rate": 0.00019498066408854515,
      "loss": 1.007,
      "step": 946
    },
    {
      "epoch": 0.07576,
      "grad_norm": 0.34060239791870117,
      "learning_rate": 0.0001949753300440059,
      "loss": 0.8716,
      "step": 947
    },
    {
      "epoch": 0.07584,
      "grad_norm": 0.2765045464038849,
      "learning_rate": 0.00019496999599946663,
      "loss": 0.7082,
      "step": 948
    },
    {
      "epoch": 0.07592,
      "grad_norm": 0.3287978172302246,
      "learning_rate": 0.00019496466195492734,
      "loss": 0.9,
      "step": 949
    },
    {
      "epoch": 0.076,
      "grad_norm": 0.32192131876945496,
      "learning_rate": 0.00019495932791038808,
      "loss": 0.6425,
      "step": 950
    },
    {
      "epoch": 0.07608,
      "grad_norm": 0.27130889892578125,
      "learning_rate": 0.0001949539938658488,
      "loss": 0.8316,
      "step": 951
    },
    {
      "epoch": 0.07616,
      "grad_norm": 0.31015583872795105,
      "learning_rate": 0.00019494865982130953,
      "loss": 0.7913,
      "step": 952
    },
    {
      "epoch": 0.07624,
      "grad_norm": 0.36675408482551575,
      "learning_rate": 0.00019494332577677025,
      "loss": 0.8052,
      "step": 953
    },
    {
      "epoch": 0.07632,
      "grad_norm": 0.24097982048988342,
      "learning_rate": 0.00019493799173223099,
      "loss": 0.8082,
      "step": 954
    },
    {
      "epoch": 0.0764,
      "grad_norm": 0.3593117296695709,
      "learning_rate": 0.0001949326576876917,
      "loss": 1.2706,
      "step": 955
    },
    {
      "epoch": 0.07648,
      "grad_norm": 0.2665477991104126,
      "learning_rate": 0.00019492732364315244,
      "loss": 0.8756,
      "step": 956
    },
    {
      "epoch": 0.07656,
      "grad_norm": 0.38406896591186523,
      "learning_rate": 0.00019492198959861315,
      "loss": 0.6037,
      "step": 957
    },
    {
      "epoch": 0.07664,
      "grad_norm": 0.30267494916915894,
      "learning_rate": 0.0001949166555540739,
      "loss": 0.9087,
      "step": 958
    },
    {
      "epoch": 0.07672,
      "grad_norm": 0.31578317284584045,
      "learning_rate": 0.0001949113215095346,
      "loss": 0.9511,
      "step": 959
    },
    {
      "epoch": 0.0768,
      "grad_norm": 0.32656025886535645,
      "learning_rate": 0.00019490598746499534,
      "loss": 0.7051,
      "step": 960
    },
    {
      "epoch": 0.07688,
      "grad_norm": 0.3184671998023987,
      "learning_rate": 0.00019490065342045608,
      "loss": 0.7757,
      "step": 961
    },
    {
      "epoch": 0.07696,
      "grad_norm": 0.3491128981113434,
      "learning_rate": 0.0001948953193759168,
      "loss": 0.8937,
      "step": 962
    },
    {
      "epoch": 0.07704,
      "grad_norm": 0.2944876253604889,
      "learning_rate": 0.00019488998533137754,
      "loss": 0.8426,
      "step": 963
    },
    {
      "epoch": 0.07712,
      "grad_norm": 0.3392990529537201,
      "learning_rate": 0.00019488465128683825,
      "loss": 0.8053,
      "step": 964
    },
    {
      "epoch": 0.0772,
      "grad_norm": 0.4412028193473816,
      "learning_rate": 0.000194879317242299,
      "loss": 0.739,
      "step": 965
    },
    {
      "epoch": 0.07728,
      "grad_norm": 0.33071592450141907,
      "learning_rate": 0.0001948739831977597,
      "loss": 0.5165,
      "step": 966
    },
    {
      "epoch": 0.07736,
      "grad_norm": 0.24173511564731598,
      "learning_rate": 0.00019486864915322044,
      "loss": 0.8639,
      "step": 967
    },
    {
      "epoch": 0.07744,
      "grad_norm": 0.3038736879825592,
      "learning_rate": 0.00019486331510868115,
      "loss": 0.9257,
      "step": 968
    },
    {
      "epoch": 0.07752,
      "grad_norm": 0.33748430013656616,
      "learning_rate": 0.0001948579810641419,
      "loss": 0.7509,
      "step": 969
    },
    {
      "epoch": 0.0776,
      "grad_norm": 0.37050682306289673,
      "learning_rate": 0.0001948526470196026,
      "loss": 0.7043,
      "step": 970
    },
    {
      "epoch": 0.07768,
      "grad_norm": 0.3582775890827179,
      "learning_rate": 0.00019484731297506335,
      "loss": 1.0477,
      "step": 971
    },
    {
      "epoch": 0.07776,
      "grad_norm": 0.3779740333557129,
      "learning_rate": 0.00019484197893052406,
      "loss": 0.7467,
      "step": 972
    },
    {
      "epoch": 0.07784,
      "grad_norm": 0.38059908151626587,
      "learning_rate": 0.0001948366448859848,
      "loss": 0.8235,
      "step": 973
    },
    {
      "epoch": 0.07792,
      "grad_norm": 0.2668849527835846,
      "learning_rate": 0.0001948313108414455,
      "loss": 0.5887,
      "step": 974
    },
    {
      "epoch": 0.078,
      "grad_norm": 0.3681196868419647,
      "learning_rate": 0.00019482597679690625,
      "loss": 1.0698,
      "step": 975
    },
    {
      "epoch": 0.07808,
      "grad_norm": 0.35359299182891846,
      "learning_rate": 0.000194820642752367,
      "loss": 0.8686,
      "step": 976
    },
    {
      "epoch": 0.07816,
      "grad_norm": 0.31210196018218994,
      "learning_rate": 0.0001948153087078277,
      "loss": 0.871,
      "step": 977
    },
    {
      "epoch": 0.07824,
      "grad_norm": 0.31569191813468933,
      "learning_rate": 0.00019480997466328844,
      "loss": 1.1227,
      "step": 978
    },
    {
      "epoch": 0.07832,
      "grad_norm": 0.24999812245368958,
      "learning_rate": 0.00019480464061874916,
      "loss": 0.898,
      "step": 979
    },
    {
      "epoch": 0.0784,
      "grad_norm": 0.3584613502025604,
      "learning_rate": 0.0001947993065742099,
      "loss": 0.722,
      "step": 980
    },
    {
      "epoch": 0.07848,
      "grad_norm": 0.392776221036911,
      "learning_rate": 0.00019479397252967064,
      "loss": 0.8508,
      "step": 981
    },
    {
      "epoch": 0.07856,
      "grad_norm": 0.33595919609069824,
      "learning_rate": 0.00019478863848513135,
      "loss": 0.9636,
      "step": 982
    },
    {
      "epoch": 0.07864,
      "grad_norm": 0.49376246333122253,
      "learning_rate": 0.0001947833044405921,
      "loss": 1.2439,
      "step": 983
    },
    {
      "epoch": 0.07872,
      "grad_norm": 0.23482230305671692,
      "learning_rate": 0.0001947779703960528,
      "loss": 1.0236,
      "step": 984
    },
    {
      "epoch": 0.0788,
      "grad_norm": 0.30046409368515015,
      "learning_rate": 0.00019477263635151354,
      "loss": 0.8344,
      "step": 985
    },
    {
      "epoch": 0.07888,
      "grad_norm": 0.3803524374961853,
      "learning_rate": 0.00019476730230697425,
      "loss": 0.7945,
      "step": 986
    },
    {
      "epoch": 0.07896,
      "grad_norm": 0.3440638780593872,
      "learning_rate": 0.000194761968262435,
      "loss": 0.9089,
      "step": 987
    },
    {
      "epoch": 0.07904,
      "grad_norm": 0.2823905944824219,
      "learning_rate": 0.00019475663421789573,
      "loss": 0.853,
      "step": 988
    },
    {
      "epoch": 0.07912,
      "grad_norm": 0.23653146624565125,
      "learning_rate": 0.00019475130017335645,
      "loss": 0.7664,
      "step": 989
    },
    {
      "epoch": 0.0792,
      "grad_norm": 0.23860955238342285,
      "learning_rate": 0.00019474596612881719,
      "loss": 0.9861,
      "step": 990
    },
    {
      "epoch": 0.07928,
      "grad_norm": 0.32198771834373474,
      "learning_rate": 0.0001947406320842779,
      "loss": 0.8535,
      "step": 991
    },
    {
      "epoch": 0.07936,
      "grad_norm": 0.28206050395965576,
      "learning_rate": 0.00019473529803973864,
      "loss": 0.978,
      "step": 992
    },
    {
      "epoch": 0.07944,
      "grad_norm": 0.2923710346221924,
      "learning_rate": 0.00019472996399519935,
      "loss": 0.6355,
      "step": 993
    },
    {
      "epoch": 0.07952,
      "grad_norm": 0.29006677865982056,
      "learning_rate": 0.0001947246299506601,
      "loss": 0.6067,
      "step": 994
    },
    {
      "epoch": 0.0796,
      "grad_norm": 0.3474816083908081,
      "learning_rate": 0.00019471929590612083,
      "loss": 0.7386,
      "step": 995
    },
    {
      "epoch": 0.07968,
      "grad_norm": 0.36946535110473633,
      "learning_rate": 0.00019471396186158154,
      "loss": 0.7888,
      "step": 996
    },
    {
      "epoch": 0.07976,
      "grad_norm": 0.33548980951309204,
      "learning_rate": 0.00019470862781704228,
      "loss": 0.809,
      "step": 997
    },
    {
      "epoch": 0.07984,
      "grad_norm": 0.25449809432029724,
      "learning_rate": 0.000194703293772503,
      "loss": 0.8766,
      "step": 998
    },
    {
      "epoch": 0.07992,
      "grad_norm": 0.2746674418449402,
      "learning_rate": 0.00019469795972796374,
      "loss": 1.0642,
      "step": 999
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.30929696559906006,
      "learning_rate": 0.00019469262568342445,
      "loss": 0.6169,
      "step": 1000
    },
    {
      "epoch": 0.08008,
      "grad_norm": 0.3851032257080078,
      "learning_rate": 0.0001946872916388852,
      "loss": 0.8644,
      "step": 1001
    },
    {
      "epoch": 0.08016,
      "grad_norm": 0.3925878405570984,
      "learning_rate": 0.00019468195759434593,
      "loss": 0.6187,
      "step": 1002
    },
    {
      "epoch": 0.08024,
      "grad_norm": 0.3829660415649414,
      "learning_rate": 0.00019467662354980664,
      "loss": 1.1249,
      "step": 1003
    },
    {
      "epoch": 0.08032,
      "grad_norm": 0.3155593276023865,
      "learning_rate": 0.00019467128950526738,
      "loss": 0.817,
      "step": 1004
    },
    {
      "epoch": 0.0804,
      "grad_norm": 0.41494959592819214,
      "learning_rate": 0.0001946659554607281,
      "loss": 0.8865,
      "step": 1005
    },
    {
      "epoch": 0.08048,
      "grad_norm": 0.3965742886066437,
      "learning_rate": 0.00019466062141618883,
      "loss": 1.0217,
      "step": 1006
    },
    {
      "epoch": 0.08056,
      "grad_norm": 0.3015025854110718,
      "learning_rate": 0.00019465528737164955,
      "loss": 0.8269,
      "step": 1007
    },
    {
      "epoch": 0.08064,
      "grad_norm": 0.29727983474731445,
      "learning_rate": 0.0001946499533271103,
      "loss": 1.0343,
      "step": 1008
    },
    {
      "epoch": 0.08072,
      "grad_norm": 0.2991851270198822,
      "learning_rate": 0.00019464461928257103,
      "loss": 0.776,
      "step": 1009
    },
    {
      "epoch": 0.0808,
      "grad_norm": 0.33853670954704285,
      "learning_rate": 0.00019463928523803174,
      "loss": 1.03,
      "step": 1010
    },
    {
      "epoch": 0.08088,
      "grad_norm": 0.30035191774368286,
      "learning_rate": 0.00019463395119349248,
      "loss": 0.9159,
      "step": 1011
    },
    {
      "epoch": 0.08096,
      "grad_norm": 0.3347112238407135,
      "learning_rate": 0.0001946286171489532,
      "loss": 0.959,
      "step": 1012
    },
    {
      "epoch": 0.08104,
      "grad_norm": 0.38428929448127747,
      "learning_rate": 0.00019462328310441393,
      "loss": 1.0565,
      "step": 1013
    },
    {
      "epoch": 0.08112,
      "grad_norm": 0.30459991097450256,
      "learning_rate": 0.00019461794905987464,
      "loss": 1.1456,
      "step": 1014
    },
    {
      "epoch": 0.0812,
      "grad_norm": 0.34737589955329895,
      "learning_rate": 0.00019461261501533538,
      "loss": 0.6814,
      "step": 1015
    },
    {
      "epoch": 0.08128,
      "grad_norm": 0.36283308267593384,
      "learning_rate": 0.00019460728097079612,
      "loss": 0.8571,
      "step": 1016
    },
    {
      "epoch": 0.08136,
      "grad_norm": 0.27790090441703796,
      "learning_rate": 0.00019460194692625684,
      "loss": 0.9468,
      "step": 1017
    },
    {
      "epoch": 0.08144,
      "grad_norm": 0.2968001663684845,
      "learning_rate": 0.00019459661288171758,
      "loss": 0.6133,
      "step": 1018
    },
    {
      "epoch": 0.08152,
      "grad_norm": 0.2935660481452942,
      "learning_rate": 0.0001945912788371783,
      "loss": 0.6959,
      "step": 1019
    },
    {
      "epoch": 0.0816,
      "grad_norm": 0.2934681475162506,
      "learning_rate": 0.00019458594479263903,
      "loss": 0.8513,
      "step": 1020
    },
    {
      "epoch": 0.08168,
      "grad_norm": 0.3077680766582489,
      "learning_rate": 0.00019458061074809974,
      "loss": 0.7177,
      "step": 1021
    },
    {
      "epoch": 0.08176,
      "grad_norm": 0.34767043590545654,
      "learning_rate": 0.00019457527670356048,
      "loss": 0.8344,
      "step": 1022
    },
    {
      "epoch": 0.08184,
      "grad_norm": 0.2579566240310669,
      "learning_rate": 0.00019456994265902122,
      "loss": 0.7337,
      "step": 1023
    },
    {
      "epoch": 0.08192,
      "grad_norm": 0.30376049876213074,
      "learning_rate": 0.00019456460861448193,
      "loss": 0.7971,
      "step": 1024
    },
    {
      "epoch": 0.082,
      "grad_norm": 0.2598000764846802,
      "learning_rate": 0.00019455927456994267,
      "loss": 0.5994,
      "step": 1025
    },
    {
      "epoch": 0.08208,
      "grad_norm": 0.28204581141471863,
      "learning_rate": 0.0001945539405254034,
      "loss": 0.8859,
      "step": 1026
    },
    {
      "epoch": 0.08216,
      "grad_norm": 0.3167698383331299,
      "learning_rate": 0.00019454860648086413,
      "loss": 0.6661,
      "step": 1027
    },
    {
      "epoch": 0.08224,
      "grad_norm": 0.25605708360671997,
      "learning_rate": 0.00019454327243632484,
      "loss": 0.4636,
      "step": 1028
    },
    {
      "epoch": 0.08232,
      "grad_norm": 0.31879526376724243,
      "learning_rate": 0.00019453793839178558,
      "loss": 0.9917,
      "step": 1029
    },
    {
      "epoch": 0.0824,
      "grad_norm": 0.3219507038593292,
      "learning_rate": 0.00019453260434724632,
      "loss": 1.0633,
      "step": 1030
    },
    {
      "epoch": 0.08248,
      "grad_norm": 0.3077673316001892,
      "learning_rate": 0.00019452727030270703,
      "loss": 0.8939,
      "step": 1031
    },
    {
      "epoch": 0.08256,
      "grad_norm": 0.3685307502746582,
      "learning_rate": 0.00019452193625816777,
      "loss": 0.7837,
      "step": 1032
    },
    {
      "epoch": 0.08264,
      "grad_norm": 0.3101881742477417,
      "learning_rate": 0.00019451660221362848,
      "loss": 1.0793,
      "step": 1033
    },
    {
      "epoch": 0.08272,
      "grad_norm": 0.3308796286582947,
      "learning_rate": 0.00019451126816908922,
      "loss": 0.689,
      "step": 1034
    },
    {
      "epoch": 0.0828,
      "grad_norm": 0.3461028039455414,
      "learning_rate": 0.00019450593412454996,
      "loss": 0.9196,
      "step": 1035
    },
    {
      "epoch": 0.08288,
      "grad_norm": 0.2788413465023041,
      "learning_rate": 0.00019450060008001068,
      "loss": 0.9207,
      "step": 1036
    },
    {
      "epoch": 0.08296,
      "grad_norm": 0.29330965876579285,
      "learning_rate": 0.00019449526603547142,
      "loss": 0.8901,
      "step": 1037
    },
    {
      "epoch": 0.08304,
      "grad_norm": 0.2811548113822937,
      "learning_rate": 0.00019448993199093213,
      "loss": 1.1685,
      "step": 1038
    },
    {
      "epoch": 0.08312,
      "grad_norm": 0.323580801486969,
      "learning_rate": 0.00019448459794639287,
      "loss": 1.1928,
      "step": 1039
    },
    {
      "epoch": 0.0832,
      "grad_norm": 0.3343007564544678,
      "learning_rate": 0.00019447926390185358,
      "loss": 0.7675,
      "step": 1040
    },
    {
      "epoch": 0.08328,
      "grad_norm": 0.4918987452983856,
      "learning_rate": 0.00019447392985731432,
      "loss": 0.86,
      "step": 1041
    },
    {
      "epoch": 0.08336,
      "grad_norm": 0.2751083970069885,
      "learning_rate": 0.00019446859581277506,
      "loss": 0.8161,
      "step": 1042
    },
    {
      "epoch": 0.08344,
      "grad_norm": 0.3546735644340515,
      "learning_rate": 0.00019446326176823577,
      "loss": 0.9704,
      "step": 1043
    },
    {
      "epoch": 0.08352,
      "grad_norm": 0.2826893627643585,
      "learning_rate": 0.00019445792772369651,
      "loss": 0.767,
      "step": 1044
    },
    {
      "epoch": 0.0836,
      "grad_norm": 0.3384901285171509,
      "learning_rate": 0.00019445259367915723,
      "loss": 0.7679,
      "step": 1045
    },
    {
      "epoch": 0.08368,
      "grad_norm": 0.29455411434173584,
      "learning_rate": 0.00019444725963461797,
      "loss": 0.705,
      "step": 1046
    },
    {
      "epoch": 0.08376,
      "grad_norm": 0.3241848051548004,
      "learning_rate": 0.00019444192559007868,
      "loss": 0.8715,
      "step": 1047
    },
    {
      "epoch": 0.08384,
      "grad_norm": 0.2473272681236267,
      "learning_rate": 0.00019443659154553942,
      "loss": 0.6428,
      "step": 1048
    },
    {
      "epoch": 0.08392,
      "grad_norm": 0.2991214692592621,
      "learning_rate": 0.00019443125750100016,
      "loss": 1.2924,
      "step": 1049
    },
    {
      "epoch": 0.084,
      "grad_norm": 0.3147828280925751,
      "learning_rate": 0.00019442592345646087,
      "loss": 0.8861,
      "step": 1050
    },
    {
      "epoch": 0.08408,
      "grad_norm": 0.3677990436553955,
      "learning_rate": 0.0001944205894119216,
      "loss": 0.8393,
      "step": 1051
    },
    {
      "epoch": 0.08416,
      "grad_norm": 0.29831603169441223,
      "learning_rate": 0.00019441525536738233,
      "loss": 0.7272,
      "step": 1052
    },
    {
      "epoch": 0.08424,
      "grad_norm": 0.353340744972229,
      "learning_rate": 0.00019440992132284306,
      "loss": 0.9185,
      "step": 1053
    },
    {
      "epoch": 0.08432,
      "grad_norm": 0.3034437894821167,
      "learning_rate": 0.00019440458727830378,
      "loss": 0.8855,
      "step": 1054
    },
    {
      "epoch": 0.0844,
      "grad_norm": 0.3356943130493164,
      "learning_rate": 0.00019439925323376452,
      "loss": 0.9747,
      "step": 1055
    },
    {
      "epoch": 0.08448,
      "grad_norm": 0.30726516246795654,
      "learning_rate": 0.00019439391918922526,
      "loss": 0.527,
      "step": 1056
    },
    {
      "epoch": 0.08456,
      "grad_norm": 0.32737335562705994,
      "learning_rate": 0.00019438858514468597,
      "loss": 1.1968,
      "step": 1057
    },
    {
      "epoch": 0.08464,
      "grad_norm": 0.2917391061782837,
      "learning_rate": 0.0001943832511001467,
      "loss": 0.9544,
      "step": 1058
    },
    {
      "epoch": 0.08472,
      "grad_norm": 0.31301501393318176,
      "learning_rate": 0.00019437791705560742,
      "loss": 0.8615,
      "step": 1059
    },
    {
      "epoch": 0.0848,
      "grad_norm": 0.4270017147064209,
      "learning_rate": 0.00019437258301106816,
      "loss": 1.2282,
      "step": 1060
    },
    {
      "epoch": 0.08488,
      "grad_norm": 0.34659886360168457,
      "learning_rate": 0.00019436724896652888,
      "loss": 0.8675,
      "step": 1061
    },
    {
      "epoch": 0.08496,
      "grad_norm": 0.2546554207801819,
      "learning_rate": 0.00019436191492198962,
      "loss": 1.0684,
      "step": 1062
    },
    {
      "epoch": 0.08504,
      "grad_norm": 0.25734153389930725,
      "learning_rate": 0.00019435658087745035,
      "loss": 0.843,
      "step": 1063
    },
    {
      "epoch": 0.08512,
      "grad_norm": 0.28452420234680176,
      "learning_rate": 0.00019435124683291107,
      "loss": 1.0841,
      "step": 1064
    },
    {
      "epoch": 0.0852,
      "grad_norm": 0.29363059997558594,
      "learning_rate": 0.0001943459127883718,
      "loss": 0.8375,
      "step": 1065
    },
    {
      "epoch": 0.08528,
      "grad_norm": 0.26241517066955566,
      "learning_rate": 0.00019434057874383252,
      "loss": 0.6502,
      "step": 1066
    },
    {
      "epoch": 0.08536,
      "grad_norm": 0.32719144225120544,
      "learning_rate": 0.00019433524469929326,
      "loss": 0.9096,
      "step": 1067
    },
    {
      "epoch": 0.08544,
      "grad_norm": 0.3381403088569641,
      "learning_rate": 0.00019432991065475397,
      "loss": 1.0286,
      "step": 1068
    },
    {
      "epoch": 0.08552,
      "grad_norm": 0.3328779935836792,
      "learning_rate": 0.0001943245766102147,
      "loss": 0.7992,
      "step": 1069
    },
    {
      "epoch": 0.0856,
      "grad_norm": 0.3035404682159424,
      "learning_rate": 0.00019431924256567545,
      "loss": 0.6081,
      "step": 1070
    },
    {
      "epoch": 0.08568,
      "grad_norm": 0.27327877283096313,
      "learning_rate": 0.00019431390852113617,
      "loss": 0.8183,
      "step": 1071
    },
    {
      "epoch": 0.08576,
      "grad_norm": 0.28262579441070557,
      "learning_rate": 0.0001943085744765969,
      "loss": 0.6657,
      "step": 1072
    },
    {
      "epoch": 0.08584,
      "grad_norm": 0.308723509311676,
      "learning_rate": 0.00019430324043205762,
      "loss": 1.082,
      "step": 1073
    },
    {
      "epoch": 0.08592,
      "grad_norm": 0.32871222496032715,
      "learning_rate": 0.00019429790638751836,
      "loss": 0.9185,
      "step": 1074
    },
    {
      "epoch": 0.086,
      "grad_norm": 0.33739838004112244,
      "learning_rate": 0.00019429257234297907,
      "loss": 0.5931,
      "step": 1075
    },
    {
      "epoch": 0.08608,
      "grad_norm": 0.25582846999168396,
      "learning_rate": 0.0001942872382984398,
      "loss": 0.821,
      "step": 1076
    },
    {
      "epoch": 0.08616,
      "grad_norm": 0.3074037730693817,
      "learning_rate": 0.00019428190425390055,
      "loss": 1.0119,
      "step": 1077
    },
    {
      "epoch": 0.08624,
      "grad_norm": 0.269886314868927,
      "learning_rate": 0.00019427657020936126,
      "loss": 0.9743,
      "step": 1078
    },
    {
      "epoch": 0.08632,
      "grad_norm": 0.2863856852054596,
      "learning_rate": 0.000194271236164822,
      "loss": 0.8529,
      "step": 1079
    },
    {
      "epoch": 0.0864,
      "grad_norm": 0.29360896348953247,
      "learning_rate": 0.00019426590212028272,
      "loss": 0.6807,
      "step": 1080
    },
    {
      "epoch": 0.08648,
      "grad_norm": 0.33718791604042053,
      "learning_rate": 0.00019426056807574346,
      "loss": 0.6855,
      "step": 1081
    },
    {
      "epoch": 0.08656,
      "grad_norm": 0.255340576171875,
      "learning_rate": 0.00019425523403120417,
      "loss": 1.0711,
      "step": 1082
    },
    {
      "epoch": 0.08664,
      "grad_norm": 0.2945016920566559,
      "learning_rate": 0.0001942498999866649,
      "loss": 0.587,
      "step": 1083
    },
    {
      "epoch": 0.08672,
      "grad_norm": 0.3142528831958771,
      "learning_rate": 0.00019424456594212562,
      "loss": 0.5414,
      "step": 1084
    },
    {
      "epoch": 0.0868,
      "grad_norm": 0.3313968777656555,
      "learning_rate": 0.00019423923189758636,
      "loss": 0.7233,
      "step": 1085
    },
    {
      "epoch": 0.08688,
      "grad_norm": 0.2975299656391144,
      "learning_rate": 0.00019423389785304707,
      "loss": 0.9303,
      "step": 1086
    },
    {
      "epoch": 0.08696,
      "grad_norm": 0.3720896244049072,
      "learning_rate": 0.0001942285638085078,
      "loss": 1.0005,
      "step": 1087
    },
    {
      "epoch": 0.08704,
      "grad_norm": 0.31425678730010986,
      "learning_rate": 0.00019422322976396855,
      "loss": 0.9347,
      "step": 1088
    },
    {
      "epoch": 0.08712,
      "grad_norm": 0.31284716725349426,
      "learning_rate": 0.00019421789571942927,
      "loss": 1.1492,
      "step": 1089
    },
    {
      "epoch": 0.0872,
      "grad_norm": 0.2938031554222107,
      "learning_rate": 0.00019421256167489,
      "loss": 0.6076,
      "step": 1090
    },
    {
      "epoch": 0.08728,
      "grad_norm": 0.3339375853538513,
      "learning_rate": 0.00019420722763035072,
      "loss": 1.176,
      "step": 1091
    },
    {
      "epoch": 0.08736,
      "grad_norm": 0.3256113529205322,
      "learning_rate": 0.00019420189358581146,
      "loss": 0.8174,
      "step": 1092
    },
    {
      "epoch": 0.08744,
      "grad_norm": 0.42913272976875305,
      "learning_rate": 0.00019419655954127217,
      "loss": 0.8956,
      "step": 1093
    },
    {
      "epoch": 0.08752,
      "grad_norm": 0.32229897379875183,
      "learning_rate": 0.0001941912254967329,
      "loss": 1.0833,
      "step": 1094
    },
    {
      "epoch": 0.0876,
      "grad_norm": 0.36828717589378357,
      "learning_rate": 0.00019418589145219362,
      "loss": 0.7529,
      "step": 1095
    },
    {
      "epoch": 0.08768,
      "grad_norm": 0.3708154261112213,
      "learning_rate": 0.00019418055740765436,
      "loss": 0.913,
      "step": 1096
    },
    {
      "epoch": 0.08776,
      "grad_norm": 0.37983572483062744,
      "learning_rate": 0.00019417522336311508,
      "loss": 0.7358,
      "step": 1097
    },
    {
      "epoch": 0.08784,
      "grad_norm": 0.33564528822898865,
      "learning_rate": 0.00019416988931857582,
      "loss": 0.7135,
      "step": 1098
    },
    {
      "epoch": 0.08792,
      "grad_norm": 0.29260754585266113,
      "learning_rate": 0.00019416455527403653,
      "loss": 0.589,
      "step": 1099
    },
    {
      "epoch": 0.088,
      "grad_norm": 0.4192415773868561,
      "learning_rate": 0.00019415922122949727,
      "loss": 0.8217,
      "step": 1100
    },
    {
      "epoch": 0.08808,
      "grad_norm": 0.24949252605438232,
      "learning_rate": 0.00019415388718495798,
      "loss": 1.2075,
      "step": 1101
    },
    {
      "epoch": 0.08816,
      "grad_norm": 0.2708565592765808,
      "learning_rate": 0.00019414855314041872,
      "loss": 0.4809,
      "step": 1102
    },
    {
      "epoch": 0.08824,
      "grad_norm": 0.30273497104644775,
      "learning_rate": 0.00019414321909587946,
      "loss": 1.0866,
      "step": 1103
    },
    {
      "epoch": 0.08832,
      "grad_norm": 0.2639644742012024,
      "learning_rate": 0.00019413788505134017,
      "loss": 0.9275,
      "step": 1104
    },
    {
      "epoch": 0.0884,
      "grad_norm": 0.2556304633617401,
      "learning_rate": 0.0001941325510068009,
      "loss": 0.5619,
      "step": 1105
    },
    {
      "epoch": 0.08848,
      "grad_norm": 0.33679890632629395,
      "learning_rate": 0.00019412721696226163,
      "loss": 0.535,
      "step": 1106
    },
    {
      "epoch": 0.08856,
      "grad_norm": 0.3150172531604767,
      "learning_rate": 0.00019412188291772237,
      "loss": 0.8917,
      "step": 1107
    },
    {
      "epoch": 0.08864,
      "grad_norm": 0.3800414800643921,
      "learning_rate": 0.00019411654887318308,
      "loss": 0.8513,
      "step": 1108
    },
    {
      "epoch": 0.08872,
      "grad_norm": 0.2952917218208313,
      "learning_rate": 0.00019411121482864382,
      "loss": 1.0768,
      "step": 1109
    },
    {
      "epoch": 0.0888,
      "grad_norm": 0.3218550384044647,
      "learning_rate": 0.00019410588078410456,
      "loss": 0.6832,
      "step": 1110
    },
    {
      "epoch": 0.08888,
      "grad_norm": 0.28781673312187195,
      "learning_rate": 0.00019410054673956527,
      "loss": 1.1781,
      "step": 1111
    },
    {
      "epoch": 0.08896,
      "grad_norm": 0.2795872986316681,
      "learning_rate": 0.000194095212695026,
      "loss": 0.9771,
      "step": 1112
    },
    {
      "epoch": 0.08904,
      "grad_norm": 0.29803407192230225,
      "learning_rate": 0.00019408987865048672,
      "loss": 0.819,
      "step": 1113
    },
    {
      "epoch": 0.08912,
      "grad_norm": 0.3199140131473541,
      "learning_rate": 0.00019408454460594746,
      "loss": 0.82,
      "step": 1114
    },
    {
      "epoch": 0.0892,
      "grad_norm": 0.2820298671722412,
      "learning_rate": 0.00019407921056140818,
      "loss": 0.6369,
      "step": 1115
    },
    {
      "epoch": 0.08928,
      "grad_norm": 0.4153532385826111,
      "learning_rate": 0.00019407387651686892,
      "loss": 0.6839,
      "step": 1116
    },
    {
      "epoch": 0.08936,
      "grad_norm": 0.26936766505241394,
      "learning_rate": 0.00019406854247232966,
      "loss": 1.087,
      "step": 1117
    },
    {
      "epoch": 0.08944,
      "grad_norm": 0.3508734703063965,
      "learning_rate": 0.00019406320842779037,
      "loss": 0.7821,
      "step": 1118
    },
    {
      "epoch": 0.08952,
      "grad_norm": 0.2889888882637024,
      "learning_rate": 0.0001940578743832511,
      "loss": 1.0091,
      "step": 1119
    },
    {
      "epoch": 0.0896,
      "grad_norm": 0.351688951253891,
      "learning_rate": 0.00019405254033871182,
      "loss": 0.7218,
      "step": 1120
    },
    {
      "epoch": 0.08968,
      "grad_norm": 0.32172298431396484,
      "learning_rate": 0.00019404720629417256,
      "loss": 0.9631,
      "step": 1121
    },
    {
      "epoch": 0.08976,
      "grad_norm": 0.3930757939815521,
      "learning_rate": 0.00019404187224963327,
      "loss": 0.8104,
      "step": 1122
    },
    {
      "epoch": 0.08984,
      "grad_norm": 0.2726898193359375,
      "learning_rate": 0.00019403653820509401,
      "loss": 0.9715,
      "step": 1123
    },
    {
      "epoch": 0.08992,
      "grad_norm": 0.29432862997055054,
      "learning_rate": 0.00019403120416055475,
      "loss": 0.8428,
      "step": 1124
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.3309992849826813,
      "learning_rate": 0.00019402587011601547,
      "loss": 0.6581,
      "step": 1125
    },
    {
      "epoch": 0.09008,
      "grad_norm": 0.34932082891464233,
      "learning_rate": 0.0001940205360714762,
      "loss": 0.8053,
      "step": 1126
    },
    {
      "epoch": 0.09016,
      "grad_norm": 0.3529101014137268,
      "learning_rate": 0.00019401520202693692,
      "loss": 0.7665,
      "step": 1127
    },
    {
      "epoch": 0.09024,
      "grad_norm": 0.31514692306518555,
      "learning_rate": 0.00019400986798239766,
      "loss": 1.0927,
      "step": 1128
    },
    {
      "epoch": 0.09032,
      "grad_norm": 0.31207266449928284,
      "learning_rate": 0.0001940045339378584,
      "loss": 0.5368,
      "step": 1129
    },
    {
      "epoch": 0.0904,
      "grad_norm": 0.3831997215747833,
      "learning_rate": 0.0001939991998933191,
      "loss": 0.7442,
      "step": 1130
    },
    {
      "epoch": 0.09048,
      "grad_norm": 0.2954742908477783,
      "learning_rate": 0.00019399386584877985,
      "loss": 0.9234,
      "step": 1131
    },
    {
      "epoch": 0.09056,
      "grad_norm": 0.31765976548194885,
      "learning_rate": 0.00019398853180424056,
      "loss": 0.6519,
      "step": 1132
    },
    {
      "epoch": 0.09064,
      "grad_norm": 0.3260997533798218,
      "learning_rate": 0.0001939831977597013,
      "loss": 0.9673,
      "step": 1133
    },
    {
      "epoch": 0.09072,
      "grad_norm": 0.23017406463623047,
      "learning_rate": 0.00019397786371516202,
      "loss": 0.8028,
      "step": 1134
    },
    {
      "epoch": 0.0908,
      "grad_norm": 0.3317072093486786,
      "learning_rate": 0.00019397252967062276,
      "loss": 1.0405,
      "step": 1135
    },
    {
      "epoch": 0.09088,
      "grad_norm": 0.35998159646987915,
      "learning_rate": 0.0001939671956260835,
      "loss": 0.8436,
      "step": 1136
    },
    {
      "epoch": 0.09096,
      "grad_norm": 0.3139459192752838,
      "learning_rate": 0.0001939618615815442,
      "loss": 0.7513,
      "step": 1137
    },
    {
      "epoch": 0.09104,
      "grad_norm": 0.29121899604797363,
      "learning_rate": 0.00019395652753700495,
      "loss": 0.8325,
      "step": 1138
    },
    {
      "epoch": 0.09112,
      "grad_norm": 0.29480451345443726,
      "learning_rate": 0.00019395119349246566,
      "loss": 0.7976,
      "step": 1139
    },
    {
      "epoch": 0.0912,
      "grad_norm": 0.2996695935726166,
      "learning_rate": 0.0001939458594479264,
      "loss": 0.9207,
      "step": 1140
    },
    {
      "epoch": 0.09128,
      "grad_norm": 0.3060220181941986,
      "learning_rate": 0.00019394052540338711,
      "loss": 1.0333,
      "step": 1141
    },
    {
      "epoch": 0.09136,
      "grad_norm": 0.2579885721206665,
      "learning_rate": 0.00019393519135884785,
      "loss": 0.9848,
      "step": 1142
    },
    {
      "epoch": 0.09144,
      "grad_norm": 0.28520190715789795,
      "learning_rate": 0.0001939298573143086,
      "loss": 0.4819,
      "step": 1143
    },
    {
      "epoch": 0.09152,
      "grad_norm": 0.2985678017139435,
      "learning_rate": 0.0001939245232697693,
      "loss": 0.6259,
      "step": 1144
    },
    {
      "epoch": 0.0916,
      "grad_norm": 0.2620675563812256,
      "learning_rate": 0.00019391918922523005,
      "loss": 1.1308,
      "step": 1145
    },
    {
      "epoch": 0.09168,
      "grad_norm": 0.34660956263542175,
      "learning_rate": 0.00019391385518069076,
      "loss": 0.7545,
      "step": 1146
    },
    {
      "epoch": 0.09176,
      "grad_norm": 0.2617446482181549,
      "learning_rate": 0.0001939085211361515,
      "loss": 0.6135,
      "step": 1147
    },
    {
      "epoch": 0.09184,
      "grad_norm": 0.3873863220214844,
      "learning_rate": 0.0001939031870916122,
      "loss": 1.1365,
      "step": 1148
    },
    {
      "epoch": 0.09192,
      "grad_norm": 0.3495737910270691,
      "learning_rate": 0.00019389785304707295,
      "loss": 0.8217,
      "step": 1149
    },
    {
      "epoch": 0.092,
      "grad_norm": 0.2627478837966919,
      "learning_rate": 0.0001938925190025337,
      "loss": 0.771,
      "step": 1150
    },
    {
      "epoch": 0.09208,
      "grad_norm": 0.3816612958908081,
      "learning_rate": 0.0001938871849579944,
      "loss": 0.9164,
      "step": 1151
    },
    {
      "epoch": 0.09216,
      "grad_norm": 0.31111395359039307,
      "learning_rate": 0.00019388185091345514,
      "loss": 0.6722,
      "step": 1152
    },
    {
      "epoch": 0.09224,
      "grad_norm": 0.2899388074874878,
      "learning_rate": 0.00019387651686891586,
      "loss": 0.562,
      "step": 1153
    },
    {
      "epoch": 0.09232,
      "grad_norm": 0.32072165608406067,
      "learning_rate": 0.0001938711828243766,
      "loss": 0.8898,
      "step": 1154
    },
    {
      "epoch": 0.0924,
      "grad_norm": 0.33286231756210327,
      "learning_rate": 0.0001938658487798373,
      "loss": 0.9138,
      "step": 1155
    },
    {
      "epoch": 0.09248,
      "grad_norm": 0.3224460184574127,
      "learning_rate": 0.00019386051473529805,
      "loss": 0.9479,
      "step": 1156
    },
    {
      "epoch": 0.09256,
      "grad_norm": 0.3659539818763733,
      "learning_rate": 0.0001938551806907588,
      "loss": 0.7993,
      "step": 1157
    },
    {
      "epoch": 0.09264,
      "grad_norm": 0.3200794458389282,
      "learning_rate": 0.0001938498466462195,
      "loss": 0.5776,
      "step": 1158
    },
    {
      "epoch": 0.09272,
      "grad_norm": 0.4648244082927704,
      "learning_rate": 0.00019384451260168024,
      "loss": 1.4572,
      "step": 1159
    },
    {
      "epoch": 0.0928,
      "grad_norm": 0.30048462748527527,
      "learning_rate": 0.00019383917855714095,
      "loss": 0.6645,
      "step": 1160
    },
    {
      "epoch": 0.09288,
      "grad_norm": 0.2853989005088806,
      "learning_rate": 0.0001938338445126017,
      "loss": 0.9238,
      "step": 1161
    },
    {
      "epoch": 0.09296,
      "grad_norm": 0.25906887650489807,
      "learning_rate": 0.0001938285104680624,
      "loss": 0.8106,
      "step": 1162
    },
    {
      "epoch": 0.09304,
      "grad_norm": 0.32691118121147156,
      "learning_rate": 0.00019382317642352315,
      "loss": 0.5768,
      "step": 1163
    },
    {
      "epoch": 0.09312,
      "grad_norm": 0.2959059774875641,
      "learning_rate": 0.0001938178423789839,
      "loss": 1.0043,
      "step": 1164
    },
    {
      "epoch": 0.0932,
      "grad_norm": 0.40360227227211,
      "learning_rate": 0.0001938125083344446,
      "loss": 0.8388,
      "step": 1165
    },
    {
      "epoch": 0.09328,
      "grad_norm": 0.33451390266418457,
      "learning_rate": 0.00019380717428990534,
      "loss": 0.8585,
      "step": 1166
    },
    {
      "epoch": 0.09336,
      "grad_norm": 0.30999261140823364,
      "learning_rate": 0.00019380184024536605,
      "loss": 0.7476,
      "step": 1167
    },
    {
      "epoch": 0.09344,
      "grad_norm": 0.3411707878112793,
      "learning_rate": 0.0001937965062008268,
      "loss": 0.9686,
      "step": 1168
    },
    {
      "epoch": 0.09352,
      "grad_norm": 0.28941255807876587,
      "learning_rate": 0.0001937911721562875,
      "loss": 0.7954,
      "step": 1169
    },
    {
      "epoch": 0.0936,
      "grad_norm": 0.26158085465431213,
      "learning_rate": 0.00019378583811174824,
      "loss": 0.538,
      "step": 1170
    },
    {
      "epoch": 0.09368,
      "grad_norm": 0.2841359078884125,
      "learning_rate": 0.00019378050406720898,
      "loss": 0.8364,
      "step": 1171
    },
    {
      "epoch": 0.09376,
      "grad_norm": 0.29458752274513245,
      "learning_rate": 0.0001937751700226697,
      "loss": 0.8727,
      "step": 1172
    },
    {
      "epoch": 0.09384,
      "grad_norm": 0.3693443834781647,
      "learning_rate": 0.00019376983597813044,
      "loss": 0.7264,
      "step": 1173
    },
    {
      "epoch": 0.09392,
      "grad_norm": 0.2783692181110382,
      "learning_rate": 0.00019376450193359115,
      "loss": 1.135,
      "step": 1174
    },
    {
      "epoch": 0.094,
      "grad_norm": 0.3734261989593506,
      "learning_rate": 0.0001937591678890519,
      "loss": 0.7789,
      "step": 1175
    },
    {
      "epoch": 0.09408,
      "grad_norm": 0.2852800488471985,
      "learning_rate": 0.0001937538338445126,
      "loss": 1.1365,
      "step": 1176
    },
    {
      "epoch": 0.09416,
      "grad_norm": 0.2858624756336212,
      "learning_rate": 0.00019374849979997334,
      "loss": 0.844,
      "step": 1177
    },
    {
      "epoch": 0.09424,
      "grad_norm": 0.34896406531333923,
      "learning_rate": 0.00019374316575543408,
      "loss": 0.9973,
      "step": 1178
    },
    {
      "epoch": 0.09432,
      "grad_norm": 0.29588350653648376,
      "learning_rate": 0.0001937378317108948,
      "loss": 0.6849,
      "step": 1179
    },
    {
      "epoch": 0.0944,
      "grad_norm": 0.3122788071632385,
      "learning_rate": 0.00019373249766635553,
      "loss": 0.6872,
      "step": 1180
    },
    {
      "epoch": 0.09448,
      "grad_norm": 0.35817158222198486,
      "learning_rate": 0.00019372716362181625,
      "loss": 0.6564,
      "step": 1181
    },
    {
      "epoch": 0.09456,
      "grad_norm": 0.35523131489753723,
      "learning_rate": 0.000193721829577277,
      "loss": 0.777,
      "step": 1182
    },
    {
      "epoch": 0.09464,
      "grad_norm": 0.25043419003486633,
      "learning_rate": 0.00019371649553273773,
      "loss": 0.8132,
      "step": 1183
    },
    {
      "epoch": 0.09472,
      "grad_norm": 0.3303733468055725,
      "learning_rate": 0.00019371116148819844,
      "loss": 0.7611,
      "step": 1184
    },
    {
      "epoch": 0.0948,
      "grad_norm": 0.43867063522338867,
      "learning_rate": 0.00019370582744365918,
      "loss": 1.1206,
      "step": 1185
    },
    {
      "epoch": 0.09488,
      "grad_norm": 0.24972456693649292,
      "learning_rate": 0.0001937004933991199,
      "loss": 0.5667,
      "step": 1186
    },
    {
      "epoch": 0.09496,
      "grad_norm": 0.33701586723327637,
      "learning_rate": 0.00019369515935458063,
      "loss": 0.8758,
      "step": 1187
    },
    {
      "epoch": 0.09504,
      "grad_norm": 0.4534333348274231,
      "learning_rate": 0.00019368982531004135,
      "loss": 1.31,
      "step": 1188
    },
    {
      "epoch": 0.09512,
      "grad_norm": 0.3289169669151306,
      "learning_rate": 0.00019368449126550209,
      "loss": 0.7861,
      "step": 1189
    },
    {
      "epoch": 0.0952,
      "grad_norm": 0.36677318811416626,
      "learning_rate": 0.00019367915722096282,
      "loss": 1.0056,
      "step": 1190
    },
    {
      "epoch": 0.09528,
      "grad_norm": 0.33826544880867004,
      "learning_rate": 0.00019367382317642354,
      "loss": 0.9224,
      "step": 1191
    },
    {
      "epoch": 0.09536,
      "grad_norm": 0.26284390687942505,
      "learning_rate": 0.00019366848913188428,
      "loss": 1.3991,
      "step": 1192
    },
    {
      "epoch": 0.09544,
      "grad_norm": 0.3591371476650238,
      "learning_rate": 0.000193663155087345,
      "loss": 0.7152,
      "step": 1193
    },
    {
      "epoch": 0.09552,
      "grad_norm": 0.35667645931243896,
      "learning_rate": 0.00019365782104280573,
      "loss": 0.895,
      "step": 1194
    },
    {
      "epoch": 0.0956,
      "grad_norm": 0.3593784272670746,
      "learning_rate": 0.00019365248699826644,
      "loss": 0.8093,
      "step": 1195
    },
    {
      "epoch": 0.09568,
      "grad_norm": 0.27855193614959717,
      "learning_rate": 0.00019364715295372718,
      "loss": 1.2286,
      "step": 1196
    },
    {
      "epoch": 0.09576,
      "grad_norm": 0.2900290787220001,
      "learning_rate": 0.00019364181890918792,
      "loss": 1.0533,
      "step": 1197
    },
    {
      "epoch": 0.09584,
      "grad_norm": 0.37557876110076904,
      "learning_rate": 0.00019363648486464864,
      "loss": 0.9521,
      "step": 1198
    },
    {
      "epoch": 0.09592,
      "grad_norm": 0.36403024196624756,
      "learning_rate": 0.00019363115082010938,
      "loss": 0.9512,
      "step": 1199
    },
    {
      "epoch": 0.096,
      "grad_norm": 0.390747994184494,
      "learning_rate": 0.0001936258167755701,
      "loss": 0.9725,
      "step": 1200
    },
    {
      "epoch": 0.09608,
      "grad_norm": 0.2963070571422577,
      "learning_rate": 0.00019362048273103083,
      "loss": 0.8253,
      "step": 1201
    },
    {
      "epoch": 0.09616,
      "grad_norm": 0.2846905291080475,
      "learning_rate": 0.00019361514868649154,
      "loss": 1.0256,
      "step": 1202
    },
    {
      "epoch": 0.09624,
      "grad_norm": 0.2928546667098999,
      "learning_rate": 0.00019360981464195228,
      "loss": 0.6325,
      "step": 1203
    },
    {
      "epoch": 0.09632,
      "grad_norm": 0.34230461716651917,
      "learning_rate": 0.00019360448059741302,
      "loss": 0.8635,
      "step": 1204
    },
    {
      "epoch": 0.0964,
      "grad_norm": 0.2541232407093048,
      "learning_rate": 0.00019359914655287373,
      "loss": 0.8842,
      "step": 1205
    },
    {
      "epoch": 0.09648,
      "grad_norm": 0.2997089624404907,
      "learning_rate": 0.00019359381250833447,
      "loss": 0.7655,
      "step": 1206
    },
    {
      "epoch": 0.09656,
      "grad_norm": 0.37072092294692993,
      "learning_rate": 0.00019358847846379519,
      "loss": 0.5707,
      "step": 1207
    },
    {
      "epoch": 0.09664,
      "grad_norm": 0.29004716873168945,
      "learning_rate": 0.00019358314441925593,
      "loss": 1.1415,
      "step": 1208
    },
    {
      "epoch": 0.09672,
      "grad_norm": 0.37554049491882324,
      "learning_rate": 0.00019357781037471664,
      "loss": 0.5424,
      "step": 1209
    },
    {
      "epoch": 0.0968,
      "grad_norm": 0.38068437576293945,
      "learning_rate": 0.00019357247633017738,
      "loss": 0.8685,
      "step": 1210
    },
    {
      "epoch": 0.09688,
      "grad_norm": 0.3599238693714142,
      "learning_rate": 0.0001935671422856381,
      "loss": 0.8354,
      "step": 1211
    },
    {
      "epoch": 0.09696,
      "grad_norm": 0.3734480142593384,
      "learning_rate": 0.00019356180824109883,
      "loss": 0.8246,
      "step": 1212
    },
    {
      "epoch": 0.09704,
      "grad_norm": 0.33884456753730774,
      "learning_rate": 0.00019355647419655954,
      "loss": 0.8336,
      "step": 1213
    },
    {
      "epoch": 0.09712,
      "grad_norm": 0.30725613236427307,
      "learning_rate": 0.00019355114015202028,
      "loss": 0.7606,
      "step": 1214
    },
    {
      "epoch": 0.0972,
      "grad_norm": 0.3744072914123535,
      "learning_rate": 0.000193545806107481,
      "loss": 0.6461,
      "step": 1215
    },
    {
      "epoch": 0.09728,
      "grad_norm": 0.3887156844139099,
      "learning_rate": 0.00019354047206294174,
      "loss": 0.7903,
      "step": 1216
    },
    {
      "epoch": 0.09736,
      "grad_norm": 0.4321421980857849,
      "learning_rate": 0.00019353513801840248,
      "loss": 0.9166,
      "step": 1217
    },
    {
      "epoch": 0.09744,
      "grad_norm": 0.33896902203559875,
      "learning_rate": 0.0001935298039738632,
      "loss": 0.6193,
      "step": 1218
    },
    {
      "epoch": 0.09752,
      "grad_norm": 0.2649596035480499,
      "learning_rate": 0.00019352446992932393,
      "loss": 0.614,
      "step": 1219
    },
    {
      "epoch": 0.0976,
      "grad_norm": 0.2405596673488617,
      "learning_rate": 0.00019351913588478464,
      "loss": 0.7294,
      "step": 1220
    },
    {
      "epoch": 0.09768,
      "grad_norm": 0.30767789483070374,
      "learning_rate": 0.00019351380184024538,
      "loss": 0.7327,
      "step": 1221
    },
    {
      "epoch": 0.09776,
      "grad_norm": 0.3378598988056183,
      "learning_rate": 0.0001935084677957061,
      "loss": 0.8471,
      "step": 1222
    },
    {
      "epoch": 0.09784,
      "grad_norm": 0.3870885968208313,
      "learning_rate": 0.00019350313375116683,
      "loss": 1.1826,
      "step": 1223
    },
    {
      "epoch": 0.09792,
      "grad_norm": 0.33091193437576294,
      "learning_rate": 0.00019349779970662755,
      "loss": 0.7997,
      "step": 1224
    },
    {
      "epoch": 0.098,
      "grad_norm": 0.298084557056427,
      "learning_rate": 0.00019349246566208829,
      "loss": 0.5224,
      "step": 1225
    },
    {
      "epoch": 0.09808,
      "grad_norm": 0.3469148576259613,
      "learning_rate": 0.000193487131617549,
      "loss": 0.6545,
      "step": 1226
    },
    {
      "epoch": 0.09816,
      "grad_norm": 0.40492144227027893,
      "learning_rate": 0.00019348179757300974,
      "loss": 1.0254,
      "step": 1227
    },
    {
      "epoch": 0.09824,
      "grad_norm": 0.3271811306476593,
      "learning_rate": 0.00019347646352847045,
      "loss": 0.8371,
      "step": 1228
    },
    {
      "epoch": 0.09832,
      "grad_norm": 0.33370843529701233,
      "learning_rate": 0.0001934711294839312,
      "loss": 0.8434,
      "step": 1229
    },
    {
      "epoch": 0.0984,
      "grad_norm": 0.33203041553497314,
      "learning_rate": 0.00019346579543939193,
      "loss": 0.7993,
      "step": 1230
    },
    {
      "epoch": 0.09848,
      "grad_norm": 0.44621261954307556,
      "learning_rate": 0.00019346046139485264,
      "loss": 0.7434,
      "step": 1231
    },
    {
      "epoch": 0.09856,
      "grad_norm": 0.2765442728996277,
      "learning_rate": 0.00019345512735031338,
      "loss": 1.0887,
      "step": 1232
    },
    {
      "epoch": 0.09864,
      "grad_norm": 0.297102153301239,
      "learning_rate": 0.0001934497933057741,
      "loss": 0.665,
      "step": 1233
    },
    {
      "epoch": 0.09872,
      "grad_norm": 0.3282278776168823,
      "learning_rate": 0.00019344445926123484,
      "loss": 0.8887,
      "step": 1234
    },
    {
      "epoch": 0.0988,
      "grad_norm": 0.26953738927841187,
      "learning_rate": 0.00019343912521669555,
      "loss": 0.6957,
      "step": 1235
    },
    {
      "epoch": 0.09888,
      "grad_norm": 0.3240709602832794,
      "learning_rate": 0.0001934337911721563,
      "loss": 0.8928,
      "step": 1236
    },
    {
      "epoch": 0.09896,
      "grad_norm": 0.2817452549934387,
      "learning_rate": 0.00019342845712761703,
      "loss": 0.6538,
      "step": 1237
    },
    {
      "epoch": 0.09904,
      "grad_norm": 0.3374130427837372,
      "learning_rate": 0.00019342312308307774,
      "loss": 0.9646,
      "step": 1238
    },
    {
      "epoch": 0.09912,
      "grad_norm": 0.23212076723575592,
      "learning_rate": 0.00019341778903853848,
      "loss": 0.9152,
      "step": 1239
    },
    {
      "epoch": 0.0992,
      "grad_norm": 0.22864696383476257,
      "learning_rate": 0.0001934124549939992,
      "loss": 0.6237,
      "step": 1240
    },
    {
      "epoch": 0.09928,
      "grad_norm": 0.3121507167816162,
      "learning_rate": 0.00019340712094945993,
      "loss": 0.6537,
      "step": 1241
    },
    {
      "epoch": 0.09936,
      "grad_norm": 0.28848975896835327,
      "learning_rate": 0.00019340178690492065,
      "loss": 0.6516,
      "step": 1242
    },
    {
      "epoch": 0.09944,
      "grad_norm": 0.35109326243400574,
      "learning_rate": 0.00019339645286038139,
      "loss": 1.0672,
      "step": 1243
    },
    {
      "epoch": 0.09952,
      "grad_norm": 0.28492605686187744,
      "learning_rate": 0.00019339111881584213,
      "loss": 1.0992,
      "step": 1244
    },
    {
      "epoch": 0.0996,
      "grad_norm": 0.28292566537857056,
      "learning_rate": 0.00019338578477130284,
      "loss": 0.5676,
      "step": 1245
    },
    {
      "epoch": 0.09968,
      "grad_norm": 0.38394588232040405,
      "learning_rate": 0.00019338045072676358,
      "loss": 0.7253,
      "step": 1246
    },
    {
      "epoch": 0.09976,
      "grad_norm": 0.3225996196269989,
      "learning_rate": 0.0001933751166822243,
      "loss": 1.0307,
      "step": 1247
    },
    {
      "epoch": 0.09984,
      "grad_norm": 0.34684842824935913,
      "learning_rate": 0.00019336978263768503,
      "loss": 0.8171,
      "step": 1248
    },
    {
      "epoch": 0.09992,
      "grad_norm": 0.34700295329093933,
      "learning_rate": 0.00019336444859314574,
      "loss": 0.8881,
      "step": 1249
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.3227720260620117,
      "learning_rate": 0.00019335911454860648,
      "loss": 0.7647,
      "step": 1250
    },
    {
      "epoch": 0.10008,
      "grad_norm": 0.2771998345851898,
      "learning_rate": 0.00019335378050406722,
      "loss": 0.9094,
      "step": 1251
    },
    {
      "epoch": 0.10016,
      "grad_norm": 0.3175908625125885,
      "learning_rate": 0.00019334844645952794,
      "loss": 1.0137,
      "step": 1252
    },
    {
      "epoch": 0.10024,
      "grad_norm": 0.30036765336990356,
      "learning_rate": 0.00019334311241498868,
      "loss": 0.9046,
      "step": 1253
    },
    {
      "epoch": 0.10032,
      "grad_norm": 0.2998429238796234,
      "learning_rate": 0.0001933377783704494,
      "loss": 0.7516,
      "step": 1254
    },
    {
      "epoch": 0.1004,
      "grad_norm": 0.3558705449104309,
      "learning_rate": 0.00019333244432591013,
      "loss": 0.8263,
      "step": 1255
    },
    {
      "epoch": 0.10048,
      "grad_norm": 0.2615313231945038,
      "learning_rate": 0.00019332711028137084,
      "loss": 0.6727,
      "step": 1256
    },
    {
      "epoch": 0.10056,
      "grad_norm": 0.4004870355129242,
      "learning_rate": 0.00019332177623683158,
      "loss": 1.0733,
      "step": 1257
    },
    {
      "epoch": 0.10064,
      "grad_norm": 0.41295361518859863,
      "learning_rate": 0.00019331644219229232,
      "loss": 1.1225,
      "step": 1258
    },
    {
      "epoch": 0.10072,
      "grad_norm": 0.34320464730262756,
      "learning_rate": 0.00019331110814775303,
      "loss": 1.0485,
      "step": 1259
    },
    {
      "epoch": 0.1008,
      "grad_norm": 0.3965831398963928,
      "learning_rate": 0.00019330577410321377,
      "loss": 0.5245,
      "step": 1260
    },
    {
      "epoch": 0.10088,
      "grad_norm": 0.3597995340824127,
      "learning_rate": 0.0001933004400586745,
      "loss": 0.5974,
      "step": 1261
    },
    {
      "epoch": 0.10096,
      "grad_norm": 0.2654712200164795,
      "learning_rate": 0.00019329510601413523,
      "loss": 0.5498,
      "step": 1262
    },
    {
      "epoch": 0.10104,
      "grad_norm": 0.3343416750431061,
      "learning_rate": 0.00019328977196959594,
      "loss": 0.6739,
      "step": 1263
    },
    {
      "epoch": 0.10112,
      "grad_norm": 0.27530837059020996,
      "learning_rate": 0.00019328443792505668,
      "loss": 0.8026,
      "step": 1264
    },
    {
      "epoch": 0.1012,
      "grad_norm": 0.278952419757843,
      "learning_rate": 0.00019327910388051742,
      "loss": 0.8095,
      "step": 1265
    },
    {
      "epoch": 0.10128,
      "grad_norm": 0.390651136636734,
      "learning_rate": 0.00019327376983597813,
      "loss": 0.6,
      "step": 1266
    },
    {
      "epoch": 0.10136,
      "grad_norm": 0.29351240396499634,
      "learning_rate": 0.00019326843579143887,
      "loss": 0.7866,
      "step": 1267
    },
    {
      "epoch": 0.10144,
      "grad_norm": 0.2586594521999359,
      "learning_rate": 0.00019326310174689958,
      "loss": 0.7489,
      "step": 1268
    },
    {
      "epoch": 0.10152,
      "grad_norm": 0.24576646089553833,
      "learning_rate": 0.00019325776770236032,
      "loss": 0.5341,
      "step": 1269
    },
    {
      "epoch": 0.1016,
      "grad_norm": 0.32970964908599854,
      "learning_rate": 0.00019325243365782104,
      "loss": 0.877,
      "step": 1270
    },
    {
      "epoch": 0.10168,
      "grad_norm": 0.33761999011039734,
      "learning_rate": 0.00019324709961328178,
      "loss": 0.9844,
      "step": 1271
    },
    {
      "epoch": 0.10176,
      "grad_norm": 0.25335779786109924,
      "learning_rate": 0.00019324176556874252,
      "loss": 0.7106,
      "step": 1272
    },
    {
      "epoch": 0.10184,
      "grad_norm": 0.35025545954704285,
      "learning_rate": 0.00019323643152420323,
      "loss": 0.9225,
      "step": 1273
    },
    {
      "epoch": 0.10192,
      "grad_norm": 0.3046474754810333,
      "learning_rate": 0.00019323109747966397,
      "loss": 0.9635,
      "step": 1274
    },
    {
      "epoch": 0.102,
      "grad_norm": 0.3022192120552063,
      "learning_rate": 0.00019322576343512468,
      "loss": 1.1771,
      "step": 1275
    },
    {
      "epoch": 0.10208,
      "grad_norm": 0.28335073590278625,
      "learning_rate": 0.00019322042939058542,
      "loss": 0.5804,
      "step": 1276
    },
    {
      "epoch": 0.10216,
      "grad_norm": 0.35936230421066284,
      "learning_rate": 0.00019321509534604613,
      "loss": 0.7272,
      "step": 1277
    },
    {
      "epoch": 0.10224,
      "grad_norm": 0.32797858119010925,
      "learning_rate": 0.00019320976130150687,
      "loss": 0.5744,
      "step": 1278
    },
    {
      "epoch": 0.10232,
      "grad_norm": 0.4411068260669708,
      "learning_rate": 0.00019320442725696761,
      "loss": 1.1365,
      "step": 1279
    },
    {
      "epoch": 0.1024,
      "grad_norm": 0.40273764729499817,
      "learning_rate": 0.00019319909321242833,
      "loss": 0.8093,
      "step": 1280
    },
    {
      "epoch": 0.10248,
      "grad_norm": 0.33689627051353455,
      "learning_rate": 0.00019319375916788907,
      "loss": 0.7918,
      "step": 1281
    },
    {
      "epoch": 0.10256,
      "grad_norm": 0.33577632904052734,
      "learning_rate": 0.00019318842512334978,
      "loss": 0.7448,
      "step": 1282
    },
    {
      "epoch": 0.10264,
      "grad_norm": 0.31028997898101807,
      "learning_rate": 0.00019318309107881052,
      "loss": 0.6326,
      "step": 1283
    },
    {
      "epoch": 0.10272,
      "grad_norm": 0.28916075825691223,
      "learning_rate": 0.00019317775703427126,
      "loss": 0.8269,
      "step": 1284
    },
    {
      "epoch": 0.1028,
      "grad_norm": 0.2531578242778778,
      "learning_rate": 0.00019317242298973197,
      "loss": 0.7759,
      "step": 1285
    },
    {
      "epoch": 0.10288,
      "grad_norm": 0.33876535296440125,
      "learning_rate": 0.0001931670889451927,
      "loss": 0.9763,
      "step": 1286
    },
    {
      "epoch": 0.10296,
      "grad_norm": 0.4798290431499481,
      "learning_rate": 0.00019316175490065342,
      "loss": 0.9898,
      "step": 1287
    },
    {
      "epoch": 0.10304,
      "grad_norm": 0.30122628808021545,
      "learning_rate": 0.00019315642085611416,
      "loss": 1.0004,
      "step": 1288
    },
    {
      "epoch": 0.10312,
      "grad_norm": 0.32538190484046936,
      "learning_rate": 0.00019315108681157488,
      "loss": 0.7329,
      "step": 1289
    },
    {
      "epoch": 0.1032,
      "grad_norm": 0.2304820567369461,
      "learning_rate": 0.00019314575276703562,
      "loss": 0.6882,
      "step": 1290
    },
    {
      "epoch": 0.10328,
      "grad_norm": 0.39951711893081665,
      "learning_rate": 0.00019314041872249636,
      "loss": 0.9952,
      "step": 1291
    },
    {
      "epoch": 0.10336,
      "grad_norm": 0.20414771139621735,
      "learning_rate": 0.00019313508467795707,
      "loss": 0.7958,
      "step": 1292
    },
    {
      "epoch": 0.10344,
      "grad_norm": 0.267992228269577,
      "learning_rate": 0.0001931297506334178,
      "loss": 0.9521,
      "step": 1293
    },
    {
      "epoch": 0.10352,
      "grad_norm": 0.33622851967811584,
      "learning_rate": 0.00019312441658887852,
      "loss": 0.8131,
      "step": 1294
    },
    {
      "epoch": 0.1036,
      "grad_norm": 0.2816378176212311,
      "learning_rate": 0.00019311908254433926,
      "loss": 0.6879,
      "step": 1295
    },
    {
      "epoch": 0.10368,
      "grad_norm": 0.32824617624282837,
      "learning_rate": 0.00019311374849979997,
      "loss": 0.7346,
      "step": 1296
    },
    {
      "epoch": 0.10376,
      "grad_norm": 0.329677551984787,
      "learning_rate": 0.00019310841445526071,
      "loss": 0.7944,
      "step": 1297
    },
    {
      "epoch": 0.10384,
      "grad_norm": 0.3757133185863495,
      "learning_rate": 0.00019310308041072145,
      "loss": 1.0257,
      "step": 1298
    },
    {
      "epoch": 0.10392,
      "grad_norm": 0.37905386090278625,
      "learning_rate": 0.00019309774636618217,
      "loss": 0.9694,
      "step": 1299
    },
    {
      "epoch": 0.104,
      "grad_norm": 0.4168866276741028,
      "learning_rate": 0.0001930924123216429,
      "loss": 1.3824,
      "step": 1300
    },
    {
      "epoch": 0.10408,
      "grad_norm": 0.29689618945121765,
      "learning_rate": 0.00019308707827710362,
      "loss": 0.8269,
      "step": 1301
    },
    {
      "epoch": 0.10416,
      "grad_norm": 0.4723348319530487,
      "learning_rate": 0.00019308174423256436,
      "loss": 1.2672,
      "step": 1302
    },
    {
      "epoch": 0.10424,
      "grad_norm": 0.38648900389671326,
      "learning_rate": 0.00019307641018802507,
      "loss": 0.9022,
      "step": 1303
    },
    {
      "epoch": 0.10432,
      "grad_norm": 0.3906022608280182,
      "learning_rate": 0.0001930710761434858,
      "loss": 0.9422,
      "step": 1304
    },
    {
      "epoch": 0.1044,
      "grad_norm": 0.2867429554462433,
      "learning_rate": 0.00019306574209894655,
      "loss": 0.6321,
      "step": 1305
    },
    {
      "epoch": 0.10448,
      "grad_norm": 0.37091323733329773,
      "learning_rate": 0.00019306040805440726,
      "loss": 0.883,
      "step": 1306
    },
    {
      "epoch": 0.10456,
      "grad_norm": 0.33581867814064026,
      "learning_rate": 0.000193055074009868,
      "loss": 0.8275,
      "step": 1307
    },
    {
      "epoch": 0.10464,
      "grad_norm": 0.3013049364089966,
      "learning_rate": 0.00019304973996532872,
      "loss": 0.9761,
      "step": 1308
    },
    {
      "epoch": 0.10472,
      "grad_norm": 0.35363101959228516,
      "learning_rate": 0.00019304440592078946,
      "loss": 0.8673,
      "step": 1309
    },
    {
      "epoch": 0.1048,
      "grad_norm": 0.36335036158561707,
      "learning_rate": 0.00019303907187625017,
      "loss": 0.7227,
      "step": 1310
    },
    {
      "epoch": 0.10488,
      "grad_norm": 0.31091421842575073,
      "learning_rate": 0.0001930337378317109,
      "loss": 0.6705,
      "step": 1311
    },
    {
      "epoch": 0.10496,
      "grad_norm": 0.34670940041542053,
      "learning_rate": 0.00019302840378717165,
      "loss": 0.7187,
      "step": 1312
    },
    {
      "epoch": 0.10504,
      "grad_norm": 0.3066520094871521,
      "learning_rate": 0.00019302306974263236,
      "loss": 0.6585,
      "step": 1313
    },
    {
      "epoch": 0.10512,
      "grad_norm": 0.3910972476005554,
      "learning_rate": 0.0001930177356980931,
      "loss": 0.7612,
      "step": 1314
    },
    {
      "epoch": 0.1052,
      "grad_norm": 0.31429600715637207,
      "learning_rate": 0.00019301240165355382,
      "loss": 0.6552,
      "step": 1315
    },
    {
      "epoch": 0.10528,
      "grad_norm": 0.3607989549636841,
      "learning_rate": 0.00019300706760901455,
      "loss": 1.0747,
      "step": 1316
    },
    {
      "epoch": 0.10536,
      "grad_norm": 0.26043468713760376,
      "learning_rate": 0.00019300173356447527,
      "loss": 0.6337,
      "step": 1317
    },
    {
      "epoch": 0.10544,
      "grad_norm": 0.3368017375469208,
      "learning_rate": 0.000192996399519936,
      "loss": 1.1238,
      "step": 1318
    },
    {
      "epoch": 0.10552,
      "grad_norm": 0.3216916024684906,
      "learning_rate": 0.00019299106547539675,
      "loss": 0.7862,
      "step": 1319
    },
    {
      "epoch": 0.1056,
      "grad_norm": 0.298048734664917,
      "learning_rate": 0.00019298573143085746,
      "loss": 1.1528,
      "step": 1320
    },
    {
      "epoch": 0.10568,
      "grad_norm": 0.29329273104667664,
      "learning_rate": 0.0001929803973863182,
      "loss": 1.0639,
      "step": 1321
    },
    {
      "epoch": 0.10576,
      "grad_norm": 0.37426382303237915,
      "learning_rate": 0.0001929750633417789,
      "loss": 0.7694,
      "step": 1322
    },
    {
      "epoch": 0.10584,
      "grad_norm": 0.3404468595981598,
      "learning_rate": 0.00019296972929723965,
      "loss": 0.9626,
      "step": 1323
    },
    {
      "epoch": 0.10592,
      "grad_norm": 0.282709002494812,
      "learning_rate": 0.00019296439525270037,
      "loss": 0.6123,
      "step": 1324
    },
    {
      "epoch": 0.106,
      "grad_norm": 0.3550357222557068,
      "learning_rate": 0.0001929590612081611,
      "loss": 0.6524,
      "step": 1325
    },
    {
      "epoch": 0.10608,
      "grad_norm": 0.3197971284389496,
      "learning_rate": 0.00019295372716362185,
      "loss": 0.9464,
      "step": 1326
    },
    {
      "epoch": 0.10616,
      "grad_norm": 0.4334516227245331,
      "learning_rate": 0.00019294839311908256,
      "loss": 1.066,
      "step": 1327
    },
    {
      "epoch": 0.10624,
      "grad_norm": 0.4135366976261139,
      "learning_rate": 0.0001929430590745433,
      "loss": 0.7887,
      "step": 1328
    },
    {
      "epoch": 0.10632,
      "grad_norm": 0.3165856897830963,
      "learning_rate": 0.000192937725030004,
      "loss": 0.8335,
      "step": 1329
    },
    {
      "epoch": 0.1064,
      "grad_norm": 0.37848198413848877,
      "learning_rate": 0.00019293239098546475,
      "loss": 0.7125,
      "step": 1330
    },
    {
      "epoch": 0.10648,
      "grad_norm": 0.30022597312927246,
      "learning_rate": 0.0001929270569409255,
      "loss": 0.9719,
      "step": 1331
    },
    {
      "epoch": 0.10656,
      "grad_norm": 0.26142483949661255,
      "learning_rate": 0.0001929217228963862,
      "loss": 1.1113,
      "step": 1332
    },
    {
      "epoch": 0.10664,
      "grad_norm": 0.42801856994628906,
      "learning_rate": 0.00019291638885184694,
      "loss": 0.8377,
      "step": 1333
    },
    {
      "epoch": 0.10672,
      "grad_norm": 0.31957846879959106,
      "learning_rate": 0.00019291105480730766,
      "loss": 0.7279,
      "step": 1334
    },
    {
      "epoch": 0.1068,
      "grad_norm": 0.3604944944381714,
      "learning_rate": 0.0001929057207627684,
      "loss": 1.0002,
      "step": 1335
    },
    {
      "epoch": 0.10688,
      "grad_norm": 0.3943299353122711,
      "learning_rate": 0.0001929003867182291,
      "loss": 0.915,
      "step": 1336
    },
    {
      "epoch": 0.10696,
      "grad_norm": 0.31512144207954407,
      "learning_rate": 0.00019289505267368985,
      "loss": 0.8275,
      "step": 1337
    },
    {
      "epoch": 0.10704,
      "grad_norm": 0.3361000716686249,
      "learning_rate": 0.00019288971862915056,
      "loss": 0.79,
      "step": 1338
    },
    {
      "epoch": 0.10712,
      "grad_norm": 0.40831395983695984,
      "learning_rate": 0.0001928843845846113,
      "loss": 0.8331,
      "step": 1339
    },
    {
      "epoch": 0.1072,
      "grad_norm": 0.35370567440986633,
      "learning_rate": 0.000192879050540072,
      "loss": 0.705,
      "step": 1340
    },
    {
      "epoch": 0.10728,
      "grad_norm": 0.3816257417201996,
      "learning_rate": 0.00019287371649553275,
      "loss": 0.8272,
      "step": 1341
    },
    {
      "epoch": 0.10736,
      "grad_norm": 0.2632853388786316,
      "learning_rate": 0.00019286838245099347,
      "loss": 0.508,
      "step": 1342
    },
    {
      "epoch": 0.10744,
      "grad_norm": 0.3445916473865509,
      "learning_rate": 0.0001928630484064542,
      "loss": 1.0091,
      "step": 1343
    },
    {
      "epoch": 0.10752,
      "grad_norm": 0.3210044801235199,
      "learning_rate": 0.00019285771436191492,
      "loss": 0.9367,
      "step": 1344
    },
    {
      "epoch": 0.1076,
      "grad_norm": 0.35676780343055725,
      "learning_rate": 0.00019285238031737566,
      "loss": 0.8371,
      "step": 1345
    },
    {
      "epoch": 0.10768,
      "grad_norm": 0.34507179260253906,
      "learning_rate": 0.0001928470462728364,
      "loss": 0.947,
      "step": 1346
    },
    {
      "epoch": 0.10776,
      "grad_norm": 0.3141016960144043,
      "learning_rate": 0.0001928417122282971,
      "loss": 0.9781,
      "step": 1347
    },
    {
      "epoch": 0.10784,
      "grad_norm": 0.2611529231071472,
      "learning_rate": 0.00019283637818375785,
      "loss": 0.963,
      "step": 1348
    },
    {
      "epoch": 0.10792,
      "grad_norm": 0.32910478115081787,
      "learning_rate": 0.00019283104413921856,
      "loss": 0.8743,
      "step": 1349
    },
    {
      "epoch": 0.108,
      "grad_norm": 0.31884780526161194,
      "learning_rate": 0.0001928257100946793,
      "loss": 0.8139,
      "step": 1350
    },
    {
      "epoch": 0.10808,
      "grad_norm": 0.34672707319259644,
      "learning_rate": 0.00019282037605014002,
      "loss": 0.8826,
      "step": 1351
    },
    {
      "epoch": 0.10816,
      "grad_norm": 0.3336324393749237,
      "learning_rate": 0.00019281504200560076,
      "loss": 0.8905,
      "step": 1352
    },
    {
      "epoch": 0.10824,
      "grad_norm": 0.3592979907989502,
      "learning_rate": 0.00019280970796106147,
      "loss": 0.8273,
      "step": 1353
    },
    {
      "epoch": 0.10832,
      "grad_norm": 0.4200979769229889,
      "learning_rate": 0.0001928043739165222,
      "loss": 1.3103,
      "step": 1354
    },
    {
      "epoch": 0.1084,
      "grad_norm": 0.28283676505088806,
      "learning_rate": 0.00019279903987198292,
      "loss": 0.927,
      "step": 1355
    },
    {
      "epoch": 0.10848,
      "grad_norm": 0.34802672266960144,
      "learning_rate": 0.00019279370582744366,
      "loss": 0.6636,
      "step": 1356
    },
    {
      "epoch": 0.10856,
      "grad_norm": 0.3257891833782196,
      "learning_rate": 0.00019278837178290437,
      "loss": 0.7216,
      "step": 1357
    },
    {
      "epoch": 0.10864,
      "grad_norm": 0.38097476959228516,
      "learning_rate": 0.00019278303773836511,
      "loss": 0.9645,
      "step": 1358
    },
    {
      "epoch": 0.10872,
      "grad_norm": 0.2417309433221817,
      "learning_rate": 0.00019277770369382585,
      "loss": 0.4943,
      "step": 1359
    },
    {
      "epoch": 0.1088,
      "grad_norm": 0.3734184205532074,
      "learning_rate": 0.00019277236964928657,
      "loss": 0.8305,
      "step": 1360
    },
    {
      "epoch": 0.10888,
      "grad_norm": 0.3353022336959839,
      "learning_rate": 0.0001927670356047473,
      "loss": 0.7626,
      "step": 1361
    },
    {
      "epoch": 0.10896,
      "grad_norm": 0.42627066373825073,
      "learning_rate": 0.00019276170156020802,
      "loss": 0.8194,
      "step": 1362
    },
    {
      "epoch": 0.10904,
      "grad_norm": 0.3256186544895172,
      "learning_rate": 0.00019275636751566876,
      "loss": 0.7407,
      "step": 1363
    },
    {
      "epoch": 0.10912,
      "grad_norm": 0.2784293293952942,
      "learning_rate": 0.00019275103347112947,
      "loss": 0.6251,
      "step": 1364
    },
    {
      "epoch": 0.1092,
      "grad_norm": 0.31560245156288147,
      "learning_rate": 0.0001927456994265902,
      "loss": 0.6537,
      "step": 1365
    },
    {
      "epoch": 0.10928,
      "grad_norm": 0.26411375403404236,
      "learning_rate": 0.00019274036538205095,
      "loss": 0.6611,
      "step": 1366
    },
    {
      "epoch": 0.10936,
      "grad_norm": 0.28556978702545166,
      "learning_rate": 0.00019273503133751166,
      "loss": 0.6132,
      "step": 1367
    },
    {
      "epoch": 0.10944,
      "grad_norm": 0.2817491888999939,
      "learning_rate": 0.0001927296972929724,
      "loss": 0.8987,
      "step": 1368
    },
    {
      "epoch": 0.10952,
      "grad_norm": 0.3360902667045593,
      "learning_rate": 0.00019272436324843312,
      "loss": 0.8484,
      "step": 1369
    },
    {
      "epoch": 0.1096,
      "grad_norm": 0.3822844624519348,
      "learning_rate": 0.00019271902920389386,
      "loss": 0.9226,
      "step": 1370
    },
    {
      "epoch": 0.10968,
      "grad_norm": 0.2879643738269806,
      "learning_rate": 0.00019271369515935457,
      "loss": 0.6502,
      "step": 1371
    },
    {
      "epoch": 0.10976,
      "grad_norm": 0.3596370816230774,
      "learning_rate": 0.0001927083611148153,
      "loss": 0.6984,
      "step": 1372
    },
    {
      "epoch": 0.10984,
      "grad_norm": 0.352327436208725,
      "learning_rate": 0.00019270302707027605,
      "loss": 0.6141,
      "step": 1373
    },
    {
      "epoch": 0.10992,
      "grad_norm": 0.35185718536376953,
      "learning_rate": 0.00019269769302573676,
      "loss": 0.6282,
      "step": 1374
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.3075934648513794,
      "learning_rate": 0.0001926923589811975,
      "loss": 0.8103,
      "step": 1375
    },
    {
      "epoch": 0.11008,
      "grad_norm": 0.3414117097854614,
      "learning_rate": 0.00019268702493665821,
      "loss": 0.5101,
      "step": 1376
    },
    {
      "epoch": 0.11016,
      "grad_norm": 0.2172021120786667,
      "learning_rate": 0.00019268169089211895,
      "loss": 0.4961,
      "step": 1377
    },
    {
      "epoch": 0.11024,
      "grad_norm": 0.293266624212265,
      "learning_rate": 0.0001926763568475797,
      "loss": 0.535,
      "step": 1378
    },
    {
      "epoch": 0.11032,
      "grad_norm": 0.33169451355934143,
      "learning_rate": 0.0001926710228030404,
      "loss": 0.7186,
      "step": 1379
    },
    {
      "epoch": 0.1104,
      "grad_norm": 0.3222217857837677,
      "learning_rate": 0.00019266568875850115,
      "loss": 1.0119,
      "step": 1380
    },
    {
      "epoch": 0.11048,
      "grad_norm": 0.27366307377815247,
      "learning_rate": 0.00019266035471396186,
      "loss": 0.8976,
      "step": 1381
    },
    {
      "epoch": 0.11056,
      "grad_norm": 0.2485465407371521,
      "learning_rate": 0.0001926550206694226,
      "loss": 0.9389,
      "step": 1382
    },
    {
      "epoch": 0.11064,
      "grad_norm": 0.2762819230556488,
      "learning_rate": 0.0001926496866248833,
      "loss": 0.5956,
      "step": 1383
    },
    {
      "epoch": 0.11072,
      "grad_norm": 0.2673598825931549,
      "learning_rate": 0.00019264435258034405,
      "loss": 0.8539,
      "step": 1384
    },
    {
      "epoch": 0.1108,
      "grad_norm": 0.3523796498775482,
      "learning_rate": 0.0001926390185358048,
      "loss": 0.9404,
      "step": 1385
    },
    {
      "epoch": 0.11088,
      "grad_norm": 0.32300156354904175,
      "learning_rate": 0.0001926336844912655,
      "loss": 1.0258,
      "step": 1386
    },
    {
      "epoch": 0.11096,
      "grad_norm": 0.3284301161766052,
      "learning_rate": 0.00019262835044672624,
      "loss": 0.9525,
      "step": 1387
    },
    {
      "epoch": 0.11104,
      "grad_norm": 0.334248423576355,
      "learning_rate": 0.00019262301640218696,
      "loss": 1.0866,
      "step": 1388
    },
    {
      "epoch": 0.11112,
      "grad_norm": 0.3848603069782257,
      "learning_rate": 0.0001926176823576477,
      "loss": 0.876,
      "step": 1389
    },
    {
      "epoch": 0.1112,
      "grad_norm": 0.3209723234176636,
      "learning_rate": 0.0001926123483131084,
      "loss": 0.9617,
      "step": 1390
    },
    {
      "epoch": 0.11128,
      "grad_norm": 0.32687509059906006,
      "learning_rate": 0.00019260701426856915,
      "loss": 0.7514,
      "step": 1391
    },
    {
      "epoch": 0.11136,
      "grad_norm": 0.21836909651756287,
      "learning_rate": 0.0001926016802240299,
      "loss": 0.3551,
      "step": 1392
    },
    {
      "epoch": 0.11144,
      "grad_norm": 0.3638189733028412,
      "learning_rate": 0.0001925963461794906,
      "loss": 0.8206,
      "step": 1393
    },
    {
      "epoch": 0.11152,
      "grad_norm": 0.37653085589408875,
      "learning_rate": 0.00019259101213495134,
      "loss": 1.318,
      "step": 1394
    },
    {
      "epoch": 0.1116,
      "grad_norm": 0.30772241950035095,
      "learning_rate": 0.00019258567809041205,
      "loss": 0.6733,
      "step": 1395
    },
    {
      "epoch": 0.11168,
      "grad_norm": 0.3054928779602051,
      "learning_rate": 0.0001925803440458728,
      "loss": 0.781,
      "step": 1396
    },
    {
      "epoch": 0.11176,
      "grad_norm": 0.31069323420524597,
      "learning_rate": 0.0001925750100013335,
      "loss": 0.6589,
      "step": 1397
    },
    {
      "epoch": 0.11184,
      "grad_norm": 0.32952672243118286,
      "learning_rate": 0.00019256967595679425,
      "loss": 0.7029,
      "step": 1398
    },
    {
      "epoch": 0.11192,
      "grad_norm": 0.31875181198120117,
      "learning_rate": 0.000192564341912255,
      "loss": 0.509,
      "step": 1399
    },
    {
      "epoch": 0.112,
      "grad_norm": 0.42021724581718445,
      "learning_rate": 0.0001925590078677157,
      "loss": 0.9978,
      "step": 1400
    },
    {
      "epoch": 0.11208,
      "grad_norm": 0.3837888240814209,
      "learning_rate": 0.00019255367382317644,
      "loss": 0.913,
      "step": 1401
    },
    {
      "epoch": 0.11216,
      "grad_norm": 0.3644677698612213,
      "learning_rate": 0.00019254833977863715,
      "loss": 1.2243,
      "step": 1402
    },
    {
      "epoch": 0.11224,
      "grad_norm": 0.24492575228214264,
      "learning_rate": 0.0001925430057340979,
      "loss": 0.4688,
      "step": 1403
    },
    {
      "epoch": 0.11232,
      "grad_norm": 0.33833643794059753,
      "learning_rate": 0.0001925376716895586,
      "loss": 0.8554,
      "step": 1404
    },
    {
      "epoch": 0.1124,
      "grad_norm": 0.3217196762561798,
      "learning_rate": 0.00019253233764501934,
      "loss": 0.8748,
      "step": 1405
    },
    {
      "epoch": 0.11248,
      "grad_norm": 0.342327743768692,
      "learning_rate": 0.00019252700360048008,
      "loss": 0.7505,
      "step": 1406
    },
    {
      "epoch": 0.11256,
      "grad_norm": 0.2574639320373535,
      "learning_rate": 0.0001925216695559408,
      "loss": 0.5887,
      "step": 1407
    },
    {
      "epoch": 0.11264,
      "grad_norm": 0.36662808060646057,
      "learning_rate": 0.00019251633551140154,
      "loss": 0.9562,
      "step": 1408
    },
    {
      "epoch": 0.11272,
      "grad_norm": 0.36799609661102295,
      "learning_rate": 0.00019251100146686225,
      "loss": 0.7667,
      "step": 1409
    },
    {
      "epoch": 0.1128,
      "grad_norm": 0.42463740706443787,
      "learning_rate": 0.000192505667422323,
      "loss": 0.8774,
      "step": 1410
    },
    {
      "epoch": 0.11288,
      "grad_norm": 0.24362878501415253,
      "learning_rate": 0.0001925003333777837,
      "loss": 0.5287,
      "step": 1411
    },
    {
      "epoch": 0.11296,
      "grad_norm": 0.3081432282924652,
      "learning_rate": 0.00019249499933324444,
      "loss": 1.1865,
      "step": 1412
    },
    {
      "epoch": 0.11304,
      "grad_norm": 0.4288824796676636,
      "learning_rate": 0.00019248966528870518,
      "loss": 0.7275,
      "step": 1413
    },
    {
      "epoch": 0.11312,
      "grad_norm": 0.31259047985076904,
      "learning_rate": 0.0001924843312441659,
      "loss": 0.7356,
      "step": 1414
    },
    {
      "epoch": 0.1132,
      "grad_norm": 0.3191147446632385,
      "learning_rate": 0.00019247899719962663,
      "loss": 0.7427,
      "step": 1415
    },
    {
      "epoch": 0.11328,
      "grad_norm": 0.3281887471675873,
      "learning_rate": 0.00019247366315508735,
      "loss": 0.7908,
      "step": 1416
    },
    {
      "epoch": 0.11336,
      "grad_norm": 0.38680386543273926,
      "learning_rate": 0.0001924683291105481,
      "loss": 0.9749,
      "step": 1417
    },
    {
      "epoch": 0.11344,
      "grad_norm": 0.28004220128059387,
      "learning_rate": 0.0001924629950660088,
      "loss": 0.8823,
      "step": 1418
    },
    {
      "epoch": 0.11352,
      "grad_norm": 0.3992859125137329,
      "learning_rate": 0.00019245766102146954,
      "loss": 0.939,
      "step": 1419
    },
    {
      "epoch": 0.1136,
      "grad_norm": 0.43828871846199036,
      "learning_rate": 0.00019245232697693028,
      "loss": 0.6357,
      "step": 1420
    },
    {
      "epoch": 0.11368,
      "grad_norm": 0.36938154697418213,
      "learning_rate": 0.000192446992932391,
      "loss": 0.953,
      "step": 1421
    },
    {
      "epoch": 0.11376,
      "grad_norm": 0.29814887046813965,
      "learning_rate": 0.00019244165888785173,
      "loss": 0.9105,
      "step": 1422
    },
    {
      "epoch": 0.11384,
      "grad_norm": 0.3386653661727905,
      "learning_rate": 0.00019243632484331244,
      "loss": 0.663,
      "step": 1423
    },
    {
      "epoch": 0.11392,
      "grad_norm": 0.3338397443294525,
      "learning_rate": 0.00019243099079877318,
      "loss": 0.5485,
      "step": 1424
    },
    {
      "epoch": 0.114,
      "grad_norm": 0.37301692366600037,
      "learning_rate": 0.0001924256567542339,
      "loss": 0.7146,
      "step": 1425
    },
    {
      "epoch": 0.11408,
      "grad_norm": 0.31537604331970215,
      "learning_rate": 0.00019242032270969464,
      "loss": 1.0336,
      "step": 1426
    },
    {
      "epoch": 0.11416,
      "grad_norm": 0.4576460123062134,
      "learning_rate": 0.00019241498866515538,
      "loss": 0.843,
      "step": 1427
    },
    {
      "epoch": 0.11424,
      "grad_norm": 0.28500914573669434,
      "learning_rate": 0.0001924096546206161,
      "loss": 0.718,
      "step": 1428
    },
    {
      "epoch": 0.11432,
      "grad_norm": 0.3399083912372589,
      "learning_rate": 0.00019240432057607683,
      "loss": 0.5594,
      "step": 1429
    },
    {
      "epoch": 0.1144,
      "grad_norm": 0.31567803025245667,
      "learning_rate": 0.00019239898653153754,
      "loss": 0.7163,
      "step": 1430
    },
    {
      "epoch": 0.11448,
      "grad_norm": 0.38314008712768555,
      "learning_rate": 0.00019239365248699828,
      "loss": 0.9987,
      "step": 1431
    },
    {
      "epoch": 0.11456,
      "grad_norm": 0.2800278961658478,
      "learning_rate": 0.00019238831844245902,
      "loss": 0.5887,
      "step": 1432
    },
    {
      "epoch": 0.11464,
      "grad_norm": 0.35132211446762085,
      "learning_rate": 0.00019238298439791973,
      "loss": 0.9892,
      "step": 1433
    },
    {
      "epoch": 0.11472,
      "grad_norm": 0.27510228753089905,
      "learning_rate": 0.00019237765035338047,
      "loss": 1.1158,
      "step": 1434
    },
    {
      "epoch": 0.1148,
      "grad_norm": 0.35061144828796387,
      "learning_rate": 0.0001923723163088412,
      "loss": 0.7451,
      "step": 1435
    },
    {
      "epoch": 0.11488,
      "grad_norm": 0.3235014081001282,
      "learning_rate": 0.00019236698226430193,
      "loss": 0.9789,
      "step": 1436
    },
    {
      "epoch": 0.11496,
      "grad_norm": 0.35369691252708435,
      "learning_rate": 0.00019236164821976264,
      "loss": 0.7309,
      "step": 1437
    },
    {
      "epoch": 0.11504,
      "grad_norm": 0.34546539187431335,
      "learning_rate": 0.00019235631417522338,
      "loss": 0.7801,
      "step": 1438
    },
    {
      "epoch": 0.11512,
      "grad_norm": 0.47057536244392395,
      "learning_rate": 0.00019235098013068412,
      "loss": 0.7162,
      "step": 1439
    },
    {
      "epoch": 0.1152,
      "grad_norm": 0.28193041682243347,
      "learning_rate": 0.00019234564608614483,
      "loss": 0.8911,
      "step": 1440
    },
    {
      "epoch": 0.11528,
      "grad_norm": 0.3137516975402832,
      "learning_rate": 0.00019234031204160557,
      "loss": 0.8649,
      "step": 1441
    },
    {
      "epoch": 0.11536,
      "grad_norm": 0.4020451009273529,
      "learning_rate": 0.00019233497799706629,
      "loss": 0.6934,
      "step": 1442
    },
    {
      "epoch": 0.11544,
      "grad_norm": 0.35728919506073,
      "learning_rate": 0.00019232964395252702,
      "loss": 0.8814,
      "step": 1443
    },
    {
      "epoch": 0.11552,
      "grad_norm": 0.3041609227657318,
      "learning_rate": 0.00019232430990798774,
      "loss": 0.9213,
      "step": 1444
    },
    {
      "epoch": 0.1156,
      "grad_norm": 0.32251372933387756,
      "learning_rate": 0.00019231897586344848,
      "loss": 0.9764,
      "step": 1445
    },
    {
      "epoch": 0.11568,
      "grad_norm": 0.3499317467212677,
      "learning_rate": 0.00019231364181890922,
      "loss": 0.7349,
      "step": 1446
    },
    {
      "epoch": 0.11576,
      "grad_norm": 0.29676130414009094,
      "learning_rate": 0.00019230830777436993,
      "loss": 0.5433,
      "step": 1447
    },
    {
      "epoch": 0.11584,
      "grad_norm": 0.3062427043914795,
      "learning_rate": 0.00019230297372983067,
      "loss": 1.0196,
      "step": 1448
    },
    {
      "epoch": 0.11592,
      "grad_norm": 0.2997651696205139,
      "learning_rate": 0.00019229763968529138,
      "loss": 0.9469,
      "step": 1449
    },
    {
      "epoch": 0.116,
      "grad_norm": 0.3307505249977112,
      "learning_rate": 0.00019229230564075212,
      "loss": 0.5906,
      "step": 1450
    },
    {
      "epoch": 0.11608,
      "grad_norm": 0.34459036588668823,
      "learning_rate": 0.00019228697159621284,
      "loss": 0.4495,
      "step": 1451
    },
    {
      "epoch": 0.11616,
      "grad_norm": 0.32012608647346497,
      "learning_rate": 0.00019228163755167358,
      "loss": 0.794,
      "step": 1452
    },
    {
      "epoch": 0.11624,
      "grad_norm": 0.3219805657863617,
      "learning_rate": 0.00019227630350713431,
      "loss": 0.7137,
      "step": 1453
    },
    {
      "epoch": 0.11632,
      "grad_norm": 0.30727991461753845,
      "learning_rate": 0.00019227096946259503,
      "loss": 0.7157,
      "step": 1454
    },
    {
      "epoch": 0.1164,
      "grad_norm": 0.47359564900398254,
      "learning_rate": 0.00019226563541805577,
      "loss": 0.9339,
      "step": 1455
    },
    {
      "epoch": 0.11648,
      "grad_norm": 0.344028502702713,
      "learning_rate": 0.00019226030137351648,
      "loss": 0.8193,
      "step": 1456
    },
    {
      "epoch": 0.11656,
      "grad_norm": 0.38787493109703064,
      "learning_rate": 0.00019225496732897722,
      "loss": 0.9365,
      "step": 1457
    },
    {
      "epoch": 0.11664,
      "grad_norm": 0.33064332604408264,
      "learning_rate": 0.00019224963328443793,
      "loss": 0.6547,
      "step": 1458
    },
    {
      "epoch": 0.11672,
      "grad_norm": 0.32378584146499634,
      "learning_rate": 0.00019224429923989867,
      "loss": 0.6372,
      "step": 1459
    },
    {
      "epoch": 0.1168,
      "grad_norm": 0.3443049192428589,
      "learning_rate": 0.0001922389651953594,
      "loss": 0.8227,
      "step": 1460
    },
    {
      "epoch": 0.11688,
      "grad_norm": 0.38045820593833923,
      "learning_rate": 0.00019223363115082013,
      "loss": 0.8772,
      "step": 1461
    },
    {
      "epoch": 0.11696,
      "grad_norm": 0.4223640561103821,
      "learning_rate": 0.00019222829710628087,
      "loss": 0.8738,
      "step": 1462
    },
    {
      "epoch": 0.11704,
      "grad_norm": 0.3176477253437042,
      "learning_rate": 0.00019222296306174158,
      "loss": 0.4743,
      "step": 1463
    },
    {
      "epoch": 0.11712,
      "grad_norm": 0.3024274408817291,
      "learning_rate": 0.00019221762901720232,
      "loss": 0.6679,
      "step": 1464
    },
    {
      "epoch": 0.1172,
      "grad_norm": 0.30713382363319397,
      "learning_rate": 0.00019221229497266303,
      "loss": 0.9743,
      "step": 1465
    },
    {
      "epoch": 0.11728,
      "grad_norm": 0.36440086364746094,
      "learning_rate": 0.00019220696092812377,
      "loss": 0.6841,
      "step": 1466
    },
    {
      "epoch": 0.11736,
      "grad_norm": 0.3287141025066376,
      "learning_rate": 0.00019220162688358448,
      "loss": 0.6363,
      "step": 1467
    },
    {
      "epoch": 0.11744,
      "grad_norm": 0.42304304242134094,
      "learning_rate": 0.00019219629283904522,
      "loss": 0.6986,
      "step": 1468
    },
    {
      "epoch": 0.11752,
      "grad_norm": 0.42346689105033875,
      "learning_rate": 0.00019219095879450594,
      "loss": 1.0055,
      "step": 1469
    },
    {
      "epoch": 0.1176,
      "grad_norm": 0.3032272160053253,
      "learning_rate": 0.00019218562474996668,
      "loss": 1.0857,
      "step": 1470
    },
    {
      "epoch": 0.11768,
      "grad_norm": 0.3269241750240326,
      "learning_rate": 0.0001921802907054274,
      "loss": 0.7103,
      "step": 1471
    },
    {
      "epoch": 0.11776,
      "grad_norm": 0.350533664226532,
      "learning_rate": 0.00019217495666088813,
      "loss": 0.7724,
      "step": 1472
    },
    {
      "epoch": 0.11784,
      "grad_norm": 0.3635333776473999,
      "learning_rate": 0.00019216962261634884,
      "loss": 1.0867,
      "step": 1473
    },
    {
      "epoch": 0.11792,
      "grad_norm": 0.2639807462692261,
      "learning_rate": 0.00019216428857180958,
      "loss": 0.916,
      "step": 1474
    },
    {
      "epoch": 0.118,
      "grad_norm": 0.2903149724006653,
      "learning_rate": 0.00019215895452727032,
      "loss": 0.5293,
      "step": 1475
    },
    {
      "epoch": 0.11808,
      "grad_norm": 0.2695561349391937,
      "learning_rate": 0.00019215362048273103,
      "loss": 0.8339,
      "step": 1476
    },
    {
      "epoch": 0.11816,
      "grad_norm": 0.3267346918582916,
      "learning_rate": 0.00019214828643819177,
      "loss": 0.7847,
      "step": 1477
    },
    {
      "epoch": 0.11824,
      "grad_norm": 0.3548450171947479,
      "learning_rate": 0.00019214295239365249,
      "loss": 0.7627,
      "step": 1478
    },
    {
      "epoch": 0.11832,
      "grad_norm": 0.27994269132614136,
      "learning_rate": 0.00019213761834911323,
      "loss": 1.0477,
      "step": 1479
    },
    {
      "epoch": 0.1184,
      "grad_norm": 0.3569497764110565,
      "learning_rate": 0.00019213228430457394,
      "loss": 0.839,
      "step": 1480
    },
    {
      "epoch": 0.11848,
      "grad_norm": 0.2909923791885376,
      "learning_rate": 0.00019212695026003468,
      "loss": 0.7372,
      "step": 1481
    },
    {
      "epoch": 0.11856,
      "grad_norm": 0.3089000880718231,
      "learning_rate": 0.0001921216162154954,
      "loss": 0.7306,
      "step": 1482
    },
    {
      "epoch": 0.11864,
      "grad_norm": 0.39171430468559265,
      "learning_rate": 0.00019211628217095613,
      "loss": 1.03,
      "step": 1483
    },
    {
      "epoch": 0.11872,
      "grad_norm": 0.39660996198654175,
      "learning_rate": 0.00019211094812641684,
      "loss": 0.734,
      "step": 1484
    },
    {
      "epoch": 0.1188,
      "grad_norm": 0.34679123759269714,
      "learning_rate": 0.00019210561408187758,
      "loss": 0.9928,
      "step": 1485
    },
    {
      "epoch": 0.11888,
      "grad_norm": 0.40692138671875,
      "learning_rate": 0.00019210028003733832,
      "loss": 0.9678,
      "step": 1486
    },
    {
      "epoch": 0.11896,
      "grad_norm": 0.34835827350616455,
      "learning_rate": 0.00019209494599279904,
      "loss": 0.6316,
      "step": 1487
    },
    {
      "epoch": 0.11904,
      "grad_norm": 0.38324716687202454,
      "learning_rate": 0.00019208961194825978,
      "loss": 0.7396,
      "step": 1488
    },
    {
      "epoch": 0.11912,
      "grad_norm": 0.33888348937034607,
      "learning_rate": 0.0001920842779037205,
      "loss": 0.7669,
      "step": 1489
    },
    {
      "epoch": 0.1192,
      "grad_norm": 0.45849159359931946,
      "learning_rate": 0.00019207894385918123,
      "loss": 0.9238,
      "step": 1490
    },
    {
      "epoch": 0.11928,
      "grad_norm": 0.2623756527900696,
      "learning_rate": 0.00019207360981464194,
      "loss": 0.5248,
      "step": 1491
    },
    {
      "epoch": 0.11936,
      "grad_norm": 0.3209735155105591,
      "learning_rate": 0.00019206827577010268,
      "loss": 0.777,
      "step": 1492
    },
    {
      "epoch": 0.11944,
      "grad_norm": 0.40853390097618103,
      "learning_rate": 0.00019206294172556342,
      "loss": 0.6784,
      "step": 1493
    },
    {
      "epoch": 0.11952,
      "grad_norm": 0.31302911043167114,
      "learning_rate": 0.00019205760768102413,
      "loss": 0.8167,
      "step": 1494
    },
    {
      "epoch": 0.1196,
      "grad_norm": 0.385823130607605,
      "learning_rate": 0.00019205227363648487,
      "loss": 0.6191,
      "step": 1495
    },
    {
      "epoch": 0.11968,
      "grad_norm": 0.3279150426387787,
      "learning_rate": 0.00019204693959194559,
      "loss": 0.8845,
      "step": 1496
    },
    {
      "epoch": 0.11976,
      "grad_norm": 0.4143381416797638,
      "learning_rate": 0.00019204160554740633,
      "loss": 0.7897,
      "step": 1497
    },
    {
      "epoch": 0.11984,
      "grad_norm": 0.2872237265110016,
      "learning_rate": 0.00019203627150286704,
      "loss": 1.0071,
      "step": 1498
    },
    {
      "epoch": 0.11992,
      "grad_norm": 0.36735936999320984,
      "learning_rate": 0.00019203093745832778,
      "loss": 0.8268,
      "step": 1499
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.3027828633785248,
      "learning_rate": 0.00019202560341378852,
      "loss": 0.6519,
      "step": 1500
    },
    {
      "epoch": 0.12008,
      "grad_norm": 0.37508025765419006,
      "learning_rate": 0.00019202026936924923,
      "loss": 0.7716,
      "step": 1501
    },
    {
      "epoch": 0.12016,
      "grad_norm": 0.462965726852417,
      "learning_rate": 0.00019201493532470997,
      "loss": 0.7801,
      "step": 1502
    },
    {
      "epoch": 0.12024,
      "grad_norm": 0.42016860842704773,
      "learning_rate": 0.00019200960128017068,
      "loss": 0.852,
      "step": 1503
    },
    {
      "epoch": 0.12032,
      "grad_norm": 0.31411871314048767,
      "learning_rate": 0.00019200426723563142,
      "loss": 0.6294,
      "step": 1504
    },
    {
      "epoch": 0.1204,
      "grad_norm": 0.37882116436958313,
      "learning_rate": 0.00019199893319109214,
      "loss": 0.9641,
      "step": 1505
    },
    {
      "epoch": 0.12048,
      "grad_norm": 0.2481466382741928,
      "learning_rate": 0.00019199359914655288,
      "loss": 0.5428,
      "step": 1506
    },
    {
      "epoch": 0.12056,
      "grad_norm": 0.31327247619628906,
      "learning_rate": 0.00019198826510201362,
      "loss": 1.0228,
      "step": 1507
    },
    {
      "epoch": 0.12064,
      "grad_norm": 0.3451671898365021,
      "learning_rate": 0.00019198293105747433,
      "loss": 1.0969,
      "step": 1508
    },
    {
      "epoch": 0.12072,
      "grad_norm": 0.4311075508594513,
      "learning_rate": 0.00019197759701293507,
      "loss": 0.806,
      "step": 1509
    },
    {
      "epoch": 0.1208,
      "grad_norm": 0.3628218173980713,
      "learning_rate": 0.00019197226296839578,
      "loss": 1.0561,
      "step": 1510
    },
    {
      "epoch": 0.12088,
      "grad_norm": 0.4463934302330017,
      "learning_rate": 0.00019196692892385652,
      "loss": 0.8234,
      "step": 1511
    },
    {
      "epoch": 0.12096,
      "grad_norm": 0.3295998275279999,
      "learning_rate": 0.00019196159487931723,
      "loss": 0.801,
      "step": 1512
    },
    {
      "epoch": 0.12104,
      "grad_norm": 0.6427950263023376,
      "learning_rate": 0.00019195626083477797,
      "loss": 1.039,
      "step": 1513
    },
    {
      "epoch": 0.12112,
      "grad_norm": 0.38061678409576416,
      "learning_rate": 0.00019195092679023871,
      "loss": 0.8622,
      "step": 1514
    },
    {
      "epoch": 0.1212,
      "grad_norm": 0.38734424114227295,
      "learning_rate": 0.00019194559274569943,
      "loss": 0.7823,
      "step": 1515
    },
    {
      "epoch": 0.12128,
      "grad_norm": 0.40168827772140503,
      "learning_rate": 0.00019194025870116017,
      "loss": 0.9068,
      "step": 1516
    },
    {
      "epoch": 0.12136,
      "grad_norm": 0.4141693711280823,
      "learning_rate": 0.00019193492465662088,
      "loss": 0.9812,
      "step": 1517
    },
    {
      "epoch": 0.12144,
      "grad_norm": 0.36090970039367676,
      "learning_rate": 0.00019192959061208162,
      "loss": 0.9146,
      "step": 1518
    },
    {
      "epoch": 0.12152,
      "grad_norm": 0.4081425368785858,
      "learning_rate": 0.00019192425656754233,
      "loss": 0.865,
      "step": 1519
    },
    {
      "epoch": 0.1216,
      "grad_norm": 0.2759111821651459,
      "learning_rate": 0.00019191892252300307,
      "loss": 0.8806,
      "step": 1520
    },
    {
      "epoch": 0.12168,
      "grad_norm": 0.48755335807800293,
      "learning_rate": 0.0001919135884784638,
      "loss": 0.8146,
      "step": 1521
    },
    {
      "epoch": 0.12176,
      "grad_norm": 0.3001735508441925,
      "learning_rate": 0.00019190825443392452,
      "loss": 0.8488,
      "step": 1522
    },
    {
      "epoch": 0.12184,
      "grad_norm": 0.3498751223087311,
      "learning_rate": 0.00019190292038938526,
      "loss": 0.7967,
      "step": 1523
    },
    {
      "epoch": 0.12192,
      "grad_norm": 0.3129950761795044,
      "learning_rate": 0.00019189758634484598,
      "loss": 0.6658,
      "step": 1524
    },
    {
      "epoch": 0.122,
      "grad_norm": 0.3186883330345154,
      "learning_rate": 0.00019189225230030672,
      "loss": 0.6624,
      "step": 1525
    },
    {
      "epoch": 0.12208,
      "grad_norm": 0.3883443772792816,
      "learning_rate": 0.00019188691825576743,
      "loss": 0.776,
      "step": 1526
    },
    {
      "epoch": 0.12216,
      "grad_norm": 0.359822154045105,
      "learning_rate": 0.00019188158421122817,
      "loss": 0.7918,
      "step": 1527
    },
    {
      "epoch": 0.12224,
      "grad_norm": 0.2818925082683563,
      "learning_rate": 0.0001918762501666889,
      "loss": 0.6059,
      "step": 1528
    },
    {
      "epoch": 0.12232,
      "grad_norm": 0.3314612805843353,
      "learning_rate": 0.00019187091612214962,
      "loss": 1.025,
      "step": 1529
    },
    {
      "epoch": 0.1224,
      "grad_norm": 0.2868194580078125,
      "learning_rate": 0.00019186558207761036,
      "loss": 0.8217,
      "step": 1530
    },
    {
      "epoch": 0.12248,
      "grad_norm": 0.4304717183113098,
      "learning_rate": 0.00019186024803307107,
      "loss": 0.8814,
      "step": 1531
    },
    {
      "epoch": 0.12256,
      "grad_norm": 0.305675208568573,
      "learning_rate": 0.00019185491398853181,
      "loss": 0.6944,
      "step": 1532
    },
    {
      "epoch": 0.12264,
      "grad_norm": 0.36538514494895935,
      "learning_rate": 0.00019184957994399255,
      "loss": 0.91,
      "step": 1533
    },
    {
      "epoch": 0.12272,
      "grad_norm": 0.3557418882846832,
      "learning_rate": 0.00019184424589945327,
      "loss": 0.7503,
      "step": 1534
    },
    {
      "epoch": 0.1228,
      "grad_norm": 0.31879526376724243,
      "learning_rate": 0.000191838911854914,
      "loss": 0.9123,
      "step": 1535
    },
    {
      "epoch": 0.12288,
      "grad_norm": 0.24624505639076233,
      "learning_rate": 0.00019183357781037472,
      "loss": 0.7534,
      "step": 1536
    },
    {
      "epoch": 0.12296,
      "grad_norm": 0.306000292301178,
      "learning_rate": 0.00019182824376583546,
      "loss": 1.2096,
      "step": 1537
    },
    {
      "epoch": 0.12304,
      "grad_norm": 0.36191079020500183,
      "learning_rate": 0.00019182290972129617,
      "loss": 0.8042,
      "step": 1538
    },
    {
      "epoch": 0.12312,
      "grad_norm": 0.4078235924243927,
      "learning_rate": 0.0001918175756767569,
      "loss": 0.9509,
      "step": 1539
    },
    {
      "epoch": 0.1232,
      "grad_norm": 0.4092755615711212,
      "learning_rate": 0.00019181224163221765,
      "loss": 1.0805,
      "step": 1540
    },
    {
      "epoch": 0.12328,
      "grad_norm": 0.2589908838272095,
      "learning_rate": 0.00019180690758767836,
      "loss": 0.4752,
      "step": 1541
    },
    {
      "epoch": 0.12336,
      "grad_norm": 0.36278870701789856,
      "learning_rate": 0.0001918015735431391,
      "loss": 1.0538,
      "step": 1542
    },
    {
      "epoch": 0.12344,
      "grad_norm": 0.335092157125473,
      "learning_rate": 0.00019179623949859982,
      "loss": 0.836,
      "step": 1543
    },
    {
      "epoch": 0.12352,
      "grad_norm": 0.3387429118156433,
      "learning_rate": 0.00019179090545406056,
      "loss": 0.9713,
      "step": 1544
    },
    {
      "epoch": 0.1236,
      "grad_norm": 0.31813904643058777,
      "learning_rate": 0.00019178557140952127,
      "loss": 0.844,
      "step": 1545
    },
    {
      "epoch": 0.12368,
      "grad_norm": 0.3240617513656616,
      "learning_rate": 0.000191780237364982,
      "loss": 0.8457,
      "step": 1546
    },
    {
      "epoch": 0.12376,
      "grad_norm": 0.4315124452114105,
      "learning_rate": 0.00019177490332044275,
      "loss": 0.7762,
      "step": 1547
    },
    {
      "epoch": 0.12384,
      "grad_norm": 0.3327386975288391,
      "learning_rate": 0.00019176956927590346,
      "loss": 0.7737,
      "step": 1548
    },
    {
      "epoch": 0.12392,
      "grad_norm": 0.32109883427619934,
      "learning_rate": 0.0001917642352313642,
      "loss": 0.9607,
      "step": 1549
    },
    {
      "epoch": 0.124,
      "grad_norm": 0.3083806335926056,
      "learning_rate": 0.00019175890118682491,
      "loss": 0.9547,
      "step": 1550
    },
    {
      "epoch": 0.12408,
      "grad_norm": 0.3691752254962921,
      "learning_rate": 0.00019175356714228565,
      "loss": 0.7326,
      "step": 1551
    },
    {
      "epoch": 0.12416,
      "grad_norm": 0.42877933382987976,
      "learning_rate": 0.00019174823309774637,
      "loss": 0.7845,
      "step": 1552
    },
    {
      "epoch": 0.12424,
      "grad_norm": 0.36625203490257263,
      "learning_rate": 0.0001917428990532071,
      "loss": 0.9464,
      "step": 1553
    },
    {
      "epoch": 0.12432,
      "grad_norm": 0.4677218198776245,
      "learning_rate": 0.00019173756500866785,
      "loss": 1.0697,
      "step": 1554
    },
    {
      "epoch": 0.1244,
      "grad_norm": 0.3792210817337036,
      "learning_rate": 0.00019173223096412856,
      "loss": 0.8702,
      "step": 1555
    },
    {
      "epoch": 0.12448,
      "grad_norm": 0.38732120394706726,
      "learning_rate": 0.0001917268969195893,
      "loss": 0.8701,
      "step": 1556
    },
    {
      "epoch": 0.12456,
      "grad_norm": 0.31906333565711975,
      "learning_rate": 0.00019172156287505,
      "loss": 0.6951,
      "step": 1557
    },
    {
      "epoch": 0.12464,
      "grad_norm": 0.38251039385795593,
      "learning_rate": 0.00019171622883051075,
      "loss": 0.7484,
      "step": 1558
    },
    {
      "epoch": 0.12472,
      "grad_norm": 0.31349489092826843,
      "learning_rate": 0.00019171089478597146,
      "loss": 0.6579,
      "step": 1559
    },
    {
      "epoch": 0.1248,
      "grad_norm": 0.39264610409736633,
      "learning_rate": 0.0001917055607414322,
      "loss": 1.0739,
      "step": 1560
    },
    {
      "epoch": 0.12488,
      "grad_norm": 0.3371306359767914,
      "learning_rate": 0.00019170022669689294,
      "loss": 0.8633,
      "step": 1561
    },
    {
      "epoch": 0.12496,
      "grad_norm": 0.3728993237018585,
      "learning_rate": 0.00019169489265235366,
      "loss": 0.9904,
      "step": 1562
    },
    {
      "epoch": 0.12504,
      "grad_norm": 0.33449724316596985,
      "learning_rate": 0.0001916895586078144,
      "loss": 0.8138,
      "step": 1563
    },
    {
      "epoch": 0.12512,
      "grad_norm": 0.31915342807769775,
      "learning_rate": 0.0001916842245632751,
      "loss": 0.7548,
      "step": 1564
    },
    {
      "epoch": 0.1252,
      "grad_norm": 0.297309547662735,
      "learning_rate": 0.00019167889051873585,
      "loss": 1.0946,
      "step": 1565
    },
    {
      "epoch": 0.12528,
      "grad_norm": 0.36059990525245667,
      "learning_rate": 0.00019167355647419656,
      "loss": 0.7606,
      "step": 1566
    },
    {
      "epoch": 0.12536,
      "grad_norm": 0.3069186806678772,
      "learning_rate": 0.0001916682224296573,
      "loss": 0.9081,
      "step": 1567
    },
    {
      "epoch": 0.12544,
      "grad_norm": 0.33708474040031433,
      "learning_rate": 0.00019166288838511804,
      "loss": 0.7992,
      "step": 1568
    },
    {
      "epoch": 0.12552,
      "grad_norm": 0.34252244234085083,
      "learning_rate": 0.00019165755434057876,
      "loss": 0.6965,
      "step": 1569
    },
    {
      "epoch": 0.1256,
      "grad_norm": 0.37087178230285645,
      "learning_rate": 0.0001916522202960395,
      "loss": 1.1091,
      "step": 1570
    },
    {
      "epoch": 0.12568,
      "grad_norm": 0.35904809832572937,
      "learning_rate": 0.0001916468862515002,
      "loss": 1.1006,
      "step": 1571
    },
    {
      "epoch": 0.12576,
      "grad_norm": 0.3444395661354065,
      "learning_rate": 0.00019164155220696095,
      "loss": 0.8462,
      "step": 1572
    },
    {
      "epoch": 0.12584,
      "grad_norm": 0.4503075182437897,
      "learning_rate": 0.00019163621816242166,
      "loss": 0.7745,
      "step": 1573
    },
    {
      "epoch": 0.12592,
      "grad_norm": 0.28469568490982056,
      "learning_rate": 0.0001916308841178824,
      "loss": 0.9108,
      "step": 1574
    },
    {
      "epoch": 0.126,
      "grad_norm": 0.30645790696144104,
      "learning_rate": 0.00019162555007334314,
      "loss": 0.7507,
      "step": 1575
    },
    {
      "epoch": 0.12608,
      "grad_norm": 0.33738380670547485,
      "learning_rate": 0.00019162021602880385,
      "loss": 0.9414,
      "step": 1576
    },
    {
      "epoch": 0.12616,
      "grad_norm": 0.3717169761657715,
      "learning_rate": 0.0001916148819842646,
      "loss": 0.6796,
      "step": 1577
    },
    {
      "epoch": 0.12624,
      "grad_norm": 0.3064756691455841,
      "learning_rate": 0.0001916095479397253,
      "loss": 1.08,
      "step": 1578
    },
    {
      "epoch": 0.12632,
      "grad_norm": 0.2863151431083679,
      "learning_rate": 0.00019160421389518605,
      "loss": 0.9285,
      "step": 1579
    },
    {
      "epoch": 0.1264,
      "grad_norm": 0.4467393755912781,
      "learning_rate": 0.00019159887985064678,
      "loss": 0.9684,
      "step": 1580
    },
    {
      "epoch": 0.12648,
      "grad_norm": 0.30693021416664124,
      "learning_rate": 0.0001915935458061075,
      "loss": 0.6136,
      "step": 1581
    },
    {
      "epoch": 0.12656,
      "grad_norm": 0.28150615096092224,
      "learning_rate": 0.00019158821176156824,
      "loss": 0.5191,
      "step": 1582
    },
    {
      "epoch": 0.12664,
      "grad_norm": 0.3038492500782013,
      "learning_rate": 0.00019158287771702895,
      "loss": 0.5062,
      "step": 1583
    },
    {
      "epoch": 0.12672,
      "grad_norm": 0.38409703969955444,
      "learning_rate": 0.0001915775436724897,
      "loss": 0.8296,
      "step": 1584
    },
    {
      "epoch": 0.1268,
      "grad_norm": 0.35532310605049133,
      "learning_rate": 0.0001915722096279504,
      "loss": 0.8348,
      "step": 1585
    },
    {
      "epoch": 0.12688,
      "grad_norm": 0.32439664006233215,
      "learning_rate": 0.00019156687558341114,
      "loss": 0.8555,
      "step": 1586
    },
    {
      "epoch": 0.12696,
      "grad_norm": 0.3693505823612213,
      "learning_rate": 0.00019156154153887188,
      "loss": 0.9331,
      "step": 1587
    },
    {
      "epoch": 0.12704,
      "grad_norm": 0.333073228597641,
      "learning_rate": 0.0001915562074943326,
      "loss": 1.0133,
      "step": 1588
    },
    {
      "epoch": 0.12712,
      "grad_norm": 0.37269270420074463,
      "learning_rate": 0.00019155087344979334,
      "loss": 0.8357,
      "step": 1589
    },
    {
      "epoch": 0.1272,
      "grad_norm": 0.27254170179367065,
      "learning_rate": 0.00019154553940525405,
      "loss": 1.1931,
      "step": 1590
    },
    {
      "epoch": 0.12728,
      "grad_norm": 0.3424210846424103,
      "learning_rate": 0.0001915402053607148,
      "loss": 0.7485,
      "step": 1591
    },
    {
      "epoch": 0.12736,
      "grad_norm": 0.3232298195362091,
      "learning_rate": 0.0001915348713161755,
      "loss": 0.5799,
      "step": 1592
    },
    {
      "epoch": 0.12744,
      "grad_norm": 0.3136034309864044,
      "learning_rate": 0.00019152953727163624,
      "loss": 0.9837,
      "step": 1593
    },
    {
      "epoch": 0.12752,
      "grad_norm": 0.31232404708862305,
      "learning_rate": 0.00019152420322709695,
      "loss": 0.8705,
      "step": 1594
    },
    {
      "epoch": 0.1276,
      "grad_norm": 0.34124454855918884,
      "learning_rate": 0.0001915188691825577,
      "loss": 1.0901,
      "step": 1595
    },
    {
      "epoch": 0.12768,
      "grad_norm": 0.36613622307777405,
      "learning_rate": 0.0001915135351380184,
      "loss": 0.8243,
      "step": 1596
    },
    {
      "epoch": 0.12776,
      "grad_norm": 0.3306519687175751,
      "learning_rate": 0.00019150820109347915,
      "loss": 0.7074,
      "step": 1597
    },
    {
      "epoch": 0.12784,
      "grad_norm": 0.3954137861728668,
      "learning_rate": 0.00019150286704893986,
      "loss": 0.9009,
      "step": 1598
    },
    {
      "epoch": 0.12792,
      "grad_norm": 0.40612202882766724,
      "learning_rate": 0.0001914975330044006,
      "loss": 0.8639,
      "step": 1599
    },
    {
      "epoch": 0.128,
      "grad_norm": 0.2888900637626648,
      "learning_rate": 0.0001914921989598613,
      "loss": 0.8814,
      "step": 1600
    },
    {
      "epoch": 0.12808,
      "grad_norm": 0.3262469470500946,
      "learning_rate": 0.00019148686491532205,
      "loss": 0.8763,
      "step": 1601
    },
    {
      "epoch": 0.12816,
      "grad_norm": 0.3529181182384491,
      "learning_rate": 0.0001914815308707828,
      "loss": 0.7145,
      "step": 1602
    },
    {
      "epoch": 0.12824,
      "grad_norm": 0.33588898181915283,
      "learning_rate": 0.0001914761968262435,
      "loss": 0.7756,
      "step": 1603
    },
    {
      "epoch": 0.12832,
      "grad_norm": 0.30346187949180603,
      "learning_rate": 0.00019147086278170424,
      "loss": 0.9077,
      "step": 1604
    },
    {
      "epoch": 0.1284,
      "grad_norm": 0.24766847491264343,
      "learning_rate": 0.00019146552873716496,
      "loss": 0.8971,
      "step": 1605
    },
    {
      "epoch": 0.12848,
      "grad_norm": 0.4436662495136261,
      "learning_rate": 0.0001914601946926257,
      "loss": 0.9734,
      "step": 1606
    },
    {
      "epoch": 0.12856,
      "grad_norm": 0.29265928268432617,
      "learning_rate": 0.0001914548606480864,
      "loss": 0.93,
      "step": 1607
    },
    {
      "epoch": 0.12864,
      "grad_norm": 0.3418262302875519,
      "learning_rate": 0.00019144952660354715,
      "loss": 0.7493,
      "step": 1608
    },
    {
      "epoch": 0.12872,
      "grad_norm": 0.31777408719062805,
      "learning_rate": 0.00019144419255900786,
      "loss": 0.6007,
      "step": 1609
    },
    {
      "epoch": 0.1288,
      "grad_norm": 0.33208587765693665,
      "learning_rate": 0.0001914388585144686,
      "loss": 0.9951,
      "step": 1610
    },
    {
      "epoch": 0.12888,
      "grad_norm": 0.2620461583137512,
      "learning_rate": 0.00019143352446992931,
      "loss": 0.6464,
      "step": 1611
    },
    {
      "epoch": 0.12896,
      "grad_norm": 0.3524248003959656,
      "learning_rate": 0.00019142819042539005,
      "loss": 0.9656,
      "step": 1612
    },
    {
      "epoch": 0.12904,
      "grad_norm": 0.34284448623657227,
      "learning_rate": 0.00019142285638085077,
      "loss": 0.8122,
      "step": 1613
    },
    {
      "epoch": 0.12912,
      "grad_norm": 0.27486667037010193,
      "learning_rate": 0.0001914175223363115,
      "loss": 0.6062,
      "step": 1614
    },
    {
      "epoch": 0.1292,
      "grad_norm": 0.31639161705970764,
      "learning_rate": 0.00019141218829177225,
      "loss": 0.7143,
      "step": 1615
    },
    {
      "epoch": 0.12928,
      "grad_norm": 0.386261522769928,
      "learning_rate": 0.00019140685424723296,
      "loss": 0.7033,
      "step": 1616
    },
    {
      "epoch": 0.12936,
      "grad_norm": 0.3805498480796814,
      "learning_rate": 0.0001914015202026937,
      "loss": 1.1118,
      "step": 1617
    },
    {
      "epoch": 0.12944,
      "grad_norm": 0.342401385307312,
      "learning_rate": 0.0001913961861581544,
      "loss": 0.542,
      "step": 1618
    },
    {
      "epoch": 0.12952,
      "grad_norm": 0.3600894808769226,
      "learning_rate": 0.00019139085211361515,
      "loss": 1.0743,
      "step": 1619
    },
    {
      "epoch": 0.1296,
      "grad_norm": 0.2771780788898468,
      "learning_rate": 0.00019138551806907586,
      "loss": 0.8837,
      "step": 1620
    },
    {
      "epoch": 0.12968,
      "grad_norm": 0.25921210646629333,
      "learning_rate": 0.0001913801840245366,
      "loss": 0.5643,
      "step": 1621
    },
    {
      "epoch": 0.12976,
      "grad_norm": 0.29142507910728455,
      "learning_rate": 0.00019137484997999734,
      "loss": 0.9095,
      "step": 1622
    },
    {
      "epoch": 0.12984,
      "grad_norm": 0.38348808884620667,
      "learning_rate": 0.00019136951593545806,
      "loss": 1.0671,
      "step": 1623
    },
    {
      "epoch": 0.12992,
      "grad_norm": 0.42393141984939575,
      "learning_rate": 0.0001913641818909188,
      "loss": 0.7672,
      "step": 1624
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.2995990812778473,
      "learning_rate": 0.0001913588478463795,
      "loss": 0.9808,
      "step": 1625
    },
    {
      "epoch": 0.13008,
      "grad_norm": 0.43153461813926697,
      "learning_rate": 0.00019135351380184025,
      "loss": 1.0594,
      "step": 1626
    },
    {
      "epoch": 0.13016,
      "grad_norm": 0.36355060338974,
      "learning_rate": 0.00019134817975730096,
      "loss": 0.744,
      "step": 1627
    },
    {
      "epoch": 0.13024,
      "grad_norm": 0.329563707113266,
      "learning_rate": 0.0001913428457127617,
      "loss": 1.2005,
      "step": 1628
    },
    {
      "epoch": 0.13032,
      "grad_norm": 0.3077915608882904,
      "learning_rate": 0.00019133751166822244,
      "loss": 0.9846,
      "step": 1629
    },
    {
      "epoch": 0.1304,
      "grad_norm": 0.409158855676651,
      "learning_rate": 0.00019133217762368315,
      "loss": 0.7194,
      "step": 1630
    },
    {
      "epoch": 0.13048,
      "grad_norm": 0.3574943542480469,
      "learning_rate": 0.0001913268435791439,
      "loss": 0.7807,
      "step": 1631
    },
    {
      "epoch": 0.13056,
      "grad_norm": 0.3344757854938507,
      "learning_rate": 0.0001913215095346046,
      "loss": 1.0793,
      "step": 1632
    },
    {
      "epoch": 0.13064,
      "grad_norm": 0.39217981696128845,
      "learning_rate": 0.00019131617549006535,
      "loss": 0.6945,
      "step": 1633
    },
    {
      "epoch": 0.13072,
      "grad_norm": 0.3598836064338684,
      "learning_rate": 0.00019131084144552609,
      "loss": 0.9427,
      "step": 1634
    },
    {
      "epoch": 0.1308,
      "grad_norm": 0.41729843616485596,
      "learning_rate": 0.0001913055074009868,
      "loss": 0.5746,
      "step": 1635
    },
    {
      "epoch": 0.13088,
      "grad_norm": 0.4543648362159729,
      "learning_rate": 0.00019130017335644754,
      "loss": 0.781,
      "step": 1636
    },
    {
      "epoch": 0.13096,
      "grad_norm": 0.3455111086368561,
      "learning_rate": 0.00019129483931190825,
      "loss": 0.7313,
      "step": 1637
    },
    {
      "epoch": 0.13104,
      "grad_norm": 0.4044169485569,
      "learning_rate": 0.000191289505267369,
      "loss": 0.7551,
      "step": 1638
    },
    {
      "epoch": 0.13112,
      "grad_norm": 0.3406437039375305,
      "learning_rate": 0.0001912841712228297,
      "loss": 0.9581,
      "step": 1639
    },
    {
      "epoch": 0.1312,
      "grad_norm": 0.27537181973457336,
      "learning_rate": 0.00019127883717829044,
      "loss": 1.0113,
      "step": 1640
    },
    {
      "epoch": 0.13128,
      "grad_norm": 0.4146762788295746,
      "learning_rate": 0.00019127350313375118,
      "loss": 0.8434,
      "step": 1641
    },
    {
      "epoch": 0.13136,
      "grad_norm": 0.33798226714134216,
      "learning_rate": 0.0001912681690892119,
      "loss": 1.0244,
      "step": 1642
    },
    {
      "epoch": 0.13144,
      "grad_norm": 0.3361968994140625,
      "learning_rate": 0.00019126283504467264,
      "loss": 0.5689,
      "step": 1643
    },
    {
      "epoch": 0.13152,
      "grad_norm": 0.3666616976261139,
      "learning_rate": 0.00019125750100013335,
      "loss": 0.7373,
      "step": 1644
    },
    {
      "epoch": 0.1316,
      "grad_norm": 0.41077739000320435,
      "learning_rate": 0.0001912521669555941,
      "loss": 1.2471,
      "step": 1645
    },
    {
      "epoch": 0.13168,
      "grad_norm": 0.5118486285209656,
      "learning_rate": 0.0001912468329110548,
      "loss": 0.9675,
      "step": 1646
    },
    {
      "epoch": 0.13176,
      "grad_norm": 0.3027259111404419,
      "learning_rate": 0.00019124149886651554,
      "loss": 0.7599,
      "step": 1647
    },
    {
      "epoch": 0.13184,
      "grad_norm": 0.4058026373386383,
      "learning_rate": 0.00019123616482197628,
      "loss": 0.8935,
      "step": 1648
    },
    {
      "epoch": 0.13192,
      "grad_norm": 0.40244224667549133,
      "learning_rate": 0.000191230830777437,
      "loss": 0.785,
      "step": 1649
    },
    {
      "epoch": 0.132,
      "grad_norm": 0.3731083571910858,
      "learning_rate": 0.00019122549673289773,
      "loss": 1.0845,
      "step": 1650
    },
    {
      "epoch": 0.13208,
      "grad_norm": 0.3918946385383606,
      "learning_rate": 0.00019122016268835845,
      "loss": 0.9587,
      "step": 1651
    },
    {
      "epoch": 0.13216,
      "grad_norm": 0.29998910427093506,
      "learning_rate": 0.0001912148286438192,
      "loss": 1.1348,
      "step": 1652
    },
    {
      "epoch": 0.13224,
      "grad_norm": 0.39267128705978394,
      "learning_rate": 0.0001912094945992799,
      "loss": 0.8975,
      "step": 1653
    },
    {
      "epoch": 0.13232,
      "grad_norm": 0.3329820930957794,
      "learning_rate": 0.00019120416055474064,
      "loss": 0.9808,
      "step": 1654
    },
    {
      "epoch": 0.1324,
      "grad_norm": 0.4133332073688507,
      "learning_rate": 0.00019119882651020138,
      "loss": 0.9816,
      "step": 1655
    },
    {
      "epoch": 0.13248,
      "grad_norm": 0.24690106511116028,
      "learning_rate": 0.0001911934924656621,
      "loss": 0.7105,
      "step": 1656
    },
    {
      "epoch": 0.13256,
      "grad_norm": 0.2588973343372345,
      "learning_rate": 0.00019118815842112283,
      "loss": 0.9267,
      "step": 1657
    },
    {
      "epoch": 0.13264,
      "grad_norm": 0.32102227210998535,
      "learning_rate": 0.00019118282437658354,
      "loss": 0.7205,
      "step": 1658
    },
    {
      "epoch": 0.13272,
      "grad_norm": 0.3348974585533142,
      "learning_rate": 0.00019117749033204428,
      "loss": 0.8012,
      "step": 1659
    },
    {
      "epoch": 0.1328,
      "grad_norm": 0.40159350633621216,
      "learning_rate": 0.000191172156287505,
      "loss": 0.992,
      "step": 1660
    },
    {
      "epoch": 0.13288,
      "grad_norm": 0.25711166858673096,
      "learning_rate": 0.00019116682224296574,
      "loss": 0.6782,
      "step": 1661
    },
    {
      "epoch": 0.13296,
      "grad_norm": 0.45973294973373413,
      "learning_rate": 0.00019116148819842648,
      "loss": 1.0597,
      "step": 1662
    },
    {
      "epoch": 0.13304,
      "grad_norm": 0.29161038994789124,
      "learning_rate": 0.0001911561541538872,
      "loss": 1.0485,
      "step": 1663
    },
    {
      "epoch": 0.13312,
      "grad_norm": 0.296986848115921,
      "learning_rate": 0.00019115082010934793,
      "loss": 0.9946,
      "step": 1664
    },
    {
      "epoch": 0.1332,
      "grad_norm": 0.36984601616859436,
      "learning_rate": 0.00019114548606480864,
      "loss": 0.7232,
      "step": 1665
    },
    {
      "epoch": 0.13328,
      "grad_norm": 0.3834679424762726,
      "learning_rate": 0.00019114015202026938,
      "loss": 0.7571,
      "step": 1666
    },
    {
      "epoch": 0.13336,
      "grad_norm": 0.35535159707069397,
      "learning_rate": 0.0001911348179757301,
      "loss": 0.6678,
      "step": 1667
    },
    {
      "epoch": 0.13344,
      "grad_norm": 0.38972190022468567,
      "learning_rate": 0.00019112948393119083,
      "loss": 0.5737,
      "step": 1668
    },
    {
      "epoch": 0.13352,
      "grad_norm": 0.39894142746925354,
      "learning_rate": 0.00019112414988665157,
      "loss": 0.8643,
      "step": 1669
    },
    {
      "epoch": 0.1336,
      "grad_norm": 0.37463536858558655,
      "learning_rate": 0.0001911188158421123,
      "loss": 0.7893,
      "step": 1670
    },
    {
      "epoch": 0.13368,
      "grad_norm": 0.37420201301574707,
      "learning_rate": 0.00019111348179757303,
      "loss": 0.8684,
      "step": 1671
    },
    {
      "epoch": 0.13376,
      "grad_norm": 0.5591493248939514,
      "learning_rate": 0.00019110814775303374,
      "loss": 0.8564,
      "step": 1672
    },
    {
      "epoch": 0.13384,
      "grad_norm": 0.3898329436779022,
      "learning_rate": 0.00019110281370849448,
      "loss": 0.8912,
      "step": 1673
    },
    {
      "epoch": 0.13392,
      "grad_norm": 0.36102548241615295,
      "learning_rate": 0.0001910974796639552,
      "loss": 1.0738,
      "step": 1674
    },
    {
      "epoch": 0.134,
      "grad_norm": 0.2712031900882721,
      "learning_rate": 0.00019109214561941593,
      "loss": 0.6649,
      "step": 1675
    },
    {
      "epoch": 0.13408,
      "grad_norm": 0.41328075528144836,
      "learning_rate": 0.00019108681157487667,
      "loss": 0.8219,
      "step": 1676
    },
    {
      "epoch": 0.13416,
      "grad_norm": 0.3599848747253418,
      "learning_rate": 0.00019108147753033738,
      "loss": 0.7366,
      "step": 1677
    },
    {
      "epoch": 0.13424,
      "grad_norm": 0.268130362033844,
      "learning_rate": 0.00019107614348579812,
      "loss": 1.2002,
      "step": 1678
    },
    {
      "epoch": 0.13432,
      "grad_norm": 0.37178876996040344,
      "learning_rate": 0.00019107080944125884,
      "loss": 0.9702,
      "step": 1679
    },
    {
      "epoch": 0.1344,
      "grad_norm": 0.2881040573120117,
      "learning_rate": 0.00019106547539671958,
      "loss": 1.0721,
      "step": 1680
    },
    {
      "epoch": 0.13448,
      "grad_norm": 0.29889094829559326,
      "learning_rate": 0.00019106014135218032,
      "loss": 1.0825,
      "step": 1681
    },
    {
      "epoch": 0.13456,
      "grad_norm": 0.3685729205608368,
      "learning_rate": 0.00019105480730764103,
      "loss": 0.6017,
      "step": 1682
    },
    {
      "epoch": 0.13464,
      "grad_norm": 0.34634438157081604,
      "learning_rate": 0.00019104947326310177,
      "loss": 0.9273,
      "step": 1683
    },
    {
      "epoch": 0.13472,
      "grad_norm": 0.3700859844684601,
      "learning_rate": 0.00019104413921856248,
      "loss": 0.7745,
      "step": 1684
    },
    {
      "epoch": 0.1348,
      "grad_norm": 0.2812062203884125,
      "learning_rate": 0.00019103880517402322,
      "loss": 0.4916,
      "step": 1685
    },
    {
      "epoch": 0.13488,
      "grad_norm": 0.3100377023220062,
      "learning_rate": 0.00019103347112948393,
      "loss": 0.6037,
      "step": 1686
    },
    {
      "epoch": 0.13496,
      "grad_norm": 0.34797045588493347,
      "learning_rate": 0.00019102813708494467,
      "loss": 0.9634,
      "step": 1687
    },
    {
      "epoch": 0.13504,
      "grad_norm": 0.32527607679367065,
      "learning_rate": 0.00019102280304040541,
      "loss": 0.8344,
      "step": 1688
    },
    {
      "epoch": 0.13512,
      "grad_norm": 0.363027423620224,
      "learning_rate": 0.00019101746899586613,
      "loss": 0.8957,
      "step": 1689
    },
    {
      "epoch": 0.1352,
      "grad_norm": 0.37209928035736084,
      "learning_rate": 0.00019101213495132687,
      "loss": 1.3908,
      "step": 1690
    },
    {
      "epoch": 0.13528,
      "grad_norm": 0.35687270760536194,
      "learning_rate": 0.00019100680090678758,
      "loss": 0.7921,
      "step": 1691
    },
    {
      "epoch": 0.13536,
      "grad_norm": 0.2758089601993561,
      "learning_rate": 0.00019100146686224832,
      "loss": 0.7597,
      "step": 1692
    },
    {
      "epoch": 0.13544,
      "grad_norm": 0.3167467415332794,
      "learning_rate": 0.00019099613281770903,
      "loss": 0.8636,
      "step": 1693
    },
    {
      "epoch": 0.13552,
      "grad_norm": 0.2663203179836273,
      "learning_rate": 0.00019099079877316977,
      "loss": 0.9104,
      "step": 1694
    },
    {
      "epoch": 0.1356,
      "grad_norm": 0.3696025311946869,
      "learning_rate": 0.0001909854647286305,
      "loss": 1.0145,
      "step": 1695
    },
    {
      "epoch": 0.13568,
      "grad_norm": 0.3448858857154846,
      "learning_rate": 0.00019098013068409122,
      "loss": 0.7393,
      "step": 1696
    },
    {
      "epoch": 0.13576,
      "grad_norm": 0.3354754149913788,
      "learning_rate": 0.00019097479663955196,
      "loss": 0.8899,
      "step": 1697
    },
    {
      "epoch": 0.13584,
      "grad_norm": 0.33300507068634033,
      "learning_rate": 0.00019096946259501268,
      "loss": 0.8136,
      "step": 1698
    },
    {
      "epoch": 0.13592,
      "grad_norm": 0.45910367369651794,
      "learning_rate": 0.00019096412855047342,
      "loss": 0.8301,
      "step": 1699
    },
    {
      "epoch": 0.136,
      "grad_norm": 0.2624008357524872,
      "learning_rate": 0.00019095879450593413,
      "loss": 0.5548,
      "step": 1700
    },
    {
      "epoch": 0.13608,
      "grad_norm": 0.3198928236961365,
      "learning_rate": 0.00019095346046139487,
      "loss": 0.5176,
      "step": 1701
    },
    {
      "epoch": 0.13616,
      "grad_norm": 0.433054119348526,
      "learning_rate": 0.0001909481264168556,
      "loss": 0.9083,
      "step": 1702
    },
    {
      "epoch": 0.13624,
      "grad_norm": 0.2983556389808655,
      "learning_rate": 0.00019094279237231632,
      "loss": 0.828,
      "step": 1703
    },
    {
      "epoch": 0.13632,
      "grad_norm": 0.361500084400177,
      "learning_rate": 0.00019093745832777706,
      "loss": 0.9837,
      "step": 1704
    },
    {
      "epoch": 0.1364,
      "grad_norm": 0.33686161041259766,
      "learning_rate": 0.00019093212428323778,
      "loss": 0.5713,
      "step": 1705
    },
    {
      "epoch": 0.13648,
      "grad_norm": 0.33739563822746277,
      "learning_rate": 0.00019092679023869852,
      "loss": 0.9868,
      "step": 1706
    },
    {
      "epoch": 0.13656,
      "grad_norm": 0.3339698910713196,
      "learning_rate": 0.00019092145619415923,
      "loss": 0.9162,
      "step": 1707
    },
    {
      "epoch": 0.13664,
      "grad_norm": 0.34037554264068604,
      "learning_rate": 0.00019091612214961997,
      "loss": 0.7731,
      "step": 1708
    },
    {
      "epoch": 0.13672,
      "grad_norm": 0.29028940200805664,
      "learning_rate": 0.0001909107881050807,
      "loss": 0.6206,
      "step": 1709
    },
    {
      "epoch": 0.1368,
      "grad_norm": 0.31766825914382935,
      "learning_rate": 0.00019090545406054142,
      "loss": 0.5417,
      "step": 1710
    },
    {
      "epoch": 0.13688,
      "grad_norm": 0.36927810311317444,
      "learning_rate": 0.00019090012001600216,
      "loss": 0.738,
      "step": 1711
    },
    {
      "epoch": 0.13696,
      "grad_norm": 0.4449663758277893,
      "learning_rate": 0.00019089478597146287,
      "loss": 0.8541,
      "step": 1712
    },
    {
      "epoch": 0.13704,
      "grad_norm": 0.3472079038619995,
      "learning_rate": 0.0001908894519269236,
      "loss": 0.7053,
      "step": 1713
    },
    {
      "epoch": 0.13712,
      "grad_norm": 0.33160945773124695,
      "learning_rate": 0.00019088411788238433,
      "loss": 0.8721,
      "step": 1714
    },
    {
      "epoch": 0.1372,
      "grad_norm": 0.4200536608695984,
      "learning_rate": 0.00019087878383784507,
      "loss": 0.963,
      "step": 1715
    },
    {
      "epoch": 0.13728,
      "grad_norm": 0.4304327666759491,
      "learning_rate": 0.0001908734497933058,
      "loss": 0.7146,
      "step": 1716
    },
    {
      "epoch": 0.13736,
      "grad_norm": 0.29683810472488403,
      "learning_rate": 0.00019086811574876652,
      "loss": 0.5889,
      "step": 1717
    },
    {
      "epoch": 0.13744,
      "grad_norm": 0.4201640784740448,
      "learning_rate": 0.00019086278170422726,
      "loss": 1.1458,
      "step": 1718
    },
    {
      "epoch": 0.13752,
      "grad_norm": 0.3972363770008087,
      "learning_rate": 0.00019085744765968797,
      "loss": 0.8344,
      "step": 1719
    },
    {
      "epoch": 0.1376,
      "grad_norm": 0.49249476194381714,
      "learning_rate": 0.0001908521136151487,
      "loss": 0.9122,
      "step": 1720
    },
    {
      "epoch": 0.13768,
      "grad_norm": 0.3223958909511566,
      "learning_rate": 0.00019084677957060942,
      "loss": 0.6586,
      "step": 1721
    },
    {
      "epoch": 0.13776,
      "grad_norm": 0.38372424244880676,
      "learning_rate": 0.00019084144552607016,
      "loss": 0.6939,
      "step": 1722
    },
    {
      "epoch": 0.13784,
      "grad_norm": 0.3806118369102478,
      "learning_rate": 0.00019083611148153088,
      "loss": 0.632,
      "step": 1723
    },
    {
      "epoch": 0.13792,
      "grad_norm": 0.357939749956131,
      "learning_rate": 0.00019083077743699162,
      "loss": 1.0696,
      "step": 1724
    },
    {
      "epoch": 0.138,
      "grad_norm": 0.2967957854270935,
      "learning_rate": 0.00019082544339245233,
      "loss": 0.926,
      "step": 1725
    },
    {
      "epoch": 0.13808,
      "grad_norm": 0.4410206079483032,
      "learning_rate": 0.00019082010934791307,
      "loss": 0.9848,
      "step": 1726
    },
    {
      "epoch": 0.13816,
      "grad_norm": 0.34677985310554504,
      "learning_rate": 0.00019081477530337378,
      "loss": 0.7906,
      "step": 1727
    },
    {
      "epoch": 0.13824,
      "grad_norm": 0.3576519787311554,
      "learning_rate": 0.00019080944125883452,
      "loss": 0.925,
      "step": 1728
    },
    {
      "epoch": 0.13832,
      "grad_norm": 0.403618186712265,
      "learning_rate": 0.00019080410721429523,
      "loss": 1.0541,
      "step": 1729
    },
    {
      "epoch": 0.1384,
      "grad_norm": 0.34622374176979065,
      "learning_rate": 0.00019079877316975597,
      "loss": 0.8218,
      "step": 1730
    },
    {
      "epoch": 0.13848,
      "grad_norm": 0.4729074537754059,
      "learning_rate": 0.0001907934391252167,
      "loss": 0.7357,
      "step": 1731
    },
    {
      "epoch": 0.13856,
      "grad_norm": 0.3594292998313904,
      "learning_rate": 0.00019078810508067743,
      "loss": 0.767,
      "step": 1732
    },
    {
      "epoch": 0.13864,
      "grad_norm": 0.39482545852661133,
      "learning_rate": 0.00019078277103613817,
      "loss": 0.6418,
      "step": 1733
    },
    {
      "epoch": 0.13872,
      "grad_norm": 0.31441211700439453,
      "learning_rate": 0.00019077743699159888,
      "loss": 0.6519,
      "step": 1734
    },
    {
      "epoch": 0.1388,
      "grad_norm": 0.31159669160842896,
      "learning_rate": 0.00019077210294705962,
      "loss": 1.0605,
      "step": 1735
    },
    {
      "epoch": 0.13888,
      "grad_norm": 0.3420369327068329,
      "learning_rate": 0.00019076676890252033,
      "loss": 0.7754,
      "step": 1736
    },
    {
      "epoch": 0.13896,
      "grad_norm": 0.3011907935142517,
      "learning_rate": 0.00019076143485798107,
      "loss": 0.9278,
      "step": 1737
    },
    {
      "epoch": 0.13904,
      "grad_norm": 0.3273962438106537,
      "learning_rate": 0.00019075610081344178,
      "loss": 0.9501,
      "step": 1738
    },
    {
      "epoch": 0.13912,
      "grad_norm": 0.28358542919158936,
      "learning_rate": 0.00019075076676890252,
      "loss": 0.49,
      "step": 1739
    },
    {
      "epoch": 0.1392,
      "grad_norm": 0.353495717048645,
      "learning_rate": 0.00019074543272436324,
      "loss": 1.0936,
      "step": 1740
    },
    {
      "epoch": 0.13928,
      "grad_norm": 0.2930774390697479,
      "learning_rate": 0.00019074009867982398,
      "loss": 0.5286,
      "step": 1741
    },
    {
      "epoch": 0.13936,
      "grad_norm": 0.27495917677879333,
      "learning_rate": 0.00019073476463528472,
      "loss": 0.5728,
      "step": 1742
    },
    {
      "epoch": 0.13944,
      "grad_norm": 0.4118700921535492,
      "learning_rate": 0.00019072943059074543,
      "loss": 0.6518,
      "step": 1743
    },
    {
      "epoch": 0.13952,
      "grad_norm": 0.28815796971321106,
      "learning_rate": 0.00019072409654620617,
      "loss": 0.7814,
      "step": 1744
    },
    {
      "epoch": 0.1396,
      "grad_norm": 0.3847582936286926,
      "learning_rate": 0.00019071876250166688,
      "loss": 0.8951,
      "step": 1745
    },
    {
      "epoch": 0.13968,
      "grad_norm": 0.3221271336078644,
      "learning_rate": 0.00019071342845712762,
      "loss": 0.9312,
      "step": 1746
    },
    {
      "epoch": 0.13976,
      "grad_norm": 0.2909730076789856,
      "learning_rate": 0.00019070809441258833,
      "loss": 0.6811,
      "step": 1747
    },
    {
      "epoch": 0.13984,
      "grad_norm": 0.44733768701553345,
      "learning_rate": 0.00019070276036804907,
      "loss": 0.9593,
      "step": 1748
    },
    {
      "epoch": 0.13992,
      "grad_norm": 0.3776947259902954,
      "learning_rate": 0.0001906974263235098,
      "loss": 0.8502,
      "step": 1749
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.2886708974838257,
      "learning_rate": 0.00019069209227897053,
      "loss": 0.8644,
      "step": 1750
    },
    {
      "epoch": 0.14008,
      "grad_norm": 0.41109004616737366,
      "learning_rate": 0.00019068675823443127,
      "loss": 0.641,
      "step": 1751
    },
    {
      "epoch": 0.14016,
      "grad_norm": 0.3105577826499939,
      "learning_rate": 0.00019068142418989198,
      "loss": 0.5395,
      "step": 1752
    },
    {
      "epoch": 0.14024,
      "grad_norm": 0.299919992685318,
      "learning_rate": 0.00019067609014535272,
      "loss": 1.1005,
      "step": 1753
    },
    {
      "epoch": 0.14032,
      "grad_norm": 0.3594439625740051,
      "learning_rate": 0.00019067075610081343,
      "loss": 0.8973,
      "step": 1754
    },
    {
      "epoch": 0.1404,
      "grad_norm": 0.3378618359565735,
      "learning_rate": 0.00019066542205627417,
      "loss": 0.7672,
      "step": 1755
    },
    {
      "epoch": 0.14048,
      "grad_norm": 0.36424243450164795,
      "learning_rate": 0.0001906600880117349,
      "loss": 0.8832,
      "step": 1756
    },
    {
      "epoch": 0.14056,
      "grad_norm": 0.3384382724761963,
      "learning_rate": 0.00019065475396719562,
      "loss": 0.8016,
      "step": 1757
    },
    {
      "epoch": 0.14064,
      "grad_norm": 0.4795299768447876,
      "learning_rate": 0.00019064941992265636,
      "loss": 0.8091,
      "step": 1758
    },
    {
      "epoch": 0.14072,
      "grad_norm": 0.3534773886203766,
      "learning_rate": 0.00019064408587811708,
      "loss": 0.9221,
      "step": 1759
    },
    {
      "epoch": 0.1408,
      "grad_norm": 0.30566856265068054,
      "learning_rate": 0.00019063875183357782,
      "loss": 0.8634,
      "step": 1760
    },
    {
      "epoch": 0.14088,
      "grad_norm": 0.3406311869621277,
      "learning_rate": 0.00019063341778903853,
      "loss": 1.1274,
      "step": 1761
    },
    {
      "epoch": 0.14096,
      "grad_norm": 0.24324150383472443,
      "learning_rate": 0.00019062808374449927,
      "loss": 1.0419,
      "step": 1762
    },
    {
      "epoch": 0.14104,
      "grad_norm": 0.273953378200531,
      "learning_rate": 0.00019062274969996,
      "loss": 0.5668,
      "step": 1763
    },
    {
      "epoch": 0.14112,
      "grad_norm": 0.3092738389968872,
      "learning_rate": 0.00019061741565542072,
      "loss": 1.0075,
      "step": 1764
    },
    {
      "epoch": 0.1412,
      "grad_norm": 0.3191870450973511,
      "learning_rate": 0.00019061208161088146,
      "loss": 0.9409,
      "step": 1765
    },
    {
      "epoch": 0.14128,
      "grad_norm": 0.3745952248573303,
      "learning_rate": 0.00019060674756634217,
      "loss": 0.7363,
      "step": 1766
    },
    {
      "epoch": 0.14136,
      "grad_norm": 0.3179483711719513,
      "learning_rate": 0.00019060141352180291,
      "loss": 0.9703,
      "step": 1767
    },
    {
      "epoch": 0.14144,
      "grad_norm": 0.2799185812473297,
      "learning_rate": 0.00019059607947726363,
      "loss": 0.5539,
      "step": 1768
    },
    {
      "epoch": 0.14152,
      "grad_norm": 0.3482363522052765,
      "learning_rate": 0.00019059074543272437,
      "loss": 1.0433,
      "step": 1769
    },
    {
      "epoch": 0.1416,
      "grad_norm": 0.33935022354125977,
      "learning_rate": 0.0001905854113881851,
      "loss": 0.5375,
      "step": 1770
    },
    {
      "epoch": 0.14168,
      "grad_norm": 0.32514917850494385,
      "learning_rate": 0.00019058007734364582,
      "loss": 0.6041,
      "step": 1771
    },
    {
      "epoch": 0.14176,
      "grad_norm": 0.3554004430770874,
      "learning_rate": 0.00019057474329910656,
      "loss": 1.2111,
      "step": 1772
    },
    {
      "epoch": 0.14184,
      "grad_norm": 0.305288165807724,
      "learning_rate": 0.00019056940925456727,
      "loss": 0.7358,
      "step": 1773
    },
    {
      "epoch": 0.14192,
      "grad_norm": 0.29261505603790283,
      "learning_rate": 0.000190564075210028,
      "loss": 0.6545,
      "step": 1774
    },
    {
      "epoch": 0.142,
      "grad_norm": 0.41505077481269836,
      "learning_rate": 0.00019055874116548872,
      "loss": 0.9723,
      "step": 1775
    },
    {
      "epoch": 0.14208,
      "grad_norm": 0.3695387542247772,
      "learning_rate": 0.00019055340712094946,
      "loss": 0.7295,
      "step": 1776
    },
    {
      "epoch": 0.14216,
      "grad_norm": 0.3562374413013458,
      "learning_rate": 0.0001905480730764102,
      "loss": 0.6205,
      "step": 1777
    },
    {
      "epoch": 0.14224,
      "grad_norm": 0.3443949818611145,
      "learning_rate": 0.00019054273903187092,
      "loss": 1.0054,
      "step": 1778
    },
    {
      "epoch": 0.14232,
      "grad_norm": 0.4132959544658661,
      "learning_rate": 0.00019053740498733166,
      "loss": 0.7711,
      "step": 1779
    },
    {
      "epoch": 0.1424,
      "grad_norm": 0.3094005584716797,
      "learning_rate": 0.00019053207094279237,
      "loss": 0.7897,
      "step": 1780
    },
    {
      "epoch": 0.14248,
      "grad_norm": 0.2722277343273163,
      "learning_rate": 0.0001905267368982531,
      "loss": 0.723,
      "step": 1781
    },
    {
      "epoch": 0.14256,
      "grad_norm": 0.41849246621131897,
      "learning_rate": 0.00019052140285371385,
      "loss": 0.6735,
      "step": 1782
    },
    {
      "epoch": 0.14264,
      "grad_norm": 0.4273959994316101,
      "learning_rate": 0.00019051606880917456,
      "loss": 0.915,
      "step": 1783
    },
    {
      "epoch": 0.14272,
      "grad_norm": 0.36731627583503723,
      "learning_rate": 0.0001905107347646353,
      "loss": 1.1454,
      "step": 1784
    },
    {
      "epoch": 0.1428,
      "grad_norm": 0.3778495490550995,
      "learning_rate": 0.00019050540072009601,
      "loss": 0.8391,
      "step": 1785
    },
    {
      "epoch": 0.14288,
      "grad_norm": 0.30477455258369446,
      "learning_rate": 0.00019050006667555675,
      "loss": 0.5446,
      "step": 1786
    },
    {
      "epoch": 0.14296,
      "grad_norm": 0.3059173822402954,
      "learning_rate": 0.00019049473263101747,
      "loss": 0.88,
      "step": 1787
    },
    {
      "epoch": 0.14304,
      "grad_norm": 0.2571353614330292,
      "learning_rate": 0.0001904893985864782,
      "loss": 0.9636,
      "step": 1788
    },
    {
      "epoch": 0.14312,
      "grad_norm": 0.3908557891845703,
      "learning_rate": 0.00019048406454193895,
      "loss": 0.5746,
      "step": 1789
    },
    {
      "epoch": 0.1432,
      "grad_norm": 0.33189070224761963,
      "learning_rate": 0.00019047873049739966,
      "loss": 0.9396,
      "step": 1790
    },
    {
      "epoch": 0.14328,
      "grad_norm": 0.3645264804363251,
      "learning_rate": 0.0001904733964528604,
      "loss": 0.8168,
      "step": 1791
    },
    {
      "epoch": 0.14336,
      "grad_norm": 0.260669469833374,
      "learning_rate": 0.0001904680624083211,
      "loss": 0.7023,
      "step": 1792
    },
    {
      "epoch": 0.14344,
      "grad_norm": 0.27146902680397034,
      "learning_rate": 0.00019046272836378185,
      "loss": 1.0395,
      "step": 1793
    },
    {
      "epoch": 0.14352,
      "grad_norm": 0.47764602303504944,
      "learning_rate": 0.00019045739431924256,
      "loss": 1.0052,
      "step": 1794
    },
    {
      "epoch": 0.1436,
      "grad_norm": 0.29430094361305237,
      "learning_rate": 0.0001904520602747033,
      "loss": 0.6654,
      "step": 1795
    },
    {
      "epoch": 0.14368,
      "grad_norm": 0.29378846287727356,
      "learning_rate": 0.00019044672623016404,
      "loss": 0.7477,
      "step": 1796
    },
    {
      "epoch": 0.14376,
      "grad_norm": 0.37945911288261414,
      "learning_rate": 0.00019044139218562476,
      "loss": 0.5642,
      "step": 1797
    },
    {
      "epoch": 0.14384,
      "grad_norm": 0.3880833685398102,
      "learning_rate": 0.0001904360581410855,
      "loss": 0.5652,
      "step": 1798
    },
    {
      "epoch": 0.14392,
      "grad_norm": 0.441728800535202,
      "learning_rate": 0.0001904307240965462,
      "loss": 0.7949,
      "step": 1799
    },
    {
      "epoch": 0.144,
      "grad_norm": 0.31394973397254944,
      "learning_rate": 0.00019042539005200695,
      "loss": 1.1602,
      "step": 1800
    },
    {
      "epoch": 0.14408,
      "grad_norm": 0.36574360728263855,
      "learning_rate": 0.00019042005600746766,
      "loss": 0.8447,
      "step": 1801
    },
    {
      "epoch": 0.14416,
      "grad_norm": 0.28528720140457153,
      "learning_rate": 0.0001904147219629284,
      "loss": 0.8654,
      "step": 1802
    },
    {
      "epoch": 0.14424,
      "grad_norm": 0.4263208210468292,
      "learning_rate": 0.00019040938791838914,
      "loss": 1.0338,
      "step": 1803
    },
    {
      "epoch": 0.14432,
      "grad_norm": 0.34467703104019165,
      "learning_rate": 0.00019040405387384985,
      "loss": 0.9812,
      "step": 1804
    },
    {
      "epoch": 0.1444,
      "grad_norm": 0.32737383246421814,
      "learning_rate": 0.0001903987198293106,
      "loss": 1.0411,
      "step": 1805
    },
    {
      "epoch": 0.14448,
      "grad_norm": 0.3616841435432434,
      "learning_rate": 0.0001903933857847713,
      "loss": 0.739,
      "step": 1806
    },
    {
      "epoch": 0.14456,
      "grad_norm": 0.3091464340686798,
      "learning_rate": 0.00019038805174023205,
      "loss": 0.8179,
      "step": 1807
    },
    {
      "epoch": 0.14464,
      "grad_norm": 0.4530462622642517,
      "learning_rate": 0.00019038271769569276,
      "loss": 0.777,
      "step": 1808
    },
    {
      "epoch": 0.14472,
      "grad_norm": 0.37861186265945435,
      "learning_rate": 0.0001903773836511535,
      "loss": 0.7759,
      "step": 1809
    },
    {
      "epoch": 0.1448,
      "grad_norm": 0.34168800711631775,
      "learning_rate": 0.00019037204960661424,
      "loss": 0.9052,
      "step": 1810
    },
    {
      "epoch": 0.14488,
      "grad_norm": 0.43220022320747375,
      "learning_rate": 0.00019036671556207495,
      "loss": 1.0133,
      "step": 1811
    },
    {
      "epoch": 0.14496,
      "grad_norm": 0.44072553515434265,
      "learning_rate": 0.0001903613815175357,
      "loss": 1.0429,
      "step": 1812
    },
    {
      "epoch": 0.14504,
      "grad_norm": 0.29417335987091064,
      "learning_rate": 0.0001903560474729964,
      "loss": 0.9016,
      "step": 1813
    },
    {
      "epoch": 0.14512,
      "grad_norm": 0.41087624430656433,
      "learning_rate": 0.00019035071342845714,
      "loss": 0.9263,
      "step": 1814
    },
    {
      "epoch": 0.1452,
      "grad_norm": 0.359635591506958,
      "learning_rate": 0.00019034537938391786,
      "loss": 0.7541,
      "step": 1815
    },
    {
      "epoch": 0.14528,
      "grad_norm": 0.3047042489051819,
      "learning_rate": 0.0001903400453393786,
      "loss": 0.8935,
      "step": 1816
    },
    {
      "epoch": 0.14536,
      "grad_norm": 0.35799217224121094,
      "learning_rate": 0.00019033471129483934,
      "loss": 0.7556,
      "step": 1817
    },
    {
      "epoch": 0.14544,
      "grad_norm": 0.37454697489738464,
      "learning_rate": 0.00019032937725030005,
      "loss": 1.0687,
      "step": 1818
    },
    {
      "epoch": 0.14552,
      "grad_norm": 0.42623475193977356,
      "learning_rate": 0.0001903240432057608,
      "loss": 0.9106,
      "step": 1819
    },
    {
      "epoch": 0.1456,
      "grad_norm": 0.3308521509170532,
      "learning_rate": 0.0001903187091612215,
      "loss": 0.7459,
      "step": 1820
    },
    {
      "epoch": 0.14568,
      "grad_norm": 0.3081994950771332,
      "learning_rate": 0.00019031337511668224,
      "loss": 0.6818,
      "step": 1821
    },
    {
      "epoch": 0.14576,
      "grad_norm": 0.25191980600357056,
      "learning_rate": 0.00019030804107214296,
      "loss": 0.5728,
      "step": 1822
    },
    {
      "epoch": 0.14584,
      "grad_norm": 0.32018181681632996,
      "learning_rate": 0.0001903027070276037,
      "loss": 1.0231,
      "step": 1823
    },
    {
      "epoch": 0.14592,
      "grad_norm": 0.35539868474006653,
      "learning_rate": 0.00019029737298306443,
      "loss": 1.3313,
      "step": 1824
    },
    {
      "epoch": 0.146,
      "grad_norm": 0.35589656233787537,
      "learning_rate": 0.00019029203893852515,
      "loss": 0.781,
      "step": 1825
    },
    {
      "epoch": 0.14608,
      "grad_norm": 0.45073017477989197,
      "learning_rate": 0.0001902867048939859,
      "loss": 0.8967,
      "step": 1826
    },
    {
      "epoch": 0.14616,
      "grad_norm": 0.47725027799606323,
      "learning_rate": 0.0001902813708494466,
      "loss": 1.2429,
      "step": 1827
    },
    {
      "epoch": 0.14624,
      "grad_norm": 0.4082988500595093,
      "learning_rate": 0.00019027603680490734,
      "loss": 0.6958,
      "step": 1828
    },
    {
      "epoch": 0.14632,
      "grad_norm": 0.4032094180583954,
      "learning_rate": 0.00019027070276036808,
      "loss": 0.8493,
      "step": 1829
    },
    {
      "epoch": 0.1464,
      "grad_norm": 0.3115529716014862,
      "learning_rate": 0.0001902653687158288,
      "loss": 0.7237,
      "step": 1830
    },
    {
      "epoch": 0.14648,
      "grad_norm": 0.34269362688064575,
      "learning_rate": 0.00019026003467128953,
      "loss": 1.0402,
      "step": 1831
    },
    {
      "epoch": 0.14656,
      "grad_norm": 0.4560100734233856,
      "learning_rate": 0.00019025470062675025,
      "loss": 1.1146,
      "step": 1832
    },
    {
      "epoch": 0.14664,
      "grad_norm": 0.3365338146686554,
      "learning_rate": 0.00019024936658221098,
      "loss": 0.8969,
      "step": 1833
    },
    {
      "epoch": 0.14672,
      "grad_norm": 0.30646467208862305,
      "learning_rate": 0.0001902440325376717,
      "loss": 0.6688,
      "step": 1834
    },
    {
      "epoch": 0.1468,
      "grad_norm": 0.33949410915374756,
      "learning_rate": 0.00019023869849313244,
      "loss": 1.1279,
      "step": 1835
    },
    {
      "epoch": 0.14688,
      "grad_norm": 0.37726888060569763,
      "learning_rate": 0.00019023336444859318,
      "loss": 0.6038,
      "step": 1836
    },
    {
      "epoch": 0.14696,
      "grad_norm": 0.6085631251335144,
      "learning_rate": 0.0001902280304040539,
      "loss": 1.0288,
      "step": 1837
    },
    {
      "epoch": 0.14704,
      "grad_norm": 0.3860204815864563,
      "learning_rate": 0.00019022269635951463,
      "loss": 0.8519,
      "step": 1838
    },
    {
      "epoch": 0.14712,
      "grad_norm": 0.4008105993270874,
      "learning_rate": 0.00019021736231497534,
      "loss": 1.0581,
      "step": 1839
    },
    {
      "epoch": 0.1472,
      "grad_norm": 0.4427426755428314,
      "learning_rate": 0.00019021202827043608,
      "loss": 0.8774,
      "step": 1840
    },
    {
      "epoch": 0.14728,
      "grad_norm": 0.2778158187866211,
      "learning_rate": 0.0001902066942258968,
      "loss": 0.5443,
      "step": 1841
    },
    {
      "epoch": 0.14736,
      "grad_norm": 0.4020811915397644,
      "learning_rate": 0.00019020136018135754,
      "loss": 0.8852,
      "step": 1842
    },
    {
      "epoch": 0.14744,
      "grad_norm": 0.2743571698665619,
      "learning_rate": 0.00019019602613681828,
      "loss": 0.6563,
      "step": 1843
    },
    {
      "epoch": 0.14752,
      "grad_norm": 0.3163749873638153,
      "learning_rate": 0.000190190692092279,
      "loss": 1.096,
      "step": 1844
    },
    {
      "epoch": 0.1476,
      "grad_norm": 0.43117231130599976,
      "learning_rate": 0.00019018535804773973,
      "loss": 0.7729,
      "step": 1845
    },
    {
      "epoch": 0.14768,
      "grad_norm": 0.3464510440826416,
      "learning_rate": 0.00019018002400320044,
      "loss": 0.8615,
      "step": 1846
    },
    {
      "epoch": 0.14776,
      "grad_norm": 0.2780773639678955,
      "learning_rate": 0.00019017468995866118,
      "loss": 0.7819,
      "step": 1847
    },
    {
      "epoch": 0.14784,
      "grad_norm": 0.378162682056427,
      "learning_rate": 0.0001901693559141219,
      "loss": 0.9096,
      "step": 1848
    },
    {
      "epoch": 0.14792,
      "grad_norm": 0.4963836371898651,
      "learning_rate": 0.00019016402186958263,
      "loss": 0.8526,
      "step": 1849
    },
    {
      "epoch": 0.148,
      "grad_norm": 0.3542135953903198,
      "learning_rate": 0.00019015868782504335,
      "loss": 0.7507,
      "step": 1850
    },
    {
      "epoch": 0.14808,
      "grad_norm": 0.3663720488548279,
      "learning_rate": 0.00019015335378050409,
      "loss": 0.7142,
      "step": 1851
    },
    {
      "epoch": 0.14816,
      "grad_norm": 0.3243033289909363,
      "learning_rate": 0.0001901480197359648,
      "loss": 0.7067,
      "step": 1852
    },
    {
      "epoch": 0.14824,
      "grad_norm": 0.3350115716457367,
      "learning_rate": 0.00019014268569142554,
      "loss": 0.7774,
      "step": 1853
    },
    {
      "epoch": 0.14832,
      "grad_norm": 0.3803846836090088,
      "learning_rate": 0.00019013735164688625,
      "loss": 1.1354,
      "step": 1854
    },
    {
      "epoch": 0.1484,
      "grad_norm": 0.42437002062797546,
      "learning_rate": 0.000190132017602347,
      "loss": 0.8824,
      "step": 1855
    },
    {
      "epoch": 0.14848,
      "grad_norm": 0.4049728214740753,
      "learning_rate": 0.0001901266835578077,
      "loss": 0.8524,
      "step": 1856
    },
    {
      "epoch": 0.14856,
      "grad_norm": 0.336567223072052,
      "learning_rate": 0.00019012134951326844,
      "loss": 0.8121,
      "step": 1857
    },
    {
      "epoch": 0.14864,
      "grad_norm": 0.42962923645973206,
      "learning_rate": 0.00019011601546872916,
      "loss": 0.8686,
      "step": 1858
    },
    {
      "epoch": 0.14872,
      "grad_norm": 0.42353928089141846,
      "learning_rate": 0.0001901106814241899,
      "loss": 0.9454,
      "step": 1859
    },
    {
      "epoch": 0.1488,
      "grad_norm": 0.3823374807834625,
      "learning_rate": 0.00019010534737965064,
      "loss": 0.547,
      "step": 1860
    },
    {
      "epoch": 0.14888,
      "grad_norm": 0.47625353932380676,
      "learning_rate": 0.00019010001333511135,
      "loss": 1.1275,
      "step": 1861
    },
    {
      "epoch": 0.14896,
      "grad_norm": 0.24935178458690643,
      "learning_rate": 0.0001900946792905721,
      "loss": 0.7585,
      "step": 1862
    },
    {
      "epoch": 0.14904,
      "grad_norm": 0.30081436038017273,
      "learning_rate": 0.0001900893452460328,
      "loss": 1.0953,
      "step": 1863
    },
    {
      "epoch": 0.14912,
      "grad_norm": 0.23759295046329498,
      "learning_rate": 0.00019008401120149354,
      "loss": 0.7121,
      "step": 1864
    },
    {
      "epoch": 0.1492,
      "grad_norm": 0.3114490509033203,
      "learning_rate": 0.00019007867715695425,
      "loss": 0.6087,
      "step": 1865
    },
    {
      "epoch": 0.14928,
      "grad_norm": 0.303237646818161,
      "learning_rate": 0.000190073343112415,
      "loss": 0.7126,
      "step": 1866
    },
    {
      "epoch": 0.14936,
      "grad_norm": 0.3199152648448944,
      "learning_rate": 0.0001900680090678757,
      "loss": 0.9358,
      "step": 1867
    },
    {
      "epoch": 0.14944,
      "grad_norm": 0.3768390119075775,
      "learning_rate": 0.00019006267502333645,
      "loss": 1.0717,
      "step": 1868
    },
    {
      "epoch": 0.14952,
      "grad_norm": 0.3270982503890991,
      "learning_rate": 0.00019005734097879716,
      "loss": 0.7138,
      "step": 1869
    },
    {
      "epoch": 0.1496,
      "grad_norm": 0.327020525932312,
      "learning_rate": 0.0001900520069342579,
      "loss": 0.6788,
      "step": 1870
    },
    {
      "epoch": 0.14968,
      "grad_norm": 0.21309424936771393,
      "learning_rate": 0.00019004667288971864,
      "loss": 1.0036,
      "step": 1871
    },
    {
      "epoch": 0.14976,
      "grad_norm": 0.35742273926734924,
      "learning_rate": 0.00019004133884517935,
      "loss": 0.9835,
      "step": 1872
    },
    {
      "epoch": 0.14984,
      "grad_norm": 0.35370075702667236,
      "learning_rate": 0.0001900360048006401,
      "loss": 0.9403,
      "step": 1873
    },
    {
      "epoch": 0.14992,
      "grad_norm": 0.31914883852005005,
      "learning_rate": 0.0001900306707561008,
      "loss": 0.713,
      "step": 1874
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.3182317018508911,
      "learning_rate": 0.00019002533671156154,
      "loss": 0.8138,
      "step": 1875
    },
    {
      "epoch": 0.15008,
      "grad_norm": 0.2837666869163513,
      "learning_rate": 0.00019002000266702226,
      "loss": 0.5549,
      "step": 1876
    },
    {
      "epoch": 0.15016,
      "grad_norm": 0.31721535325050354,
      "learning_rate": 0.000190014668622483,
      "loss": 0.9385,
      "step": 1877
    },
    {
      "epoch": 0.15024,
      "grad_norm": 0.30024293065071106,
      "learning_rate": 0.00019000933457794374,
      "loss": 0.4913,
      "step": 1878
    },
    {
      "epoch": 0.15032,
      "grad_norm": 0.3166831135749817,
      "learning_rate": 0.00019000400053340445,
      "loss": 0.9759,
      "step": 1879
    },
    {
      "epoch": 0.1504,
      "grad_norm": 0.3050183653831482,
      "learning_rate": 0.0001899986664888652,
      "loss": 1.1355,
      "step": 1880
    },
    {
      "epoch": 0.15048,
      "grad_norm": 0.3389085531234741,
      "learning_rate": 0.0001899933324443259,
      "loss": 0.7593,
      "step": 1881
    },
    {
      "epoch": 0.15056,
      "grad_norm": 0.22686898708343506,
      "learning_rate": 0.00018998799839978664,
      "loss": 1.0968,
      "step": 1882
    },
    {
      "epoch": 0.15064,
      "grad_norm": 0.3735449016094208,
      "learning_rate": 0.00018998266435524738,
      "loss": 0.8163,
      "step": 1883
    },
    {
      "epoch": 0.15072,
      "grad_norm": 0.3843504786491394,
      "learning_rate": 0.0001899773303107081,
      "loss": 1.008,
      "step": 1884
    },
    {
      "epoch": 0.1508,
      "grad_norm": 0.34816813468933105,
      "learning_rate": 0.00018997199626616883,
      "loss": 0.8633,
      "step": 1885
    },
    {
      "epoch": 0.15088,
      "grad_norm": 0.29743126034736633,
      "learning_rate": 0.00018996666222162955,
      "loss": 1.0931,
      "step": 1886
    },
    {
      "epoch": 0.15096,
      "grad_norm": 0.37629592418670654,
      "learning_rate": 0.00018996132817709029,
      "loss": 0.977,
      "step": 1887
    },
    {
      "epoch": 0.15104,
      "grad_norm": 0.26351088285446167,
      "learning_rate": 0.000189955994132551,
      "loss": 0.4579,
      "step": 1888
    },
    {
      "epoch": 0.15112,
      "grad_norm": 0.3220629394054413,
      "learning_rate": 0.00018995066008801174,
      "loss": 0.79,
      "step": 1889
    },
    {
      "epoch": 0.1512,
      "grad_norm": 0.3295590281486511,
      "learning_rate": 0.00018994532604347248,
      "loss": 0.6607,
      "step": 1890
    },
    {
      "epoch": 0.15128,
      "grad_norm": 0.4594476521015167,
      "learning_rate": 0.0001899399919989332,
      "loss": 0.6551,
      "step": 1891
    },
    {
      "epoch": 0.15136,
      "grad_norm": 0.32685840129852295,
      "learning_rate": 0.00018993465795439393,
      "loss": 0.7053,
      "step": 1892
    },
    {
      "epoch": 0.15144,
      "grad_norm": 0.3915022015571594,
      "learning_rate": 0.00018992932390985464,
      "loss": 1.0801,
      "step": 1893
    },
    {
      "epoch": 0.15152,
      "grad_norm": 0.40324583649635315,
      "learning_rate": 0.00018992398986531538,
      "loss": 0.9356,
      "step": 1894
    },
    {
      "epoch": 0.1516,
      "grad_norm": 0.34946396946907043,
      "learning_rate": 0.0001899186558207761,
      "loss": 0.9242,
      "step": 1895
    },
    {
      "epoch": 0.15168,
      "grad_norm": 0.3624553084373474,
      "learning_rate": 0.00018991332177623684,
      "loss": 0.7319,
      "step": 1896
    },
    {
      "epoch": 0.15176,
      "grad_norm": 0.3020382821559906,
      "learning_rate": 0.00018990798773169758,
      "loss": 0.8746,
      "step": 1897
    },
    {
      "epoch": 0.15184,
      "grad_norm": 0.32483506202697754,
      "learning_rate": 0.0001899026536871583,
      "loss": 0.8213,
      "step": 1898
    },
    {
      "epoch": 0.15192,
      "grad_norm": 0.3479045033454895,
      "learning_rate": 0.00018989731964261903,
      "loss": 0.822,
      "step": 1899
    },
    {
      "epoch": 0.152,
      "grad_norm": 0.49086153507232666,
      "learning_rate": 0.00018989198559807974,
      "loss": 0.873,
      "step": 1900
    },
    {
      "epoch": 0.15208,
      "grad_norm": 0.3191189169883728,
      "learning_rate": 0.00018988665155354048,
      "loss": 1.0552,
      "step": 1901
    },
    {
      "epoch": 0.15216,
      "grad_norm": 0.2758271098136902,
      "learning_rate": 0.0001898813175090012,
      "loss": 0.8007,
      "step": 1902
    },
    {
      "epoch": 0.15224,
      "grad_norm": 0.35208263993263245,
      "learning_rate": 0.00018987598346446193,
      "loss": 0.9492,
      "step": 1903
    },
    {
      "epoch": 0.15232,
      "grad_norm": 0.3458918631076813,
      "learning_rate": 0.00018987064941992267,
      "loss": 0.5893,
      "step": 1904
    },
    {
      "epoch": 0.1524,
      "grad_norm": 0.36584070324897766,
      "learning_rate": 0.0001898653153753834,
      "loss": 0.7232,
      "step": 1905
    },
    {
      "epoch": 0.15248,
      "grad_norm": 0.25501134991645813,
      "learning_rate": 0.00018985998133084413,
      "loss": 0.8215,
      "step": 1906
    },
    {
      "epoch": 0.15256,
      "grad_norm": 0.32499682903289795,
      "learning_rate": 0.00018985464728630484,
      "loss": 0.6442,
      "step": 1907
    },
    {
      "epoch": 0.15264,
      "grad_norm": 0.3349858522415161,
      "learning_rate": 0.00018984931324176558,
      "loss": 1.02,
      "step": 1908
    },
    {
      "epoch": 0.15272,
      "grad_norm": 0.335511714220047,
      "learning_rate": 0.0001898439791972263,
      "loss": 0.6528,
      "step": 1909
    },
    {
      "epoch": 0.1528,
      "grad_norm": 0.3467414379119873,
      "learning_rate": 0.00018983864515268703,
      "loss": 1.0964,
      "step": 1910
    },
    {
      "epoch": 0.15288,
      "grad_norm": 0.40261945128440857,
      "learning_rate": 0.00018983331110814777,
      "loss": 0.8981,
      "step": 1911
    },
    {
      "epoch": 0.15296,
      "grad_norm": 0.3460160195827484,
      "learning_rate": 0.00018982797706360848,
      "loss": 0.7791,
      "step": 1912
    },
    {
      "epoch": 0.15304,
      "grad_norm": 0.27386143803596497,
      "learning_rate": 0.00018982264301906922,
      "loss": 0.8361,
      "step": 1913
    },
    {
      "epoch": 0.15312,
      "grad_norm": 0.4073110520839691,
      "learning_rate": 0.00018981730897452994,
      "loss": 0.8193,
      "step": 1914
    },
    {
      "epoch": 0.1532,
      "grad_norm": 0.38541924953460693,
      "learning_rate": 0.00018981197492999068,
      "loss": 0.6124,
      "step": 1915
    },
    {
      "epoch": 0.15328,
      "grad_norm": 0.2412843108177185,
      "learning_rate": 0.0001898066408854514,
      "loss": 0.4586,
      "step": 1916
    },
    {
      "epoch": 0.15336,
      "grad_norm": 0.3325851857662201,
      "learning_rate": 0.00018980130684091213,
      "loss": 1.0286,
      "step": 1917
    },
    {
      "epoch": 0.15344,
      "grad_norm": 0.3710291087627411,
      "learning_rate": 0.00018979597279637287,
      "loss": 0.7894,
      "step": 1918
    },
    {
      "epoch": 0.15352,
      "grad_norm": 0.31341174244880676,
      "learning_rate": 0.00018979063875183358,
      "loss": 0.9363,
      "step": 1919
    },
    {
      "epoch": 0.1536,
      "grad_norm": 0.29548314213752747,
      "learning_rate": 0.00018978530470729432,
      "loss": 0.9367,
      "step": 1920
    },
    {
      "epoch": 0.15368,
      "grad_norm": 0.30683058500289917,
      "learning_rate": 0.00018977997066275503,
      "loss": 0.8165,
      "step": 1921
    },
    {
      "epoch": 0.15376,
      "grad_norm": 0.3626696467399597,
      "learning_rate": 0.00018977463661821577,
      "loss": 0.9682,
      "step": 1922
    },
    {
      "epoch": 0.15384,
      "grad_norm": 0.35924434661865234,
      "learning_rate": 0.0001897693025736765,
      "loss": 0.6721,
      "step": 1923
    },
    {
      "epoch": 0.15392,
      "grad_norm": 0.34526845812797546,
      "learning_rate": 0.00018976396852913723,
      "loss": 0.8403,
      "step": 1924
    },
    {
      "epoch": 0.154,
      "grad_norm": 0.37799954414367676,
      "learning_rate": 0.00018975863448459797,
      "loss": 1.0149,
      "step": 1925
    },
    {
      "epoch": 0.15408,
      "grad_norm": 0.31785494089126587,
      "learning_rate": 0.00018975330044005868,
      "loss": 1.0367,
      "step": 1926
    },
    {
      "epoch": 0.15416,
      "grad_norm": 0.3990667760372162,
      "learning_rate": 0.00018974796639551942,
      "loss": 0.9331,
      "step": 1927
    },
    {
      "epoch": 0.15424,
      "grad_norm": 0.4495895206928253,
      "learning_rate": 0.00018974263235098013,
      "loss": 1.3374,
      "step": 1928
    },
    {
      "epoch": 0.15432,
      "grad_norm": 0.3459828197956085,
      "learning_rate": 0.00018973729830644087,
      "loss": 1.0002,
      "step": 1929
    },
    {
      "epoch": 0.1544,
      "grad_norm": 0.31268882751464844,
      "learning_rate": 0.0001897319642619016,
      "loss": 0.6571,
      "step": 1930
    },
    {
      "epoch": 0.15448,
      "grad_norm": 0.34241536259651184,
      "learning_rate": 0.00018972663021736232,
      "loss": 1.0911,
      "step": 1931
    },
    {
      "epoch": 0.15456,
      "grad_norm": 0.4362037181854248,
      "learning_rate": 0.00018972129617282306,
      "loss": 0.7762,
      "step": 1932
    },
    {
      "epoch": 0.15464,
      "grad_norm": 0.4256126582622528,
      "learning_rate": 0.00018971596212828378,
      "loss": 0.9433,
      "step": 1933
    },
    {
      "epoch": 0.15472,
      "grad_norm": 0.31788045167922974,
      "learning_rate": 0.00018971062808374452,
      "loss": 0.832,
      "step": 1934
    },
    {
      "epoch": 0.1548,
      "grad_norm": 0.3223385214805603,
      "learning_rate": 0.00018970529403920523,
      "loss": 0.9581,
      "step": 1935
    },
    {
      "epoch": 0.15488,
      "grad_norm": 0.3136177659034729,
      "learning_rate": 0.00018969995999466597,
      "loss": 1.2528,
      "step": 1936
    },
    {
      "epoch": 0.15496,
      "grad_norm": 0.3421786427497864,
      "learning_rate": 0.0001896946259501267,
      "loss": 0.82,
      "step": 1937
    },
    {
      "epoch": 0.15504,
      "grad_norm": 0.3036457598209381,
      "learning_rate": 0.00018968929190558742,
      "loss": 0.726,
      "step": 1938
    },
    {
      "epoch": 0.15512,
      "grad_norm": 0.36650899052619934,
      "learning_rate": 0.00018968395786104816,
      "loss": 0.6424,
      "step": 1939
    },
    {
      "epoch": 0.1552,
      "grad_norm": 0.30390048027038574,
      "learning_rate": 0.00018967862381650887,
      "loss": 0.8074,
      "step": 1940
    },
    {
      "epoch": 0.15528,
      "grad_norm": 0.3700094223022461,
      "learning_rate": 0.00018967328977196961,
      "loss": 0.768,
      "step": 1941
    },
    {
      "epoch": 0.15536,
      "grad_norm": 0.4036709666252136,
      "learning_rate": 0.00018966795572743033,
      "loss": 1.0148,
      "step": 1942
    },
    {
      "epoch": 0.15544,
      "grad_norm": 0.3982688784599304,
      "learning_rate": 0.00018966262168289107,
      "loss": 0.9606,
      "step": 1943
    },
    {
      "epoch": 0.15552,
      "grad_norm": 0.3578212261199951,
      "learning_rate": 0.0001896572876383518,
      "loss": 0.6904,
      "step": 1944
    },
    {
      "epoch": 0.1556,
      "grad_norm": 0.3469463884830475,
      "learning_rate": 0.00018965195359381252,
      "loss": 0.8212,
      "step": 1945
    },
    {
      "epoch": 0.15568,
      "grad_norm": 0.3440670967102051,
      "learning_rate": 0.00018964661954927326,
      "loss": 0.9089,
      "step": 1946
    },
    {
      "epoch": 0.15576,
      "grad_norm": 0.32569143176078796,
      "learning_rate": 0.00018964128550473397,
      "loss": 0.8239,
      "step": 1947
    },
    {
      "epoch": 0.15584,
      "grad_norm": 0.49528491497039795,
      "learning_rate": 0.0001896359514601947,
      "loss": 1.1134,
      "step": 1948
    },
    {
      "epoch": 0.15592,
      "grad_norm": 0.41150087118148804,
      "learning_rate": 0.00018963061741565543,
      "loss": 0.8695,
      "step": 1949
    },
    {
      "epoch": 0.156,
      "grad_norm": 0.35447242856025696,
      "learning_rate": 0.00018962528337111616,
      "loss": 0.8994,
      "step": 1950
    },
    {
      "epoch": 0.15608,
      "grad_norm": 0.3147229850292206,
      "learning_rate": 0.0001896199493265769,
      "loss": 1.0339,
      "step": 1951
    },
    {
      "epoch": 0.15616,
      "grad_norm": 0.36035922169685364,
      "learning_rate": 0.00018961461528203762,
      "loss": 0.5053,
      "step": 1952
    },
    {
      "epoch": 0.15624,
      "grad_norm": 0.30192530155181885,
      "learning_rate": 0.00018960928123749836,
      "loss": 1.0109,
      "step": 1953
    },
    {
      "epoch": 0.15632,
      "grad_norm": 0.3418175280094147,
      "learning_rate": 0.00018960394719295907,
      "loss": 0.8944,
      "step": 1954
    },
    {
      "epoch": 0.1564,
      "grad_norm": 0.38559988141059875,
      "learning_rate": 0.0001895986131484198,
      "loss": 0.6866,
      "step": 1955
    },
    {
      "epoch": 0.15648,
      "grad_norm": 0.27135539054870605,
      "learning_rate": 0.00018959327910388052,
      "loss": 0.7622,
      "step": 1956
    },
    {
      "epoch": 0.15656,
      "grad_norm": 0.3713832497596741,
      "learning_rate": 0.00018958794505934126,
      "loss": 0.6309,
      "step": 1957
    },
    {
      "epoch": 0.15664,
      "grad_norm": 0.41466179490089417,
      "learning_rate": 0.000189582611014802,
      "loss": 0.5494,
      "step": 1958
    },
    {
      "epoch": 0.15672,
      "grad_norm": 0.2726982533931732,
      "learning_rate": 0.00018957727697026272,
      "loss": 0.6509,
      "step": 1959
    },
    {
      "epoch": 0.1568,
      "grad_norm": 0.3127789795398712,
      "learning_rate": 0.00018957194292572345,
      "loss": 0.8944,
      "step": 1960
    },
    {
      "epoch": 0.15688,
      "grad_norm": 0.36977526545524597,
      "learning_rate": 0.00018956660888118417,
      "loss": 1.0799,
      "step": 1961
    },
    {
      "epoch": 0.15696,
      "grad_norm": 0.32471129298210144,
      "learning_rate": 0.0001895612748366449,
      "loss": 0.9533,
      "step": 1962
    },
    {
      "epoch": 0.15704,
      "grad_norm": 0.3450220227241516,
      "learning_rate": 0.00018955594079210562,
      "loss": 0.8799,
      "step": 1963
    },
    {
      "epoch": 0.15712,
      "grad_norm": 0.3104134202003479,
      "learning_rate": 0.00018955060674756636,
      "loss": 1.0657,
      "step": 1964
    },
    {
      "epoch": 0.1572,
      "grad_norm": 0.32126036286354065,
      "learning_rate": 0.0001895452727030271,
      "loss": 0.7703,
      "step": 1965
    },
    {
      "epoch": 0.15728,
      "grad_norm": 0.3518780767917633,
      "learning_rate": 0.0001895399386584878,
      "loss": 0.6577,
      "step": 1966
    },
    {
      "epoch": 0.15736,
      "grad_norm": 0.4494785666465759,
      "learning_rate": 0.00018953460461394855,
      "loss": 1.0968,
      "step": 1967
    },
    {
      "epoch": 0.15744,
      "grad_norm": 0.32139360904693604,
      "learning_rate": 0.00018952927056940927,
      "loss": 0.6998,
      "step": 1968
    },
    {
      "epoch": 0.15752,
      "grad_norm": 0.3583162724971771,
      "learning_rate": 0.00018952393652487,
      "loss": 0.862,
      "step": 1969
    },
    {
      "epoch": 0.1576,
      "grad_norm": 0.3830515146255493,
      "learning_rate": 0.00018951860248033072,
      "loss": 0.8732,
      "step": 1970
    },
    {
      "epoch": 0.15768,
      "grad_norm": 0.3025935888290405,
      "learning_rate": 0.00018951326843579146,
      "loss": 1.0475,
      "step": 1971
    },
    {
      "epoch": 0.15776,
      "grad_norm": 0.27729806303977966,
      "learning_rate": 0.0001895079343912522,
      "loss": 1.0347,
      "step": 1972
    },
    {
      "epoch": 0.15784,
      "grad_norm": 0.2656860053539276,
      "learning_rate": 0.0001895026003467129,
      "loss": 0.5413,
      "step": 1973
    },
    {
      "epoch": 0.15792,
      "grad_norm": 0.30812227725982666,
      "learning_rate": 0.00018949726630217365,
      "loss": 0.5985,
      "step": 1974
    },
    {
      "epoch": 0.158,
      "grad_norm": 0.3573472499847412,
      "learning_rate": 0.00018949193225763436,
      "loss": 0.6708,
      "step": 1975
    },
    {
      "epoch": 0.15808,
      "grad_norm": 0.43622326850891113,
      "learning_rate": 0.0001894865982130951,
      "loss": 0.8911,
      "step": 1976
    },
    {
      "epoch": 0.15816,
      "grad_norm": 0.35213714838027954,
      "learning_rate": 0.00018948126416855582,
      "loss": 0.5504,
      "step": 1977
    },
    {
      "epoch": 0.15824,
      "grad_norm": 0.32172220945358276,
      "learning_rate": 0.00018947593012401656,
      "loss": 0.6199,
      "step": 1978
    },
    {
      "epoch": 0.15832,
      "grad_norm": 0.2898867428302765,
      "learning_rate": 0.00018947059607947727,
      "loss": 0.6289,
      "step": 1979
    },
    {
      "epoch": 0.1584,
      "grad_norm": 0.33130261301994324,
      "learning_rate": 0.000189465262034938,
      "loss": 0.5686,
      "step": 1980
    },
    {
      "epoch": 0.15848,
      "grad_norm": 0.3193137049674988,
      "learning_rate": 0.00018945992799039872,
      "loss": 1.0169,
      "step": 1981
    },
    {
      "epoch": 0.15856,
      "grad_norm": 0.3520176112651825,
      "learning_rate": 0.00018945459394585946,
      "loss": 0.8538,
      "step": 1982
    },
    {
      "epoch": 0.15864,
      "grad_norm": 0.3008982539176941,
      "learning_rate": 0.00018944925990132017,
      "loss": 0.748,
      "step": 1983
    },
    {
      "epoch": 0.15872,
      "grad_norm": 0.3546990156173706,
      "learning_rate": 0.0001894439258567809,
      "loss": 0.6756,
      "step": 1984
    },
    {
      "epoch": 0.1588,
      "grad_norm": 0.35142621397972107,
      "learning_rate": 0.00018943859181224163,
      "loss": 0.8431,
      "step": 1985
    },
    {
      "epoch": 0.15888,
      "grad_norm": 0.34958115220069885,
      "learning_rate": 0.00018943325776770237,
      "loss": 0.9559,
      "step": 1986
    },
    {
      "epoch": 0.15896,
      "grad_norm": 0.2827504277229309,
      "learning_rate": 0.0001894279237231631,
      "loss": 0.5223,
      "step": 1987
    },
    {
      "epoch": 0.15904,
      "grad_norm": 0.34028708934783936,
      "learning_rate": 0.00018942258967862382,
      "loss": 0.8941,
      "step": 1988
    },
    {
      "epoch": 0.15912,
      "grad_norm": 0.32430052757263184,
      "learning_rate": 0.00018941725563408456,
      "loss": 1.0829,
      "step": 1989
    },
    {
      "epoch": 0.1592,
      "grad_norm": 0.3616026043891907,
      "learning_rate": 0.00018941192158954527,
      "loss": 1.0978,
      "step": 1990
    },
    {
      "epoch": 0.15928,
      "grad_norm": 0.3537682890892029,
      "learning_rate": 0.000189406587545006,
      "loss": 1.0008,
      "step": 1991
    },
    {
      "epoch": 0.15936,
      "grad_norm": 0.3653145134449005,
      "learning_rate": 0.00018940125350046672,
      "loss": 0.9222,
      "step": 1992
    },
    {
      "epoch": 0.15944,
      "grad_norm": 0.34603768587112427,
      "learning_rate": 0.00018939591945592746,
      "loss": 0.7613,
      "step": 1993
    },
    {
      "epoch": 0.15952,
      "grad_norm": 0.4119409918785095,
      "learning_rate": 0.00018939058541138818,
      "loss": 0.8075,
      "step": 1994
    },
    {
      "epoch": 0.1596,
      "grad_norm": 0.35494720935821533,
      "learning_rate": 0.00018938525136684892,
      "loss": 0.7831,
      "step": 1995
    },
    {
      "epoch": 0.15968,
      "grad_norm": 0.4293736517429352,
      "learning_rate": 0.00018937991732230963,
      "loss": 1.032,
      "step": 1996
    },
    {
      "epoch": 0.15976,
      "grad_norm": 0.3669940233230591,
      "learning_rate": 0.00018937458327777037,
      "loss": 0.8434,
      "step": 1997
    },
    {
      "epoch": 0.15984,
      "grad_norm": 0.34907448291778564,
      "learning_rate": 0.0001893692492332311,
      "loss": 0.706,
      "step": 1998
    },
    {
      "epoch": 0.15992,
      "grad_norm": 0.3022593557834625,
      "learning_rate": 0.00018936391518869182,
      "loss": 0.8099,
      "step": 1999
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.4557502567768097,
      "learning_rate": 0.00018935858114415256,
      "loss": 0.7285,
      "step": 2000
    },
    {
      "epoch": 0.16008,
      "grad_norm": 0.2710752487182617,
      "learning_rate": 0.00018935324709961327,
      "loss": 0.6946,
      "step": 2001
    },
    {
      "epoch": 0.16016,
      "grad_norm": 0.3266327977180481,
      "learning_rate": 0.00018934791305507401,
      "loss": 0.626,
      "step": 2002
    },
    {
      "epoch": 0.16024,
      "grad_norm": 0.4990604519844055,
      "learning_rate": 0.00018934257901053473,
      "loss": 0.8987,
      "step": 2003
    },
    {
      "epoch": 0.16032,
      "grad_norm": 0.3736708164215088,
      "learning_rate": 0.00018933724496599547,
      "loss": 0.7599,
      "step": 2004
    },
    {
      "epoch": 0.1604,
      "grad_norm": 0.35840341448783875,
      "learning_rate": 0.0001893319109214562,
      "loss": 0.7008,
      "step": 2005
    },
    {
      "epoch": 0.16048,
      "grad_norm": 0.3150808811187744,
      "learning_rate": 0.00018932657687691692,
      "loss": 0.8237,
      "step": 2006
    },
    {
      "epoch": 0.16056,
      "grad_norm": 0.28463712334632874,
      "learning_rate": 0.00018932124283237766,
      "loss": 0.9051,
      "step": 2007
    },
    {
      "epoch": 0.16064,
      "grad_norm": 0.399763822555542,
      "learning_rate": 0.00018931590878783837,
      "loss": 1.1326,
      "step": 2008
    },
    {
      "epoch": 0.16072,
      "grad_norm": 0.3528863191604614,
      "learning_rate": 0.0001893105747432991,
      "loss": 0.8056,
      "step": 2009
    },
    {
      "epoch": 0.1608,
      "grad_norm": 0.3317248821258545,
      "learning_rate": 0.00018930524069875982,
      "loss": 0.7337,
      "step": 2010
    },
    {
      "epoch": 0.16088,
      "grad_norm": 0.301634818315506,
      "learning_rate": 0.00018929990665422056,
      "loss": 0.8363,
      "step": 2011
    },
    {
      "epoch": 0.16096,
      "grad_norm": 0.35842692852020264,
      "learning_rate": 0.0001892945726096813,
      "loss": 0.8722,
      "step": 2012
    },
    {
      "epoch": 0.16104,
      "grad_norm": 0.37281858921051025,
      "learning_rate": 0.00018928923856514202,
      "loss": 0.5169,
      "step": 2013
    },
    {
      "epoch": 0.16112,
      "grad_norm": 0.43942245841026306,
      "learning_rate": 0.00018928390452060276,
      "loss": 0.8768,
      "step": 2014
    },
    {
      "epoch": 0.1612,
      "grad_norm": 0.3415932357311249,
      "learning_rate": 0.00018927857047606347,
      "loss": 0.7784,
      "step": 2015
    },
    {
      "epoch": 0.16128,
      "grad_norm": 0.22225521504878998,
      "learning_rate": 0.0001892732364315242,
      "loss": 0.7246,
      "step": 2016
    },
    {
      "epoch": 0.16136,
      "grad_norm": 0.37932154536247253,
      "learning_rate": 0.00018926790238698492,
      "loss": 1.043,
      "step": 2017
    },
    {
      "epoch": 0.16144,
      "grad_norm": 0.2775234878063202,
      "learning_rate": 0.00018926256834244566,
      "loss": 0.6682,
      "step": 2018
    },
    {
      "epoch": 0.16152,
      "grad_norm": 0.29640108346939087,
      "learning_rate": 0.0001892572342979064,
      "loss": 0.8302,
      "step": 2019
    },
    {
      "epoch": 0.1616,
      "grad_norm": 0.4183621406555176,
      "learning_rate": 0.00018925190025336711,
      "loss": 1.0026,
      "step": 2020
    },
    {
      "epoch": 0.16168,
      "grad_norm": 0.47650954127311707,
      "learning_rate": 0.00018924656620882785,
      "loss": 0.7255,
      "step": 2021
    },
    {
      "epoch": 0.16176,
      "grad_norm": 0.3768073618412018,
      "learning_rate": 0.00018924123216428857,
      "loss": 0.8979,
      "step": 2022
    },
    {
      "epoch": 0.16184,
      "grad_norm": 0.3143993616104126,
      "learning_rate": 0.0001892358981197493,
      "loss": 0.8991,
      "step": 2023
    },
    {
      "epoch": 0.16192,
      "grad_norm": 0.23983216285705566,
      "learning_rate": 0.00018923056407521002,
      "loss": 0.7298,
      "step": 2024
    },
    {
      "epoch": 0.162,
      "grad_norm": 0.3733765780925751,
      "learning_rate": 0.00018922523003067076,
      "loss": 0.9225,
      "step": 2025
    },
    {
      "epoch": 0.16208,
      "grad_norm": 0.3072473406791687,
      "learning_rate": 0.0001892198959861315,
      "loss": 1.095,
      "step": 2026
    },
    {
      "epoch": 0.16216,
      "grad_norm": 0.407406747341156,
      "learning_rate": 0.0001892145619415922,
      "loss": 0.6332,
      "step": 2027
    },
    {
      "epoch": 0.16224,
      "grad_norm": 0.3148289620876312,
      "learning_rate": 0.00018920922789705295,
      "loss": 0.8,
      "step": 2028
    },
    {
      "epoch": 0.16232,
      "grad_norm": 0.2994510531425476,
      "learning_rate": 0.00018920389385251366,
      "loss": 0.7365,
      "step": 2029
    },
    {
      "epoch": 0.1624,
      "grad_norm": 0.37508121132850647,
      "learning_rate": 0.0001891985598079744,
      "loss": 0.5923,
      "step": 2030
    },
    {
      "epoch": 0.16248,
      "grad_norm": 0.4117525815963745,
      "learning_rate": 0.00018919322576343514,
      "loss": 0.9631,
      "step": 2031
    },
    {
      "epoch": 0.16256,
      "grad_norm": 0.4648147225379944,
      "learning_rate": 0.00018918789171889586,
      "loss": 0.7931,
      "step": 2032
    },
    {
      "epoch": 0.16264,
      "grad_norm": 0.476104736328125,
      "learning_rate": 0.0001891825576743566,
      "loss": 1.1965,
      "step": 2033
    },
    {
      "epoch": 0.16272,
      "grad_norm": 0.32723498344421387,
      "learning_rate": 0.0001891772236298173,
      "loss": 0.9218,
      "step": 2034
    },
    {
      "epoch": 0.1628,
      "grad_norm": 0.33909693360328674,
      "learning_rate": 0.00018917188958527805,
      "loss": 0.8548,
      "step": 2035
    },
    {
      "epoch": 0.16288,
      "grad_norm": 0.34514376521110535,
      "learning_rate": 0.00018916655554073876,
      "loss": 0.7018,
      "step": 2036
    },
    {
      "epoch": 0.16296,
      "grad_norm": 0.3403888940811157,
      "learning_rate": 0.0001891612214961995,
      "loss": 0.8779,
      "step": 2037
    },
    {
      "epoch": 0.16304,
      "grad_norm": 0.39617982506752014,
      "learning_rate": 0.00018915588745166024,
      "loss": 0.813,
      "step": 2038
    },
    {
      "epoch": 0.16312,
      "grad_norm": 0.4493006765842438,
      "learning_rate": 0.00018915055340712095,
      "loss": 1.199,
      "step": 2039
    },
    {
      "epoch": 0.1632,
      "grad_norm": 0.3181686997413635,
      "learning_rate": 0.0001891452193625817,
      "loss": 0.7196,
      "step": 2040
    },
    {
      "epoch": 0.16328,
      "grad_norm": 0.3674936890602112,
      "learning_rate": 0.0001891398853180424,
      "loss": 0.6709,
      "step": 2041
    },
    {
      "epoch": 0.16336,
      "grad_norm": 0.2623794674873352,
      "learning_rate": 0.00018913455127350315,
      "loss": 0.8027,
      "step": 2042
    },
    {
      "epoch": 0.16344,
      "grad_norm": 0.42567968368530273,
      "learning_rate": 0.00018912921722896386,
      "loss": 0.9516,
      "step": 2043
    },
    {
      "epoch": 0.16352,
      "grad_norm": 0.370288223028183,
      "learning_rate": 0.0001891238831844246,
      "loss": 1.1176,
      "step": 2044
    },
    {
      "epoch": 0.1636,
      "grad_norm": 0.437142014503479,
      "learning_rate": 0.00018911854913988534,
      "loss": 0.8896,
      "step": 2045
    },
    {
      "epoch": 0.16368,
      "grad_norm": 0.38471317291259766,
      "learning_rate": 0.00018911321509534605,
      "loss": 0.7236,
      "step": 2046
    },
    {
      "epoch": 0.16376,
      "grad_norm": 0.27236926555633545,
      "learning_rate": 0.0001891078810508068,
      "loss": 0.5203,
      "step": 2047
    },
    {
      "epoch": 0.16384,
      "grad_norm": 0.3301093876361847,
      "learning_rate": 0.0001891025470062675,
      "loss": 0.6894,
      "step": 2048
    },
    {
      "epoch": 0.16392,
      "grad_norm": 0.39179375767707825,
      "learning_rate": 0.00018909721296172824,
      "loss": 1.0893,
      "step": 2049
    },
    {
      "epoch": 0.164,
      "grad_norm": 0.3710300922393799,
      "learning_rate": 0.00018909187891718896,
      "loss": 0.6909,
      "step": 2050
    },
    {
      "epoch": 0.16408,
      "grad_norm": 0.40604278445243835,
      "learning_rate": 0.0001890865448726497,
      "loss": 0.659,
      "step": 2051
    },
    {
      "epoch": 0.16416,
      "grad_norm": 0.27639082074165344,
      "learning_rate": 0.00018908121082811044,
      "loss": 0.673,
      "step": 2052
    },
    {
      "epoch": 0.16424,
      "grad_norm": 0.44580015540122986,
      "learning_rate": 0.00018907587678357115,
      "loss": 0.8048,
      "step": 2053
    },
    {
      "epoch": 0.16432,
      "grad_norm": 0.334540456533432,
      "learning_rate": 0.0001890705427390319,
      "loss": 0.6542,
      "step": 2054
    },
    {
      "epoch": 0.1644,
      "grad_norm": 0.38132646679878235,
      "learning_rate": 0.0001890652086944926,
      "loss": 0.8118,
      "step": 2055
    },
    {
      "epoch": 0.16448,
      "grad_norm": 0.35645586252212524,
      "learning_rate": 0.00018905987464995334,
      "loss": 0.8074,
      "step": 2056
    },
    {
      "epoch": 0.16456,
      "grad_norm": 0.3264291286468506,
      "learning_rate": 0.00018905454060541405,
      "loss": 0.4833,
      "step": 2057
    },
    {
      "epoch": 0.16464,
      "grad_norm": 0.36587414145469666,
      "learning_rate": 0.0001890492065608748,
      "loss": 1.055,
      "step": 2058
    },
    {
      "epoch": 0.16472,
      "grad_norm": 0.3453708589076996,
      "learning_rate": 0.00018904387251633553,
      "loss": 0.6556,
      "step": 2059
    },
    {
      "epoch": 0.1648,
      "grad_norm": 0.32211917638778687,
      "learning_rate": 0.00018903853847179625,
      "loss": 0.9695,
      "step": 2060
    },
    {
      "epoch": 0.16488,
      "grad_norm": 0.34179404377937317,
      "learning_rate": 0.000189033204427257,
      "loss": 0.7922,
      "step": 2061
    },
    {
      "epoch": 0.16496,
      "grad_norm": 0.31568074226379395,
      "learning_rate": 0.0001890278703827177,
      "loss": 0.776,
      "step": 2062
    },
    {
      "epoch": 0.16504,
      "grad_norm": 0.3377537131309509,
      "learning_rate": 0.00018902253633817844,
      "loss": 0.6928,
      "step": 2063
    },
    {
      "epoch": 0.16512,
      "grad_norm": 0.3426806330680847,
      "learning_rate": 0.00018901720229363915,
      "loss": 0.934,
      "step": 2064
    },
    {
      "epoch": 0.1652,
      "grad_norm": 0.3217337727546692,
      "learning_rate": 0.0001890118682490999,
      "loss": 0.6535,
      "step": 2065
    },
    {
      "epoch": 0.16528,
      "grad_norm": 0.2695411145687103,
      "learning_rate": 0.00018900653420456063,
      "loss": 0.9944,
      "step": 2066
    },
    {
      "epoch": 0.16536,
      "grad_norm": 0.4082188308238983,
      "learning_rate": 0.00018900120016002134,
      "loss": 1.1444,
      "step": 2067
    },
    {
      "epoch": 0.16544,
      "grad_norm": 0.369709849357605,
      "learning_rate": 0.00018899586611548208,
      "loss": 1.0563,
      "step": 2068
    },
    {
      "epoch": 0.16552,
      "grad_norm": 0.2607083320617676,
      "learning_rate": 0.0001889905320709428,
      "loss": 1.0061,
      "step": 2069
    },
    {
      "epoch": 0.1656,
      "grad_norm": 0.4174383580684662,
      "learning_rate": 0.00018898519802640354,
      "loss": 0.6593,
      "step": 2070
    },
    {
      "epoch": 0.16568,
      "grad_norm": 0.3557680547237396,
      "learning_rate": 0.00018897986398186425,
      "loss": 0.9314,
      "step": 2071
    },
    {
      "epoch": 0.16576,
      "grad_norm": 0.4099193513393402,
      "learning_rate": 0.000188974529937325,
      "loss": 0.951,
      "step": 2072
    },
    {
      "epoch": 0.16584,
      "grad_norm": 0.29472529888153076,
      "learning_rate": 0.00018896919589278573,
      "loss": 1.3332,
      "step": 2073
    },
    {
      "epoch": 0.16592,
      "grad_norm": 0.3659498691558838,
      "learning_rate": 0.00018896386184824644,
      "loss": 1.1997,
      "step": 2074
    },
    {
      "epoch": 0.166,
      "grad_norm": 0.37068480253219604,
      "learning_rate": 0.00018895852780370718,
      "loss": 0.986,
      "step": 2075
    },
    {
      "epoch": 0.16608,
      "grad_norm": 0.35507479310035706,
      "learning_rate": 0.0001889531937591679,
      "loss": 0.8242,
      "step": 2076
    },
    {
      "epoch": 0.16616,
      "grad_norm": 0.3203275203704834,
      "learning_rate": 0.00018894785971462863,
      "loss": 0.7635,
      "step": 2077
    },
    {
      "epoch": 0.16624,
      "grad_norm": 0.2735564112663269,
      "learning_rate": 0.00018894252567008937,
      "loss": 0.9407,
      "step": 2078
    },
    {
      "epoch": 0.16632,
      "grad_norm": 0.4018906354904175,
      "learning_rate": 0.0001889371916255501,
      "loss": 0.6397,
      "step": 2079
    },
    {
      "epoch": 0.1664,
      "grad_norm": 0.3609955608844757,
      "learning_rate": 0.00018893185758101083,
      "loss": 0.7568,
      "step": 2080
    },
    {
      "epoch": 0.16648,
      "grad_norm": 0.37524065375328064,
      "learning_rate": 0.00018892652353647154,
      "loss": 1.1541,
      "step": 2081
    },
    {
      "epoch": 0.16656,
      "grad_norm": 0.33187729120254517,
      "learning_rate": 0.00018892118949193228,
      "loss": 0.7712,
      "step": 2082
    },
    {
      "epoch": 0.16664,
      "grad_norm": 0.39478111267089844,
      "learning_rate": 0.000188915855447393,
      "loss": 0.633,
      "step": 2083
    },
    {
      "epoch": 0.16672,
      "grad_norm": 0.31535032391548157,
      "learning_rate": 0.00018891052140285373,
      "loss": 0.6279,
      "step": 2084
    },
    {
      "epoch": 0.1668,
      "grad_norm": 0.4813656508922577,
      "learning_rate": 0.00018890518735831447,
      "loss": 1.0794,
      "step": 2085
    },
    {
      "epoch": 0.16688,
      "grad_norm": 0.33574676513671875,
      "learning_rate": 0.00018889985331377519,
      "loss": 0.7016,
      "step": 2086
    },
    {
      "epoch": 0.16696,
      "grad_norm": 0.387046217918396,
      "learning_rate": 0.00018889451926923592,
      "loss": 0.7354,
      "step": 2087
    },
    {
      "epoch": 0.16704,
      "grad_norm": 0.43296608328819275,
      "learning_rate": 0.00018888918522469664,
      "loss": 0.8255,
      "step": 2088
    },
    {
      "epoch": 0.16712,
      "grad_norm": 0.3530251085758209,
      "learning_rate": 0.00018888385118015738,
      "loss": 0.9466,
      "step": 2089
    },
    {
      "epoch": 0.1672,
      "grad_norm": 0.258087694644928,
      "learning_rate": 0.0001888785171356181,
      "loss": 1.0338,
      "step": 2090
    },
    {
      "epoch": 0.16728,
      "grad_norm": 0.325164794921875,
      "learning_rate": 0.00018887318309107883,
      "loss": 0.8864,
      "step": 2091
    },
    {
      "epoch": 0.16736,
      "grad_norm": 0.3093002736568451,
      "learning_rate": 0.00018886784904653957,
      "loss": 0.8209,
      "step": 2092
    },
    {
      "epoch": 0.16744,
      "grad_norm": 0.3136517107486725,
      "learning_rate": 0.00018886251500200028,
      "loss": 0.9646,
      "step": 2093
    },
    {
      "epoch": 0.16752,
      "grad_norm": 0.31689688563346863,
      "learning_rate": 0.00018885718095746102,
      "loss": 0.9619,
      "step": 2094
    },
    {
      "epoch": 0.1676,
      "grad_norm": 0.36668914556503296,
      "learning_rate": 0.00018885184691292174,
      "loss": 0.7481,
      "step": 2095
    },
    {
      "epoch": 0.16768,
      "grad_norm": 0.3008263409137726,
      "learning_rate": 0.00018884651286838248,
      "loss": 0.8296,
      "step": 2096
    },
    {
      "epoch": 0.16776,
      "grad_norm": 0.381531685590744,
      "learning_rate": 0.0001888411788238432,
      "loss": 0.695,
      "step": 2097
    },
    {
      "epoch": 0.16784,
      "grad_norm": 0.39712652564048767,
      "learning_rate": 0.00018883584477930393,
      "loss": 0.8972,
      "step": 2098
    },
    {
      "epoch": 0.16792,
      "grad_norm": 0.41461774706840515,
      "learning_rate": 0.00018883051073476467,
      "loss": 0.9032,
      "step": 2099
    },
    {
      "epoch": 0.168,
      "grad_norm": 0.30831634998321533,
      "learning_rate": 0.00018882517669022538,
      "loss": 0.8737,
      "step": 2100
    },
    {
      "epoch": 0.16808,
      "grad_norm": 0.41267895698547363,
      "learning_rate": 0.00018881984264568612,
      "loss": 0.9687,
      "step": 2101
    },
    {
      "epoch": 0.16816,
      "grad_norm": 0.31264618039131165,
      "learning_rate": 0.00018881450860114683,
      "loss": 0.7673,
      "step": 2102
    },
    {
      "epoch": 0.16824,
      "grad_norm": 0.2785303294658661,
      "learning_rate": 0.00018880917455660757,
      "loss": 0.4933,
      "step": 2103
    },
    {
      "epoch": 0.16832,
      "grad_norm": 0.3348238468170166,
      "learning_rate": 0.00018880384051206829,
      "loss": 0.641,
      "step": 2104
    },
    {
      "epoch": 0.1684,
      "grad_norm": 0.35655805468559265,
      "learning_rate": 0.00018879850646752903,
      "loss": 0.93,
      "step": 2105
    },
    {
      "epoch": 0.16848,
      "grad_norm": 0.3637591600418091,
      "learning_rate": 0.00018879317242298974,
      "loss": 0.7847,
      "step": 2106
    },
    {
      "epoch": 0.16856,
      "grad_norm": 0.30777642130851746,
      "learning_rate": 0.00018878783837845048,
      "loss": 0.7002,
      "step": 2107
    },
    {
      "epoch": 0.16864,
      "grad_norm": 0.34908100962638855,
      "learning_rate": 0.0001887825043339112,
      "loss": 0.8769,
      "step": 2108
    },
    {
      "epoch": 0.16872,
      "grad_norm": 0.3907524347305298,
      "learning_rate": 0.00018877717028937193,
      "loss": 0.6976,
      "step": 2109
    },
    {
      "epoch": 0.1688,
      "grad_norm": 0.24170415103435516,
      "learning_rate": 0.00018877183624483264,
      "loss": 0.9715,
      "step": 2110
    },
    {
      "epoch": 0.16888,
      "grad_norm": 0.3789976239204407,
      "learning_rate": 0.00018876650220029338,
      "loss": 0.917,
      "step": 2111
    },
    {
      "epoch": 0.16896,
      "grad_norm": 0.4449421465396881,
      "learning_rate": 0.0001887611681557541,
      "loss": 0.8514,
      "step": 2112
    },
    {
      "epoch": 0.16904,
      "grad_norm": 0.27223777770996094,
      "learning_rate": 0.00018875583411121484,
      "loss": 1.1415,
      "step": 2113
    },
    {
      "epoch": 0.16912,
      "grad_norm": 0.2724209427833557,
      "learning_rate": 0.00018875050006667555,
      "loss": 0.7244,
      "step": 2114
    },
    {
      "epoch": 0.1692,
      "grad_norm": 0.39630404114723206,
      "learning_rate": 0.0001887451660221363,
      "loss": 0.833,
      "step": 2115
    },
    {
      "epoch": 0.16928,
      "grad_norm": 0.3586556315422058,
      "learning_rate": 0.00018873983197759703,
      "loss": 0.9736,
      "step": 2116
    },
    {
      "epoch": 0.16936,
      "grad_norm": 0.3668118715286255,
      "learning_rate": 0.00018873449793305774,
      "loss": 1.1457,
      "step": 2117
    },
    {
      "epoch": 0.16944,
      "grad_norm": 0.368977427482605,
      "learning_rate": 0.00018872916388851848,
      "loss": 0.8843,
      "step": 2118
    },
    {
      "epoch": 0.16952,
      "grad_norm": 0.36646169424057007,
      "learning_rate": 0.0001887238298439792,
      "loss": 0.8438,
      "step": 2119
    },
    {
      "epoch": 0.1696,
      "grad_norm": 0.3882926106452942,
      "learning_rate": 0.00018871849579943993,
      "loss": 0.7822,
      "step": 2120
    },
    {
      "epoch": 0.16968,
      "grad_norm": 0.32381471991539,
      "learning_rate": 0.00018871316175490065,
      "loss": 0.7527,
      "step": 2121
    },
    {
      "epoch": 0.16976,
      "grad_norm": 0.3980560600757599,
      "learning_rate": 0.00018870782771036139,
      "loss": 0.6871,
      "step": 2122
    },
    {
      "epoch": 0.16984,
      "grad_norm": 0.40849006175994873,
      "learning_rate": 0.0001887024936658221,
      "loss": 1.0548,
      "step": 2123
    },
    {
      "epoch": 0.16992,
      "grad_norm": 0.3341907262802124,
      "learning_rate": 0.00018869715962128284,
      "loss": 0.7196,
      "step": 2124
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.47611209750175476,
      "learning_rate": 0.00018869182557674355,
      "loss": 0.9311,
      "step": 2125
    },
    {
      "epoch": 0.17008,
      "grad_norm": 0.32506564259529114,
      "learning_rate": 0.0001886864915322043,
      "loss": 0.7375,
      "step": 2126
    },
    {
      "epoch": 0.17016,
      "grad_norm": 0.4217394292354584,
      "learning_rate": 0.00018868115748766503,
      "loss": 0.9362,
      "step": 2127
    },
    {
      "epoch": 0.17024,
      "grad_norm": 0.2766711413860321,
      "learning_rate": 0.00018867582344312574,
      "loss": 0.7531,
      "step": 2128
    },
    {
      "epoch": 0.17032,
      "grad_norm": 0.41326525807380676,
      "learning_rate": 0.00018867048939858648,
      "loss": 1.0515,
      "step": 2129
    },
    {
      "epoch": 0.1704,
      "grad_norm": 0.2970746159553528,
      "learning_rate": 0.0001886651553540472,
      "loss": 0.9181,
      "step": 2130
    },
    {
      "epoch": 0.17048,
      "grad_norm": 0.30235493183135986,
      "learning_rate": 0.00018865982130950794,
      "loss": 1.1041,
      "step": 2131
    },
    {
      "epoch": 0.17056,
      "grad_norm": 0.33552291989326477,
      "learning_rate": 0.00018865448726496868,
      "loss": 0.7472,
      "step": 2132
    },
    {
      "epoch": 0.17064,
      "grad_norm": 0.3967358469963074,
      "learning_rate": 0.0001886491532204294,
      "loss": 0.7512,
      "step": 2133
    },
    {
      "epoch": 0.17072,
      "grad_norm": 0.46862900257110596,
      "learning_rate": 0.00018864381917589013,
      "loss": 0.6345,
      "step": 2134
    },
    {
      "epoch": 0.1708,
      "grad_norm": 0.31601300835609436,
      "learning_rate": 0.00018863848513135084,
      "loss": 0.9584,
      "step": 2135
    },
    {
      "epoch": 0.17088,
      "grad_norm": 0.35702964663505554,
      "learning_rate": 0.00018863315108681158,
      "loss": 0.7164,
      "step": 2136
    },
    {
      "epoch": 0.17096,
      "grad_norm": 0.3700026273727417,
      "learning_rate": 0.0001886278170422723,
      "loss": 0.5624,
      "step": 2137
    },
    {
      "epoch": 0.17104,
      "grad_norm": 0.44978204369544983,
      "learning_rate": 0.00018862248299773303,
      "loss": 1.0401,
      "step": 2138
    },
    {
      "epoch": 0.17112,
      "grad_norm": 0.3246082067489624,
      "learning_rate": 0.00018861714895319377,
      "loss": 0.8454,
      "step": 2139
    },
    {
      "epoch": 0.1712,
      "grad_norm": 0.394368439912796,
      "learning_rate": 0.00018861181490865449,
      "loss": 0.9511,
      "step": 2140
    },
    {
      "epoch": 0.17128,
      "grad_norm": 0.38775020837783813,
      "learning_rate": 0.00018860648086411523,
      "loss": 0.9239,
      "step": 2141
    },
    {
      "epoch": 0.17136,
      "grad_norm": 0.4017460346221924,
      "learning_rate": 0.00018860114681957594,
      "loss": 0.975,
      "step": 2142
    },
    {
      "epoch": 0.17144,
      "grad_norm": 0.34066641330718994,
      "learning_rate": 0.00018859581277503668,
      "loss": 0.928,
      "step": 2143
    },
    {
      "epoch": 0.17152,
      "grad_norm": 0.28395792841911316,
      "learning_rate": 0.0001885904787304974,
      "loss": 0.5978,
      "step": 2144
    },
    {
      "epoch": 0.1716,
      "grad_norm": 0.273120641708374,
      "learning_rate": 0.00018858514468595813,
      "loss": 1.2036,
      "step": 2145
    },
    {
      "epoch": 0.17168,
      "grad_norm": 0.4284878373146057,
      "learning_rate": 0.00018857981064141887,
      "loss": 0.6497,
      "step": 2146
    },
    {
      "epoch": 0.17176,
      "grad_norm": 0.48627129197120667,
      "learning_rate": 0.00018857447659687958,
      "loss": 0.9603,
      "step": 2147
    },
    {
      "epoch": 0.17184,
      "grad_norm": 0.3352358043193817,
      "learning_rate": 0.00018856914255234032,
      "loss": 0.9076,
      "step": 2148
    },
    {
      "epoch": 0.17192,
      "grad_norm": 0.32011815905570984,
      "learning_rate": 0.00018856380850780104,
      "loss": 1.1338,
      "step": 2149
    },
    {
      "epoch": 0.172,
      "grad_norm": 0.3721872866153717,
      "learning_rate": 0.00018855847446326178,
      "loss": 0.8701,
      "step": 2150
    },
    {
      "epoch": 0.17208,
      "grad_norm": 0.35676220059394836,
      "learning_rate": 0.0001885531404187225,
      "loss": 0.5667,
      "step": 2151
    },
    {
      "epoch": 0.17216,
      "grad_norm": 0.2936592102050781,
      "learning_rate": 0.00018854780637418323,
      "loss": 0.8658,
      "step": 2152
    },
    {
      "epoch": 0.17224,
      "grad_norm": 0.30014023184776306,
      "learning_rate": 0.00018854247232964397,
      "loss": 0.6357,
      "step": 2153
    },
    {
      "epoch": 0.17232,
      "grad_norm": 0.36368313431739807,
      "learning_rate": 0.00018853713828510468,
      "loss": 1.2119,
      "step": 2154
    },
    {
      "epoch": 0.1724,
      "grad_norm": 0.5267201066017151,
      "learning_rate": 0.00018853180424056542,
      "loss": 1.043,
      "step": 2155
    },
    {
      "epoch": 0.17248,
      "grad_norm": 0.4544357657432556,
      "learning_rate": 0.00018852647019602613,
      "loss": 0.9877,
      "step": 2156
    },
    {
      "epoch": 0.17256,
      "grad_norm": 0.33050537109375,
      "learning_rate": 0.00018852113615148687,
      "loss": 0.613,
      "step": 2157
    },
    {
      "epoch": 0.17264,
      "grad_norm": 0.24472635984420776,
      "learning_rate": 0.0001885158021069476,
      "loss": 0.4778,
      "step": 2158
    },
    {
      "epoch": 0.17272,
      "grad_norm": 0.38052716851234436,
      "learning_rate": 0.00018851046806240833,
      "loss": 0.8568,
      "step": 2159
    },
    {
      "epoch": 0.1728,
      "grad_norm": 0.29433345794677734,
      "learning_rate": 0.00018850513401786907,
      "loss": 0.6904,
      "step": 2160
    },
    {
      "epoch": 0.17288,
      "grad_norm": 0.41959312558174133,
      "learning_rate": 0.00018849979997332978,
      "loss": 0.9433,
      "step": 2161
    },
    {
      "epoch": 0.17296,
      "grad_norm": 0.346271812915802,
      "learning_rate": 0.00018849446592879052,
      "loss": 0.904,
      "step": 2162
    },
    {
      "epoch": 0.17304,
      "grad_norm": 0.4823477268218994,
      "learning_rate": 0.00018848913188425123,
      "loss": 0.8956,
      "step": 2163
    },
    {
      "epoch": 0.17312,
      "grad_norm": 0.36935028433799744,
      "learning_rate": 0.00018848379783971197,
      "loss": 0.7068,
      "step": 2164
    },
    {
      "epoch": 0.1732,
      "grad_norm": 0.32923993468284607,
      "learning_rate": 0.00018847846379517268,
      "loss": 1.0566,
      "step": 2165
    },
    {
      "epoch": 0.17328,
      "grad_norm": 0.45112308859825134,
      "learning_rate": 0.00018847312975063342,
      "loss": 0.9079,
      "step": 2166
    },
    {
      "epoch": 0.17336,
      "grad_norm": 0.3231503963470459,
      "learning_rate": 0.00018846779570609416,
      "loss": 0.7379,
      "step": 2167
    },
    {
      "epoch": 0.17344,
      "grad_norm": 0.3411017060279846,
      "learning_rate": 0.00018846246166155488,
      "loss": 0.5263,
      "step": 2168
    },
    {
      "epoch": 0.17352,
      "grad_norm": 0.46902817487716675,
      "learning_rate": 0.00018845712761701562,
      "loss": 0.9109,
      "step": 2169
    },
    {
      "epoch": 0.1736,
      "grad_norm": 0.357647567987442,
      "learning_rate": 0.00018845179357247633,
      "loss": 0.8547,
      "step": 2170
    },
    {
      "epoch": 0.17368,
      "grad_norm": 0.3230915665626526,
      "learning_rate": 0.00018844645952793707,
      "loss": 0.8487,
      "step": 2171
    },
    {
      "epoch": 0.17376,
      "grad_norm": 0.30251345038414,
      "learning_rate": 0.00018844112548339778,
      "loss": 0.5348,
      "step": 2172
    },
    {
      "epoch": 0.17384,
      "grad_norm": 0.42279088497161865,
      "learning_rate": 0.00018843579143885852,
      "loss": 0.7959,
      "step": 2173
    },
    {
      "epoch": 0.17392,
      "grad_norm": 0.2809142768383026,
      "learning_rate": 0.00018843045739431926,
      "loss": 0.8575,
      "step": 2174
    },
    {
      "epoch": 0.174,
      "grad_norm": 0.341835081577301,
      "learning_rate": 0.00018842512334977997,
      "loss": 0.6497,
      "step": 2175
    },
    {
      "epoch": 0.17408,
      "grad_norm": 0.30936044454574585,
      "learning_rate": 0.00018841978930524071,
      "loss": 1.1166,
      "step": 2176
    },
    {
      "epoch": 0.17416,
      "grad_norm": 0.4815003275871277,
      "learning_rate": 0.00018841445526070143,
      "loss": 0.8136,
      "step": 2177
    },
    {
      "epoch": 0.17424,
      "grad_norm": 0.43643516302108765,
      "learning_rate": 0.00018840912121616217,
      "loss": 0.6939,
      "step": 2178
    },
    {
      "epoch": 0.17432,
      "grad_norm": 0.44365760684013367,
      "learning_rate": 0.0001884037871716229,
      "loss": 1.0678,
      "step": 2179
    },
    {
      "epoch": 0.1744,
      "grad_norm": 0.32401981949806213,
      "learning_rate": 0.00018839845312708362,
      "loss": 0.701,
      "step": 2180
    },
    {
      "epoch": 0.17448,
      "grad_norm": 0.39680272340774536,
      "learning_rate": 0.00018839311908254436,
      "loss": 0.9703,
      "step": 2181
    },
    {
      "epoch": 0.17456,
      "grad_norm": 0.35364893078804016,
      "learning_rate": 0.00018838778503800507,
      "loss": 0.7409,
      "step": 2182
    },
    {
      "epoch": 0.17464,
      "grad_norm": 0.2866501808166504,
      "learning_rate": 0.0001883824509934658,
      "loss": 0.5034,
      "step": 2183
    },
    {
      "epoch": 0.17472,
      "grad_norm": 0.26604077219963074,
      "learning_rate": 0.00018837711694892652,
      "loss": 0.8695,
      "step": 2184
    },
    {
      "epoch": 0.1748,
      "grad_norm": 0.2888980507850647,
      "learning_rate": 0.00018837178290438726,
      "loss": 0.6841,
      "step": 2185
    },
    {
      "epoch": 0.17488,
      "grad_norm": 0.42773279547691345,
      "learning_rate": 0.000188366448859848,
      "loss": 0.7995,
      "step": 2186
    },
    {
      "epoch": 0.17496,
      "grad_norm": 0.2452104091644287,
      "learning_rate": 0.00018836111481530872,
      "loss": 0.4952,
      "step": 2187
    },
    {
      "epoch": 0.17504,
      "grad_norm": 0.4420461356639862,
      "learning_rate": 0.00018835578077076946,
      "loss": 0.9945,
      "step": 2188
    },
    {
      "epoch": 0.17512,
      "grad_norm": 0.32777243852615356,
      "learning_rate": 0.00018835044672623017,
      "loss": 0.8947,
      "step": 2189
    },
    {
      "epoch": 0.1752,
      "grad_norm": 0.3010941743850708,
      "learning_rate": 0.0001883451126816909,
      "loss": 1.1047,
      "step": 2190
    },
    {
      "epoch": 0.17528,
      "grad_norm": 0.37969067692756653,
      "learning_rate": 0.00018833977863715162,
      "loss": 0.7258,
      "step": 2191
    },
    {
      "epoch": 0.17536,
      "grad_norm": 0.31379571557044983,
      "learning_rate": 0.00018833444459261236,
      "loss": 0.6438,
      "step": 2192
    },
    {
      "epoch": 0.17544,
      "grad_norm": 0.5249098539352417,
      "learning_rate": 0.0001883291105480731,
      "loss": 1.0118,
      "step": 2193
    },
    {
      "epoch": 0.17552,
      "grad_norm": 0.2835063636302948,
      "learning_rate": 0.00018832377650353381,
      "loss": 0.6558,
      "step": 2194
    },
    {
      "epoch": 0.1756,
      "grad_norm": 0.3713095784187317,
      "learning_rate": 0.00018831844245899455,
      "loss": 1.0746,
      "step": 2195
    },
    {
      "epoch": 0.17568,
      "grad_norm": 0.3609543442726135,
      "learning_rate": 0.00018831310841445527,
      "loss": 1.1567,
      "step": 2196
    },
    {
      "epoch": 0.17576,
      "grad_norm": 0.37521544098854065,
      "learning_rate": 0.000188307774369916,
      "loss": 0.7935,
      "step": 2197
    },
    {
      "epoch": 0.17584,
      "grad_norm": 0.3685835301876068,
      "learning_rate": 0.00018830244032537672,
      "loss": 0.7919,
      "step": 2198
    },
    {
      "epoch": 0.17592,
      "grad_norm": 0.3692159354686737,
      "learning_rate": 0.00018829710628083746,
      "loss": 0.7758,
      "step": 2199
    },
    {
      "epoch": 0.176,
      "grad_norm": 0.3202475309371948,
      "learning_rate": 0.0001882917722362982,
      "loss": 0.7419,
      "step": 2200
    },
    {
      "epoch": 0.17608,
      "grad_norm": 0.3167773187160492,
      "learning_rate": 0.0001882864381917589,
      "loss": 1.0552,
      "step": 2201
    },
    {
      "epoch": 0.17616,
      "grad_norm": 0.4893186092376709,
      "learning_rate": 0.00018828110414721965,
      "loss": 0.6987,
      "step": 2202
    },
    {
      "epoch": 0.17624,
      "grad_norm": 0.40307340025901794,
      "learning_rate": 0.00018827577010268036,
      "loss": 0.8721,
      "step": 2203
    },
    {
      "epoch": 0.17632,
      "grad_norm": 0.31388139724731445,
      "learning_rate": 0.0001882704360581411,
      "loss": 0.8797,
      "step": 2204
    },
    {
      "epoch": 0.1764,
      "grad_norm": 0.3153011202812195,
      "learning_rate": 0.00018826510201360182,
      "loss": 1.244,
      "step": 2205
    },
    {
      "epoch": 0.17648,
      "grad_norm": 0.36917319893836975,
      "learning_rate": 0.00018825976796906256,
      "loss": 0.9395,
      "step": 2206
    },
    {
      "epoch": 0.17656,
      "grad_norm": 0.28965941071510315,
      "learning_rate": 0.0001882544339245233,
      "loss": 0.8468,
      "step": 2207
    },
    {
      "epoch": 0.17664,
      "grad_norm": 0.41937127709388733,
      "learning_rate": 0.000188249099879984,
      "loss": 0.6382,
      "step": 2208
    },
    {
      "epoch": 0.17672,
      "grad_norm": 0.3725910782814026,
      "learning_rate": 0.00018824376583544475,
      "loss": 0.7109,
      "step": 2209
    },
    {
      "epoch": 0.1768,
      "grad_norm": 0.3599144518375397,
      "learning_rate": 0.00018823843179090546,
      "loss": 0.8497,
      "step": 2210
    },
    {
      "epoch": 0.17688,
      "grad_norm": 0.32133597135543823,
      "learning_rate": 0.0001882330977463662,
      "loss": 1.0002,
      "step": 2211
    },
    {
      "epoch": 0.17696,
      "grad_norm": 0.2585693895816803,
      "learning_rate": 0.00018822776370182692,
      "loss": 0.6283,
      "step": 2212
    },
    {
      "epoch": 0.17704,
      "grad_norm": 0.3597343862056732,
      "learning_rate": 0.00018822242965728765,
      "loss": 1.0371,
      "step": 2213
    },
    {
      "epoch": 0.17712,
      "grad_norm": 0.4354041814804077,
      "learning_rate": 0.0001882170956127484,
      "loss": 0.6746,
      "step": 2214
    },
    {
      "epoch": 0.1772,
      "grad_norm": 0.3847283720970154,
      "learning_rate": 0.0001882117615682091,
      "loss": 0.8433,
      "step": 2215
    },
    {
      "epoch": 0.17728,
      "grad_norm": 0.30071234703063965,
      "learning_rate": 0.00018820642752366985,
      "loss": 0.6818,
      "step": 2216
    },
    {
      "epoch": 0.17736,
      "grad_norm": 0.434552937746048,
      "learning_rate": 0.00018820109347913056,
      "loss": 0.8853,
      "step": 2217
    },
    {
      "epoch": 0.17744,
      "grad_norm": 0.31017208099365234,
      "learning_rate": 0.0001881957594345913,
      "loss": 0.9843,
      "step": 2218
    },
    {
      "epoch": 0.17752,
      "grad_norm": 0.3522539734840393,
      "learning_rate": 0.000188190425390052,
      "loss": 0.8731,
      "step": 2219
    },
    {
      "epoch": 0.1776,
      "grad_norm": 0.3618016242980957,
      "learning_rate": 0.00018818509134551275,
      "loss": 0.6388,
      "step": 2220
    },
    {
      "epoch": 0.17768,
      "grad_norm": 0.2830063998699188,
      "learning_rate": 0.0001881797573009735,
      "loss": 0.6647,
      "step": 2221
    },
    {
      "epoch": 0.17776,
      "grad_norm": 0.3311662971973419,
      "learning_rate": 0.0001881744232564342,
      "loss": 0.7094,
      "step": 2222
    },
    {
      "epoch": 0.17784,
      "grad_norm": 0.29444238543510437,
      "learning_rate": 0.00018816908921189495,
      "loss": 0.8634,
      "step": 2223
    },
    {
      "epoch": 0.17792,
      "grad_norm": 0.4402753412723541,
      "learning_rate": 0.00018816375516735566,
      "loss": 1.0191,
      "step": 2224
    },
    {
      "epoch": 0.178,
      "grad_norm": 0.34191033244132996,
      "learning_rate": 0.0001881584211228164,
      "loss": 0.7204,
      "step": 2225
    },
    {
      "epoch": 0.17808,
      "grad_norm": 0.29873019456863403,
      "learning_rate": 0.0001881530870782771,
      "loss": 1.3356,
      "step": 2226
    },
    {
      "epoch": 0.17816,
      "grad_norm": 0.3571300804615021,
      "learning_rate": 0.00018814775303373785,
      "loss": 0.813,
      "step": 2227
    },
    {
      "epoch": 0.17824,
      "grad_norm": 0.40565288066864014,
      "learning_rate": 0.0001881424189891986,
      "loss": 0.8124,
      "step": 2228
    },
    {
      "epoch": 0.17832,
      "grad_norm": 0.4409120976924896,
      "learning_rate": 0.0001881370849446593,
      "loss": 1.0449,
      "step": 2229
    },
    {
      "epoch": 0.1784,
      "grad_norm": 0.35764044523239136,
      "learning_rate": 0.00018813175090012004,
      "loss": 0.6816,
      "step": 2230
    },
    {
      "epoch": 0.17848,
      "grad_norm": 0.3945012390613556,
      "learning_rate": 0.00018812641685558076,
      "loss": 1.2101,
      "step": 2231
    },
    {
      "epoch": 0.17856,
      "grad_norm": 0.36528974771499634,
      "learning_rate": 0.0001881210828110415,
      "loss": 1.1274,
      "step": 2232
    },
    {
      "epoch": 0.17864,
      "grad_norm": 0.3515462875366211,
      "learning_rate": 0.0001881157487665022,
      "loss": 0.9176,
      "step": 2233
    },
    {
      "epoch": 0.17872,
      "grad_norm": 0.3675158619880676,
      "learning_rate": 0.00018811041472196295,
      "loss": 0.9689,
      "step": 2234
    },
    {
      "epoch": 0.1788,
      "grad_norm": 0.3851342499256134,
      "learning_rate": 0.00018810508067742366,
      "loss": 0.6175,
      "step": 2235
    },
    {
      "epoch": 0.17888,
      "grad_norm": 0.323830783367157,
      "learning_rate": 0.0001880997466328844,
      "loss": 0.8725,
      "step": 2236
    },
    {
      "epoch": 0.17896,
      "grad_norm": 0.42154234647750854,
      "learning_rate": 0.0001880944125883451,
      "loss": 0.8896,
      "step": 2237
    },
    {
      "epoch": 0.17904,
      "grad_norm": 0.3976556658744812,
      "learning_rate": 0.00018808907854380585,
      "loss": 0.9072,
      "step": 2238
    },
    {
      "epoch": 0.17912,
      "grad_norm": 0.39807701110839844,
      "learning_rate": 0.00018808374449926657,
      "loss": 0.7768,
      "step": 2239
    },
    {
      "epoch": 0.1792,
      "grad_norm": 0.23784686625003815,
      "learning_rate": 0.0001880784104547273,
      "loss": 0.8834,
      "step": 2240
    },
    {
      "epoch": 0.17928,
      "grad_norm": 0.24633380770683289,
      "learning_rate": 0.00018807307641018802,
      "loss": 0.5094,
      "step": 2241
    },
    {
      "epoch": 0.17936,
      "grad_norm": 0.4067772626876831,
      "learning_rate": 0.00018806774236564876,
      "loss": 1.4773,
      "step": 2242
    },
    {
      "epoch": 0.17944,
      "grad_norm": 0.3451550602912903,
      "learning_rate": 0.00018806240832110947,
      "loss": 0.8903,
      "step": 2243
    },
    {
      "epoch": 0.17952,
      "grad_norm": 0.4044013023376465,
      "learning_rate": 0.0001880570742765702,
      "loss": 1.4281,
      "step": 2244
    },
    {
      "epoch": 0.1796,
      "grad_norm": 0.3164941966533661,
      "learning_rate": 0.00018805174023203095,
      "loss": 0.6529,
      "step": 2245
    },
    {
      "epoch": 0.17968,
      "grad_norm": 0.3524496555328369,
      "learning_rate": 0.00018804640618749166,
      "loss": 0.7877,
      "step": 2246
    },
    {
      "epoch": 0.17976,
      "grad_norm": 0.32615235447883606,
      "learning_rate": 0.0001880410721429524,
      "loss": 0.9528,
      "step": 2247
    },
    {
      "epoch": 0.17984,
      "grad_norm": 0.3448561429977417,
      "learning_rate": 0.00018803573809841312,
      "loss": 1.3362,
      "step": 2248
    },
    {
      "epoch": 0.17992,
      "grad_norm": 0.3405447006225586,
      "learning_rate": 0.00018803040405387386,
      "loss": 0.8136,
      "step": 2249
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.3101603090763092,
      "learning_rate": 0.00018802507000933457,
      "loss": 1.0003,
      "step": 2250
    },
    {
      "epoch": 0.18008,
      "grad_norm": 0.3005741238594055,
      "learning_rate": 0.0001880197359647953,
      "loss": 0.7479,
      "step": 2251
    },
    {
      "epoch": 0.18016,
      "grad_norm": 0.4218522906303406,
      "learning_rate": 0.00018801440192025602,
      "loss": 0.7379,
      "step": 2252
    },
    {
      "epoch": 0.18024,
      "grad_norm": 0.34691762924194336,
      "learning_rate": 0.00018800906787571676,
      "loss": 1.0245,
      "step": 2253
    },
    {
      "epoch": 0.18032,
      "grad_norm": 0.28695958852767944,
      "learning_rate": 0.0001880037338311775,
      "loss": 1.0872,
      "step": 2254
    },
    {
      "epoch": 0.1804,
      "grad_norm": 0.4165194630622864,
      "learning_rate": 0.00018799839978663821,
      "loss": 0.8206,
      "step": 2255
    },
    {
      "epoch": 0.18048,
      "grad_norm": 0.3731226921081543,
      "learning_rate": 0.00018799306574209895,
      "loss": 1.0016,
      "step": 2256
    },
    {
      "epoch": 0.18056,
      "grad_norm": 0.40776628255844116,
      "learning_rate": 0.00018798773169755967,
      "loss": 0.8391,
      "step": 2257
    },
    {
      "epoch": 0.18064,
      "grad_norm": 0.4408875107765198,
      "learning_rate": 0.0001879823976530204,
      "loss": 0.7302,
      "step": 2258
    },
    {
      "epoch": 0.18072,
      "grad_norm": 0.3133455812931061,
      "learning_rate": 0.00018797706360848112,
      "loss": 0.8907,
      "step": 2259
    },
    {
      "epoch": 0.1808,
      "grad_norm": 0.31890764832496643,
      "learning_rate": 0.00018797172956394186,
      "loss": 0.6611,
      "step": 2260
    },
    {
      "epoch": 0.18088,
      "grad_norm": 0.3432661294937134,
      "learning_rate": 0.0001879663955194026,
      "loss": 1.1022,
      "step": 2261
    },
    {
      "epoch": 0.18096,
      "grad_norm": 0.3824038803577423,
      "learning_rate": 0.0001879610614748633,
      "loss": 0.9616,
      "step": 2262
    },
    {
      "epoch": 0.18104,
      "grad_norm": 0.37664881348609924,
      "learning_rate": 0.00018795572743032405,
      "loss": 0.9502,
      "step": 2263
    },
    {
      "epoch": 0.18112,
      "grad_norm": 0.40658921003341675,
      "learning_rate": 0.00018795039338578476,
      "loss": 0.8898,
      "step": 2264
    },
    {
      "epoch": 0.1812,
      "grad_norm": 0.30680379271507263,
      "learning_rate": 0.0001879450593412455,
      "loss": 0.8996,
      "step": 2265
    },
    {
      "epoch": 0.18128,
      "grad_norm": 0.3370855152606964,
      "learning_rate": 0.00018793972529670622,
      "loss": 1.0311,
      "step": 2266
    },
    {
      "epoch": 0.18136,
      "grad_norm": 0.29448434710502625,
      "learning_rate": 0.00018793439125216696,
      "loss": 0.724,
      "step": 2267
    },
    {
      "epoch": 0.18144,
      "grad_norm": 0.397826611995697,
      "learning_rate": 0.0001879290572076277,
      "loss": 0.787,
      "step": 2268
    },
    {
      "epoch": 0.18152,
      "grad_norm": 0.37643420696258545,
      "learning_rate": 0.0001879237231630884,
      "loss": 0.6801,
      "step": 2269
    },
    {
      "epoch": 0.1816,
      "grad_norm": 0.3589625358581543,
      "learning_rate": 0.00018791838911854915,
      "loss": 0.6967,
      "step": 2270
    },
    {
      "epoch": 0.18168,
      "grad_norm": 0.3396891951560974,
      "learning_rate": 0.00018791305507400986,
      "loss": 0.9992,
      "step": 2271
    },
    {
      "epoch": 0.18176,
      "grad_norm": 0.33597567677497864,
      "learning_rate": 0.0001879077210294706,
      "loss": 0.6172,
      "step": 2272
    },
    {
      "epoch": 0.18184,
      "grad_norm": 0.3327507972717285,
      "learning_rate": 0.00018790238698493131,
      "loss": 1.1597,
      "step": 2273
    },
    {
      "epoch": 0.18192,
      "grad_norm": 0.29477551579475403,
      "learning_rate": 0.00018789705294039205,
      "loss": 1.1863,
      "step": 2274
    },
    {
      "epoch": 0.182,
      "grad_norm": 0.3446171283721924,
      "learning_rate": 0.0001878917188958528,
      "loss": 0.6911,
      "step": 2275
    },
    {
      "epoch": 0.18208,
      "grad_norm": 0.36725789308547974,
      "learning_rate": 0.0001878863848513135,
      "loss": 0.9674,
      "step": 2276
    },
    {
      "epoch": 0.18216,
      "grad_norm": 0.3439522087574005,
      "learning_rate": 0.00018788105080677425,
      "loss": 0.9184,
      "step": 2277
    },
    {
      "epoch": 0.18224,
      "grad_norm": 0.32373785972595215,
      "learning_rate": 0.00018787571676223496,
      "loss": 0.6749,
      "step": 2278
    },
    {
      "epoch": 0.18232,
      "grad_norm": 0.2840856611728668,
      "learning_rate": 0.0001878703827176957,
      "loss": 0.8624,
      "step": 2279
    },
    {
      "epoch": 0.1824,
      "grad_norm": 0.3830486536026001,
      "learning_rate": 0.00018786504867315644,
      "loss": 0.8719,
      "step": 2280
    },
    {
      "epoch": 0.18248,
      "grad_norm": 0.33444058895111084,
      "learning_rate": 0.00018785971462861715,
      "loss": 0.8296,
      "step": 2281
    },
    {
      "epoch": 0.18256,
      "grad_norm": 0.3426816761493683,
      "learning_rate": 0.0001878543805840779,
      "loss": 1.0406,
      "step": 2282
    },
    {
      "epoch": 0.18264,
      "grad_norm": 0.5132821202278137,
      "learning_rate": 0.0001878490465395386,
      "loss": 1.2398,
      "step": 2283
    },
    {
      "epoch": 0.18272,
      "grad_norm": 0.33694586157798767,
      "learning_rate": 0.00018784371249499934,
      "loss": 0.7428,
      "step": 2284
    },
    {
      "epoch": 0.1828,
      "grad_norm": 0.35058289766311646,
      "learning_rate": 0.00018783837845046006,
      "loss": 0.6049,
      "step": 2285
    },
    {
      "epoch": 0.18288,
      "grad_norm": 0.30383381247520447,
      "learning_rate": 0.0001878330444059208,
      "loss": 0.662,
      "step": 2286
    },
    {
      "epoch": 0.18296,
      "grad_norm": 0.3345995247364044,
      "learning_rate": 0.00018782771036138154,
      "loss": 0.5849,
      "step": 2287
    },
    {
      "epoch": 0.18304,
      "grad_norm": 0.3428308665752411,
      "learning_rate": 0.00018782237631684225,
      "loss": 0.7861,
      "step": 2288
    },
    {
      "epoch": 0.18312,
      "grad_norm": 0.3728344142436981,
      "learning_rate": 0.000187817042272303,
      "loss": 0.8587,
      "step": 2289
    },
    {
      "epoch": 0.1832,
      "grad_norm": 0.41478586196899414,
      "learning_rate": 0.0001878117082277637,
      "loss": 0.844,
      "step": 2290
    },
    {
      "epoch": 0.18328,
      "grad_norm": 0.372475266456604,
      "learning_rate": 0.00018780637418322444,
      "loss": 1.0693,
      "step": 2291
    },
    {
      "epoch": 0.18336,
      "grad_norm": 0.3105473518371582,
      "learning_rate": 0.00018780104013868515,
      "loss": 1.0121,
      "step": 2292
    },
    {
      "epoch": 0.18344,
      "grad_norm": 0.3764703869819641,
      "learning_rate": 0.0001877957060941459,
      "loss": 0.8061,
      "step": 2293
    },
    {
      "epoch": 0.18352,
      "grad_norm": 0.30578815937042236,
      "learning_rate": 0.00018779037204960663,
      "loss": 0.7904,
      "step": 2294
    },
    {
      "epoch": 0.1836,
      "grad_norm": 0.39126431941986084,
      "learning_rate": 0.00018778503800506735,
      "loss": 0.7393,
      "step": 2295
    },
    {
      "epoch": 0.18368,
      "grad_norm": 0.34837380051612854,
      "learning_rate": 0.0001877797039605281,
      "loss": 0.9457,
      "step": 2296
    },
    {
      "epoch": 0.18376,
      "grad_norm": 0.46940189599990845,
      "learning_rate": 0.0001877743699159888,
      "loss": 0.9951,
      "step": 2297
    },
    {
      "epoch": 0.18384,
      "grad_norm": 0.32357361912727356,
      "learning_rate": 0.00018776903587144954,
      "loss": 0.4977,
      "step": 2298
    },
    {
      "epoch": 0.18392,
      "grad_norm": 0.3035482168197632,
      "learning_rate": 0.00018776370182691025,
      "loss": 0.9781,
      "step": 2299
    },
    {
      "epoch": 0.184,
      "grad_norm": 0.3594682514667511,
      "learning_rate": 0.000187758367782371,
      "loss": 0.8267,
      "step": 2300
    },
    {
      "epoch": 0.18408,
      "grad_norm": 0.4650120437145233,
      "learning_rate": 0.00018775303373783173,
      "loss": 0.6687,
      "step": 2301
    },
    {
      "epoch": 0.18416,
      "grad_norm": 0.4204518795013428,
      "learning_rate": 0.00018774769969329244,
      "loss": 0.941,
      "step": 2302
    },
    {
      "epoch": 0.18424,
      "grad_norm": 0.35814571380615234,
      "learning_rate": 0.00018774236564875318,
      "loss": 0.8233,
      "step": 2303
    },
    {
      "epoch": 0.18432,
      "grad_norm": 0.35187748074531555,
      "learning_rate": 0.0001877370316042139,
      "loss": 0.6798,
      "step": 2304
    },
    {
      "epoch": 0.1844,
      "grad_norm": 0.4020680785179138,
      "learning_rate": 0.00018773169755967464,
      "loss": 0.7941,
      "step": 2305
    },
    {
      "epoch": 0.18448,
      "grad_norm": 0.38149863481521606,
      "learning_rate": 0.00018772636351513535,
      "loss": 0.8936,
      "step": 2306
    },
    {
      "epoch": 0.18456,
      "grad_norm": 0.4426337480545044,
      "learning_rate": 0.0001877210294705961,
      "loss": 0.7259,
      "step": 2307
    },
    {
      "epoch": 0.18464,
      "grad_norm": 0.3215859830379486,
      "learning_rate": 0.00018771569542605683,
      "loss": 1.0463,
      "step": 2308
    },
    {
      "epoch": 0.18472,
      "grad_norm": 0.3500947654247284,
      "learning_rate": 0.00018771036138151754,
      "loss": 0.7583,
      "step": 2309
    },
    {
      "epoch": 0.1848,
      "grad_norm": 0.29753515124320984,
      "learning_rate": 0.00018770502733697828,
      "loss": 0.752,
      "step": 2310
    },
    {
      "epoch": 0.18488,
      "grad_norm": 0.3364914059638977,
      "learning_rate": 0.000187699693292439,
      "loss": 0.6196,
      "step": 2311
    },
    {
      "epoch": 0.18496,
      "grad_norm": 0.33454573154449463,
      "learning_rate": 0.00018769435924789973,
      "loss": 0.8521,
      "step": 2312
    },
    {
      "epoch": 0.18504,
      "grad_norm": 0.4357101321220398,
      "learning_rate": 0.00018768902520336045,
      "loss": 0.8946,
      "step": 2313
    },
    {
      "epoch": 0.18512,
      "grad_norm": 0.2942586839199066,
      "learning_rate": 0.0001876836911588212,
      "loss": 0.983,
      "step": 2314
    },
    {
      "epoch": 0.1852,
      "grad_norm": 0.3682578504085541,
      "learning_rate": 0.00018767835711428193,
      "loss": 0.8817,
      "step": 2315
    },
    {
      "epoch": 0.18528,
      "grad_norm": 0.37627914547920227,
      "learning_rate": 0.00018767302306974264,
      "loss": 1.0607,
      "step": 2316
    },
    {
      "epoch": 0.18536,
      "grad_norm": 0.40619775652885437,
      "learning_rate": 0.00018766768902520338,
      "loss": 0.9879,
      "step": 2317
    },
    {
      "epoch": 0.18544,
      "grad_norm": 0.4477858245372772,
      "learning_rate": 0.0001876623549806641,
      "loss": 0.8303,
      "step": 2318
    },
    {
      "epoch": 0.18552,
      "grad_norm": 0.48047488927841187,
      "learning_rate": 0.00018765702093612483,
      "loss": 0.8857,
      "step": 2319
    },
    {
      "epoch": 0.1856,
      "grad_norm": 0.3729963004589081,
      "learning_rate": 0.00018765168689158554,
      "loss": 0.7531,
      "step": 2320
    },
    {
      "epoch": 0.18568,
      "grad_norm": 0.4233678877353668,
      "learning_rate": 0.00018764635284704628,
      "loss": 0.8161,
      "step": 2321
    },
    {
      "epoch": 0.18576,
      "grad_norm": 0.41029518842697144,
      "learning_rate": 0.00018764101880250702,
      "loss": 1.1473,
      "step": 2322
    },
    {
      "epoch": 0.18584,
      "grad_norm": 0.4494975805282593,
      "learning_rate": 0.00018763568475796774,
      "loss": 1.0255,
      "step": 2323
    },
    {
      "epoch": 0.18592,
      "grad_norm": 0.33978089690208435,
      "learning_rate": 0.00018763035071342848,
      "loss": 0.8607,
      "step": 2324
    },
    {
      "epoch": 0.186,
      "grad_norm": 0.42063745856285095,
      "learning_rate": 0.0001876250166688892,
      "loss": 0.7368,
      "step": 2325
    },
    {
      "epoch": 0.18608,
      "grad_norm": 0.4016968905925751,
      "learning_rate": 0.00018761968262434993,
      "loss": 0.6339,
      "step": 2326
    },
    {
      "epoch": 0.18616,
      "grad_norm": 0.31313762068748474,
      "learning_rate": 0.00018761434857981067,
      "loss": 0.8007,
      "step": 2327
    },
    {
      "epoch": 0.18624,
      "grad_norm": 0.2974037826061249,
      "learning_rate": 0.00018760901453527138,
      "loss": 0.5436,
      "step": 2328
    },
    {
      "epoch": 0.18632,
      "grad_norm": 0.35403913259506226,
      "learning_rate": 0.00018760368049073212,
      "loss": 0.7671,
      "step": 2329
    },
    {
      "epoch": 0.1864,
      "grad_norm": 0.37358027696609497,
      "learning_rate": 0.00018759834644619283,
      "loss": 0.879,
      "step": 2330
    },
    {
      "epoch": 0.18648,
      "grad_norm": 0.38646748661994934,
      "learning_rate": 0.00018759301240165357,
      "loss": 0.8359,
      "step": 2331
    },
    {
      "epoch": 0.18656,
      "grad_norm": 0.3206501007080078,
      "learning_rate": 0.0001875876783571143,
      "loss": 0.4747,
      "step": 2332
    },
    {
      "epoch": 0.18664,
      "grad_norm": 0.34136030077934265,
      "learning_rate": 0.00018758234431257503,
      "loss": 0.9563,
      "step": 2333
    },
    {
      "epoch": 0.18672,
      "grad_norm": 0.32737046480178833,
      "learning_rate": 0.00018757701026803577,
      "loss": 1.0793,
      "step": 2334
    },
    {
      "epoch": 0.1868,
      "grad_norm": 0.36938491463661194,
      "learning_rate": 0.00018757167622349648,
      "loss": 0.942,
      "step": 2335
    },
    {
      "epoch": 0.18688,
      "grad_norm": 0.3110586404800415,
      "learning_rate": 0.00018756634217895722,
      "loss": 0.6427,
      "step": 2336
    },
    {
      "epoch": 0.18696,
      "grad_norm": 0.4255715012550354,
      "learning_rate": 0.00018756100813441793,
      "loss": 0.8595,
      "step": 2337
    },
    {
      "epoch": 0.18704,
      "grad_norm": 0.30533456802368164,
      "learning_rate": 0.00018755567408987867,
      "loss": 0.6827,
      "step": 2338
    },
    {
      "epoch": 0.18712,
      "grad_norm": 0.32285940647125244,
      "learning_rate": 0.00018755034004533939,
      "loss": 0.5652,
      "step": 2339
    },
    {
      "epoch": 0.1872,
      "grad_norm": 0.4720195233821869,
      "learning_rate": 0.00018754500600080012,
      "loss": 1.2018,
      "step": 2340
    },
    {
      "epoch": 0.18728,
      "grad_norm": 0.34633392095565796,
      "learning_rate": 0.00018753967195626086,
      "loss": 0.7541,
      "step": 2341
    },
    {
      "epoch": 0.18736,
      "grad_norm": 0.3429434299468994,
      "learning_rate": 0.00018753433791172158,
      "loss": 0.6586,
      "step": 2342
    },
    {
      "epoch": 0.18744,
      "grad_norm": 0.29994696378707886,
      "learning_rate": 0.00018752900386718232,
      "loss": 1.0914,
      "step": 2343
    },
    {
      "epoch": 0.18752,
      "grad_norm": 0.4454459249973297,
      "learning_rate": 0.00018752366982264303,
      "loss": 0.746,
      "step": 2344
    },
    {
      "epoch": 0.1876,
      "grad_norm": 0.38346078991889954,
      "learning_rate": 0.00018751833577810377,
      "loss": 0.7208,
      "step": 2345
    },
    {
      "epoch": 0.18768,
      "grad_norm": 0.27595382928848267,
      "learning_rate": 0.00018751300173356448,
      "loss": 0.8071,
      "step": 2346
    },
    {
      "epoch": 0.18776,
      "grad_norm": 0.32287275791168213,
      "learning_rate": 0.00018750766768902522,
      "loss": 0.7091,
      "step": 2347
    },
    {
      "epoch": 0.18784,
      "grad_norm": 0.3798264265060425,
      "learning_rate": 0.00018750233364448596,
      "loss": 0.9294,
      "step": 2348
    },
    {
      "epoch": 0.18792,
      "grad_norm": 0.33501484990119934,
      "learning_rate": 0.00018749699959994668,
      "loss": 0.8095,
      "step": 2349
    },
    {
      "epoch": 0.188,
      "grad_norm": 0.29050642251968384,
      "learning_rate": 0.00018749166555540741,
      "loss": 0.5183,
      "step": 2350
    },
    {
      "epoch": 0.18808,
      "grad_norm": 0.2950053811073303,
      "learning_rate": 0.00018748633151086813,
      "loss": 0.7544,
      "step": 2351
    },
    {
      "epoch": 0.18816,
      "grad_norm": 0.2528565526008606,
      "learning_rate": 0.00018748099746632887,
      "loss": 0.6235,
      "step": 2352
    },
    {
      "epoch": 0.18824,
      "grad_norm": 0.37959498167037964,
      "learning_rate": 0.00018747566342178958,
      "loss": 0.8356,
      "step": 2353
    },
    {
      "epoch": 0.18832,
      "grad_norm": 0.3172677457332611,
      "learning_rate": 0.00018747032937725032,
      "loss": 1.0607,
      "step": 2354
    },
    {
      "epoch": 0.1884,
      "grad_norm": 0.413632869720459,
      "learning_rate": 0.00018746499533271103,
      "loss": 0.8135,
      "step": 2355
    },
    {
      "epoch": 0.18848,
      "grad_norm": 0.6109998226165771,
      "learning_rate": 0.00018745966128817177,
      "loss": 1.4543,
      "step": 2356
    },
    {
      "epoch": 0.18856,
      "grad_norm": 0.35832253098487854,
      "learning_rate": 0.0001874543272436325,
      "loss": 1.185,
      "step": 2357
    },
    {
      "epoch": 0.18864,
      "grad_norm": 0.25708484649658203,
      "learning_rate": 0.00018744899319909323,
      "loss": 1.1253,
      "step": 2358
    },
    {
      "epoch": 0.18872,
      "grad_norm": 0.35745471715927124,
      "learning_rate": 0.00018744365915455397,
      "loss": 0.836,
      "step": 2359
    },
    {
      "epoch": 0.1888,
      "grad_norm": 0.3311043977737427,
      "learning_rate": 0.00018743832511001468,
      "loss": 0.6699,
      "step": 2360
    },
    {
      "epoch": 0.18888,
      "grad_norm": 0.5240599513053894,
      "learning_rate": 0.00018743299106547542,
      "loss": 0.6998,
      "step": 2361
    },
    {
      "epoch": 0.18896,
      "grad_norm": 0.3102854788303375,
      "learning_rate": 0.00018742765702093613,
      "loss": 0.8405,
      "step": 2362
    },
    {
      "epoch": 0.18904,
      "grad_norm": 0.3673190474510193,
      "learning_rate": 0.00018742232297639687,
      "loss": 0.8709,
      "step": 2363
    },
    {
      "epoch": 0.18912,
      "grad_norm": 0.38497576117515564,
      "learning_rate": 0.00018741698893185758,
      "loss": 1.1066,
      "step": 2364
    },
    {
      "epoch": 0.1892,
      "grad_norm": 0.38318392634391785,
      "learning_rate": 0.00018741165488731832,
      "loss": 0.5186,
      "step": 2365
    },
    {
      "epoch": 0.18928,
      "grad_norm": 0.30003613233566284,
      "learning_rate": 0.00018740632084277904,
      "loss": 0.937,
      "step": 2366
    },
    {
      "epoch": 0.18936,
      "grad_norm": 0.2747465968132019,
      "learning_rate": 0.00018740098679823978,
      "loss": 0.6802,
      "step": 2367
    },
    {
      "epoch": 0.18944,
      "grad_norm": 0.34420567750930786,
      "learning_rate": 0.0001873956527537005,
      "loss": 1.1185,
      "step": 2368
    },
    {
      "epoch": 0.18952,
      "grad_norm": 0.4390820264816284,
      "learning_rate": 0.00018739031870916123,
      "loss": 0.7963,
      "step": 2369
    },
    {
      "epoch": 0.1896,
      "grad_norm": 0.3324306905269623,
      "learning_rate": 0.00018738498466462194,
      "loss": 1.0716,
      "step": 2370
    },
    {
      "epoch": 0.18968,
      "grad_norm": 0.32714080810546875,
      "learning_rate": 0.00018737965062008268,
      "loss": 0.7413,
      "step": 2371
    },
    {
      "epoch": 0.18976,
      "grad_norm": 0.2721840739250183,
      "learning_rate": 0.00018737431657554342,
      "loss": 0.5531,
      "step": 2372
    },
    {
      "epoch": 0.18984,
      "grad_norm": 0.37477606534957886,
      "learning_rate": 0.00018736898253100413,
      "loss": 0.6861,
      "step": 2373
    },
    {
      "epoch": 0.18992,
      "grad_norm": 0.3409576117992401,
      "learning_rate": 0.00018736364848646487,
      "loss": 0.9099,
      "step": 2374
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.24210675060749054,
      "learning_rate": 0.00018735831444192559,
      "loss": 0.4588,
      "step": 2375
    },
    {
      "epoch": 0.19008,
      "grad_norm": 0.30562257766723633,
      "learning_rate": 0.00018735298039738633,
      "loss": 0.955,
      "step": 2376
    },
    {
      "epoch": 0.19016,
      "grad_norm": 0.3706670105457306,
      "learning_rate": 0.00018734764635284704,
      "loss": 0.8551,
      "step": 2377
    },
    {
      "epoch": 0.19024,
      "grad_norm": 0.371002197265625,
      "learning_rate": 0.00018734231230830778,
      "loss": 0.7957,
      "step": 2378
    },
    {
      "epoch": 0.19032,
      "grad_norm": 0.3926239609718323,
      "learning_rate": 0.0001873369782637685,
      "loss": 0.7497,
      "step": 2379
    },
    {
      "epoch": 0.1904,
      "grad_norm": 0.3853813707828522,
      "learning_rate": 0.00018733164421922923,
      "loss": 0.9186,
      "step": 2380
    },
    {
      "epoch": 0.19048,
      "grad_norm": 0.2895117700099945,
      "learning_rate": 0.00018732631017468997,
      "loss": 1.2013,
      "step": 2381
    },
    {
      "epoch": 0.19056,
      "grad_norm": 0.38725385069847107,
      "learning_rate": 0.00018732097613015068,
      "loss": 0.9825,
      "step": 2382
    },
    {
      "epoch": 0.19064,
      "grad_norm": 0.32347598671913147,
      "learning_rate": 0.00018731564208561142,
      "loss": 0.7034,
      "step": 2383
    },
    {
      "epoch": 0.19072,
      "grad_norm": 0.4493199586868286,
      "learning_rate": 0.00018731030804107214,
      "loss": 0.888,
      "step": 2384
    },
    {
      "epoch": 0.1908,
      "grad_norm": 0.5070852041244507,
      "learning_rate": 0.00018730497399653288,
      "loss": 0.9165,
      "step": 2385
    },
    {
      "epoch": 0.19088,
      "grad_norm": 0.49450868368148804,
      "learning_rate": 0.0001872996399519936,
      "loss": 0.801,
      "step": 2386
    },
    {
      "epoch": 0.19096,
      "grad_norm": 0.29035139083862305,
      "learning_rate": 0.00018729430590745433,
      "loss": 0.8092,
      "step": 2387
    },
    {
      "epoch": 0.19104,
      "grad_norm": 0.39168044924736023,
      "learning_rate": 0.00018728897186291507,
      "loss": 0.9501,
      "step": 2388
    },
    {
      "epoch": 0.19112,
      "grad_norm": 0.3060527741909027,
      "learning_rate": 0.00018728363781837578,
      "loss": 0.7721,
      "step": 2389
    },
    {
      "epoch": 0.1912,
      "grad_norm": 0.33277732133865356,
      "learning_rate": 0.00018727830377383652,
      "loss": 0.9878,
      "step": 2390
    },
    {
      "epoch": 0.19128,
      "grad_norm": 0.3138829469680786,
      "learning_rate": 0.00018727296972929723,
      "loss": 0.8807,
      "step": 2391
    },
    {
      "epoch": 0.19136,
      "grad_norm": 0.48762500286102295,
      "learning_rate": 0.00018726763568475797,
      "loss": 0.9492,
      "step": 2392
    },
    {
      "epoch": 0.19144,
      "grad_norm": 0.35602086782455444,
      "learning_rate": 0.00018726230164021869,
      "loss": 1.0638,
      "step": 2393
    },
    {
      "epoch": 0.19152,
      "grad_norm": 0.3045036196708679,
      "learning_rate": 0.00018725696759567943,
      "loss": 0.6757,
      "step": 2394
    },
    {
      "epoch": 0.1916,
      "grad_norm": 0.34892433881759644,
      "learning_rate": 0.00018725163355114017,
      "loss": 0.7447,
      "step": 2395
    },
    {
      "epoch": 0.19168,
      "grad_norm": 0.5080041885375977,
      "learning_rate": 0.00018724629950660088,
      "loss": 1.2972,
      "step": 2396
    },
    {
      "epoch": 0.19176,
      "grad_norm": 0.4682293236255646,
      "learning_rate": 0.00018724096546206162,
      "loss": 0.9325,
      "step": 2397
    },
    {
      "epoch": 0.19184,
      "grad_norm": 0.2970661520957947,
      "learning_rate": 0.00018723563141752233,
      "loss": 0.5726,
      "step": 2398
    },
    {
      "epoch": 0.19192,
      "grad_norm": 0.3306007981300354,
      "learning_rate": 0.00018723029737298307,
      "loss": 0.6712,
      "step": 2399
    },
    {
      "epoch": 0.192,
      "grad_norm": 0.4257286489009857,
      "learning_rate": 0.00018722496332844378,
      "loss": 0.9039,
      "step": 2400
    },
    {
      "epoch": 0.19208,
      "grad_norm": 0.3592603802680969,
      "learning_rate": 0.00018721962928390452,
      "loss": 0.898,
      "step": 2401
    },
    {
      "epoch": 0.19216,
      "grad_norm": 0.2686426639556885,
      "learning_rate": 0.00018721429523936526,
      "loss": 0.9904,
      "step": 2402
    },
    {
      "epoch": 0.19224,
      "grad_norm": 0.3703707456588745,
      "learning_rate": 0.00018720896119482598,
      "loss": 1.1684,
      "step": 2403
    },
    {
      "epoch": 0.19232,
      "grad_norm": 0.440273255109787,
      "learning_rate": 0.00018720362715028672,
      "loss": 0.6986,
      "step": 2404
    },
    {
      "epoch": 0.1924,
      "grad_norm": 0.28658080101013184,
      "learning_rate": 0.00018719829310574743,
      "loss": 0.8423,
      "step": 2405
    },
    {
      "epoch": 0.19248,
      "grad_norm": 0.22678378224372864,
      "learning_rate": 0.00018719295906120817,
      "loss": 0.5025,
      "step": 2406
    },
    {
      "epoch": 0.19256,
      "grad_norm": 0.4145601689815521,
      "learning_rate": 0.00018718762501666888,
      "loss": 0.7805,
      "step": 2407
    },
    {
      "epoch": 0.19264,
      "grad_norm": 0.43782034516334534,
      "learning_rate": 0.00018718229097212962,
      "loss": 0.8722,
      "step": 2408
    },
    {
      "epoch": 0.19272,
      "grad_norm": 0.3161981701850891,
      "learning_rate": 0.00018717695692759036,
      "loss": 0.9376,
      "step": 2409
    },
    {
      "epoch": 0.1928,
      "grad_norm": 0.3937050402164459,
      "learning_rate": 0.00018717162288305107,
      "loss": 0.7165,
      "step": 2410
    },
    {
      "epoch": 0.19288,
      "grad_norm": 0.37834808230400085,
      "learning_rate": 0.00018716628883851181,
      "loss": 0.6731,
      "step": 2411
    },
    {
      "epoch": 0.19296,
      "grad_norm": 0.33351609110832214,
      "learning_rate": 0.00018716095479397253,
      "loss": 0.9197,
      "step": 2412
    },
    {
      "epoch": 0.19304,
      "grad_norm": 0.33618494868278503,
      "learning_rate": 0.00018715562074943327,
      "loss": 1.162,
      "step": 2413
    },
    {
      "epoch": 0.19312,
      "grad_norm": 0.4173821210861206,
      "learning_rate": 0.00018715028670489398,
      "loss": 0.6349,
      "step": 2414
    },
    {
      "epoch": 0.1932,
      "grad_norm": 0.2843706011772156,
      "learning_rate": 0.00018714495266035472,
      "loss": 0.7848,
      "step": 2415
    },
    {
      "epoch": 0.19328,
      "grad_norm": 0.3009262979030609,
      "learning_rate": 0.00018713961861581546,
      "loss": 0.8751,
      "step": 2416
    },
    {
      "epoch": 0.19336,
      "grad_norm": 0.3509789705276489,
      "learning_rate": 0.00018713428457127617,
      "loss": 0.6319,
      "step": 2417
    },
    {
      "epoch": 0.19344,
      "grad_norm": 0.3577412962913513,
      "learning_rate": 0.0001871289505267369,
      "loss": 0.7906,
      "step": 2418
    },
    {
      "epoch": 0.19352,
      "grad_norm": 0.33269527554512024,
      "learning_rate": 0.00018712361648219762,
      "loss": 0.9376,
      "step": 2419
    },
    {
      "epoch": 0.1936,
      "grad_norm": 0.38712912797927856,
      "learning_rate": 0.00018711828243765836,
      "loss": 0.852,
      "step": 2420
    },
    {
      "epoch": 0.19368,
      "grad_norm": 0.361029714345932,
      "learning_rate": 0.00018711294839311908,
      "loss": 0.7143,
      "step": 2421
    },
    {
      "epoch": 0.19376,
      "grad_norm": 0.37129560112953186,
      "learning_rate": 0.00018710761434857982,
      "loss": 0.7707,
      "step": 2422
    },
    {
      "epoch": 0.19384,
      "grad_norm": 0.32101964950561523,
      "learning_rate": 0.00018710228030404056,
      "loss": 0.9846,
      "step": 2423
    },
    {
      "epoch": 0.19392,
      "grad_norm": 0.35038816928863525,
      "learning_rate": 0.00018709694625950127,
      "loss": 0.8511,
      "step": 2424
    },
    {
      "epoch": 0.194,
      "grad_norm": 0.41828620433807373,
      "learning_rate": 0.000187091612214962,
      "loss": 0.7259,
      "step": 2425
    },
    {
      "epoch": 0.19408,
      "grad_norm": 0.30659860372543335,
      "learning_rate": 0.00018708627817042272,
      "loss": 0.9203,
      "step": 2426
    },
    {
      "epoch": 0.19416,
      "grad_norm": 0.4124746024608612,
      "learning_rate": 0.00018708094412588346,
      "loss": 0.8846,
      "step": 2427
    },
    {
      "epoch": 0.19424,
      "grad_norm": 0.4018566906452179,
      "learning_rate": 0.0001870756100813442,
      "loss": 0.785,
      "step": 2428
    },
    {
      "epoch": 0.19432,
      "grad_norm": 0.3418154716491699,
      "learning_rate": 0.00018707027603680491,
      "loss": 0.9321,
      "step": 2429
    },
    {
      "epoch": 0.1944,
      "grad_norm": 0.29033973813056946,
      "learning_rate": 0.00018706494199226565,
      "loss": 0.6299,
      "step": 2430
    },
    {
      "epoch": 0.19448,
      "grad_norm": 0.40016013383865356,
      "learning_rate": 0.00018705960794772637,
      "loss": 0.7533,
      "step": 2431
    },
    {
      "epoch": 0.19456,
      "grad_norm": 0.294696182012558,
      "learning_rate": 0.0001870542739031871,
      "loss": 0.8415,
      "step": 2432
    },
    {
      "epoch": 0.19464,
      "grad_norm": 0.37812018394470215,
      "learning_rate": 0.00018704893985864782,
      "loss": 0.6058,
      "step": 2433
    },
    {
      "epoch": 0.19472,
      "grad_norm": 0.3934409022331238,
      "learning_rate": 0.00018704360581410856,
      "loss": 0.7849,
      "step": 2434
    },
    {
      "epoch": 0.1948,
      "grad_norm": 0.32044103741645813,
      "learning_rate": 0.0001870382717695693,
      "loss": 1.0376,
      "step": 2435
    },
    {
      "epoch": 0.19488,
      "grad_norm": 0.32365328073501587,
      "learning_rate": 0.00018703293772503,
      "loss": 0.6405,
      "step": 2436
    },
    {
      "epoch": 0.19496,
      "grad_norm": 0.33662840723991394,
      "learning_rate": 0.00018702760368049075,
      "loss": 1.039,
      "step": 2437
    },
    {
      "epoch": 0.19504,
      "grad_norm": 0.3140758275985718,
      "learning_rate": 0.00018702226963595146,
      "loss": 0.5692,
      "step": 2438
    },
    {
      "epoch": 0.19512,
      "grad_norm": 0.27548229694366455,
      "learning_rate": 0.0001870169355914122,
      "loss": 0.7009,
      "step": 2439
    },
    {
      "epoch": 0.1952,
      "grad_norm": 0.3558300733566284,
      "learning_rate": 0.00018701160154687292,
      "loss": 0.7136,
      "step": 2440
    },
    {
      "epoch": 0.19528,
      "grad_norm": 0.342257022857666,
      "learning_rate": 0.00018700626750233366,
      "loss": 0.6755,
      "step": 2441
    },
    {
      "epoch": 0.19536,
      "grad_norm": 0.3767121434211731,
      "learning_rate": 0.0001870009334577944,
      "loss": 0.9123,
      "step": 2442
    },
    {
      "epoch": 0.19544,
      "grad_norm": 0.3482215404510498,
      "learning_rate": 0.0001869955994132551,
      "loss": 0.9459,
      "step": 2443
    },
    {
      "epoch": 0.19552,
      "grad_norm": 0.3024723529815674,
      "learning_rate": 0.00018699026536871585,
      "loss": 0.9962,
      "step": 2444
    },
    {
      "epoch": 0.1956,
      "grad_norm": 0.3584405183792114,
      "learning_rate": 0.00018698493132417656,
      "loss": 0.771,
      "step": 2445
    },
    {
      "epoch": 0.19568,
      "grad_norm": 0.30002307891845703,
      "learning_rate": 0.0001869795972796373,
      "loss": 1.0547,
      "step": 2446
    },
    {
      "epoch": 0.19576,
      "grad_norm": 0.3699049651622772,
      "learning_rate": 0.00018697426323509801,
      "loss": 0.6677,
      "step": 2447
    },
    {
      "epoch": 0.19584,
      "grad_norm": 0.35263606905937195,
      "learning_rate": 0.00018696892919055875,
      "loss": 0.7876,
      "step": 2448
    },
    {
      "epoch": 0.19592,
      "grad_norm": 0.3523199260234833,
      "learning_rate": 0.0001869635951460195,
      "loss": 0.79,
      "step": 2449
    },
    {
      "epoch": 0.196,
      "grad_norm": 0.3443637490272522,
      "learning_rate": 0.0001869582611014802,
      "loss": 0.6384,
      "step": 2450
    },
    {
      "epoch": 0.19608,
      "grad_norm": 0.3806522786617279,
      "learning_rate": 0.00018695292705694095,
      "loss": 0.7584,
      "step": 2451
    },
    {
      "epoch": 0.19616,
      "grad_norm": 0.43875500559806824,
      "learning_rate": 0.00018694759301240166,
      "loss": 1.0721,
      "step": 2452
    },
    {
      "epoch": 0.19624,
      "grad_norm": 0.3277840316295624,
      "learning_rate": 0.0001869422589678624,
      "loss": 0.747,
      "step": 2453
    },
    {
      "epoch": 0.19632,
      "grad_norm": 0.3499270975589752,
      "learning_rate": 0.0001869369249233231,
      "loss": 0.9054,
      "step": 2454
    },
    {
      "epoch": 0.1964,
      "grad_norm": 0.4085642397403717,
      "learning_rate": 0.00018693159087878385,
      "loss": 0.7108,
      "step": 2455
    },
    {
      "epoch": 0.19648,
      "grad_norm": 0.27238690853118896,
      "learning_rate": 0.0001869262568342446,
      "loss": 0.9599,
      "step": 2456
    },
    {
      "epoch": 0.19656,
      "grad_norm": 0.4819178581237793,
      "learning_rate": 0.0001869209227897053,
      "loss": 0.7227,
      "step": 2457
    },
    {
      "epoch": 0.19664,
      "grad_norm": 0.41192084550857544,
      "learning_rate": 0.00018691558874516604,
      "loss": 0.7629,
      "step": 2458
    },
    {
      "epoch": 0.19672,
      "grad_norm": 0.3231801390647888,
      "learning_rate": 0.00018691025470062676,
      "loss": 1.1289,
      "step": 2459
    },
    {
      "epoch": 0.1968,
      "grad_norm": 0.4104160666465759,
      "learning_rate": 0.0001869049206560875,
      "loss": 0.6611,
      "step": 2460
    },
    {
      "epoch": 0.19688,
      "grad_norm": 0.2929980754852295,
      "learning_rate": 0.0001868995866115482,
      "loss": 0.7361,
      "step": 2461
    },
    {
      "epoch": 0.19696,
      "grad_norm": 0.3300288915634155,
      "learning_rate": 0.00018689425256700895,
      "loss": 0.8987,
      "step": 2462
    },
    {
      "epoch": 0.19704,
      "grad_norm": 0.3341507911682129,
      "learning_rate": 0.0001868889185224697,
      "loss": 0.8712,
      "step": 2463
    },
    {
      "epoch": 0.19712,
      "grad_norm": 0.29291126132011414,
      "learning_rate": 0.0001868835844779304,
      "loss": 0.9281,
      "step": 2464
    },
    {
      "epoch": 0.1972,
      "grad_norm": 0.34465456008911133,
      "learning_rate": 0.00018687825043339114,
      "loss": 0.9259,
      "step": 2465
    },
    {
      "epoch": 0.19728,
      "grad_norm": 0.41770580410957336,
      "learning_rate": 0.00018687291638885186,
      "loss": 1.0263,
      "step": 2466
    },
    {
      "epoch": 0.19736,
      "grad_norm": 0.3187249004840851,
      "learning_rate": 0.0001868675823443126,
      "loss": 0.9908,
      "step": 2467
    },
    {
      "epoch": 0.19744,
      "grad_norm": 0.38447797298431396,
      "learning_rate": 0.0001868622482997733,
      "loss": 0.8507,
      "step": 2468
    },
    {
      "epoch": 0.19752,
      "grad_norm": 0.32260075211524963,
      "learning_rate": 0.00018685691425523405,
      "loss": 0.8281,
      "step": 2469
    },
    {
      "epoch": 0.1976,
      "grad_norm": 0.5885809063911438,
      "learning_rate": 0.0001868515802106948,
      "loss": 1.0515,
      "step": 2470
    },
    {
      "epoch": 0.19768,
      "grad_norm": 0.4487901031970978,
      "learning_rate": 0.0001868462461661555,
      "loss": 1.0013,
      "step": 2471
    },
    {
      "epoch": 0.19776,
      "grad_norm": 0.44458743929862976,
      "learning_rate": 0.00018684091212161624,
      "loss": 0.9793,
      "step": 2472
    },
    {
      "epoch": 0.19784,
      "grad_norm": 0.3051903545856476,
      "learning_rate": 0.00018683557807707695,
      "loss": 0.4768,
      "step": 2473
    },
    {
      "epoch": 0.19792,
      "grad_norm": 0.32853883504867554,
      "learning_rate": 0.0001868302440325377,
      "loss": 0.8459,
      "step": 2474
    },
    {
      "epoch": 0.198,
      "grad_norm": 0.46324455738067627,
      "learning_rate": 0.0001868249099879984,
      "loss": 0.7623,
      "step": 2475
    },
    {
      "epoch": 0.19808,
      "grad_norm": 0.49249011278152466,
      "learning_rate": 0.00018681957594345915,
      "loss": 1.2065,
      "step": 2476
    },
    {
      "epoch": 0.19816,
      "grad_norm": 0.33853134512901306,
      "learning_rate": 0.00018681424189891988,
      "loss": 0.7223,
      "step": 2477
    },
    {
      "epoch": 0.19824,
      "grad_norm": 0.4428042769432068,
      "learning_rate": 0.0001868089078543806,
      "loss": 0.8266,
      "step": 2478
    },
    {
      "epoch": 0.19832,
      "grad_norm": 0.4403071105480194,
      "learning_rate": 0.00018680357380984134,
      "loss": 1.0238,
      "step": 2479
    },
    {
      "epoch": 0.1984,
      "grad_norm": 0.44930610060691833,
      "learning_rate": 0.00018679823976530205,
      "loss": 0.9341,
      "step": 2480
    },
    {
      "epoch": 0.19848,
      "grad_norm": 0.3066444993019104,
      "learning_rate": 0.0001867929057207628,
      "loss": 0.6879,
      "step": 2481
    },
    {
      "epoch": 0.19856,
      "grad_norm": 0.34262362122535706,
      "learning_rate": 0.0001867875716762235,
      "loss": 1.0324,
      "step": 2482
    },
    {
      "epoch": 0.19864,
      "grad_norm": 0.421932190656662,
      "learning_rate": 0.00018678223763168424,
      "loss": 1.0696,
      "step": 2483
    },
    {
      "epoch": 0.19872,
      "grad_norm": 0.41732192039489746,
      "learning_rate": 0.00018677690358714498,
      "loss": 0.9153,
      "step": 2484
    },
    {
      "epoch": 0.1988,
      "grad_norm": 0.2821003198623657,
      "learning_rate": 0.0001867715695426057,
      "loss": 0.5829,
      "step": 2485
    },
    {
      "epoch": 0.19888,
      "grad_norm": 0.39557555317878723,
      "learning_rate": 0.00018676623549806644,
      "loss": 0.8665,
      "step": 2486
    },
    {
      "epoch": 0.19896,
      "grad_norm": 0.3389923870563507,
      "learning_rate": 0.00018676090145352715,
      "loss": 0.7539,
      "step": 2487
    },
    {
      "epoch": 0.19904,
      "grad_norm": 0.34688135981559753,
      "learning_rate": 0.0001867555674089879,
      "loss": 0.7336,
      "step": 2488
    },
    {
      "epoch": 0.19912,
      "grad_norm": 0.40467751026153564,
      "learning_rate": 0.0001867502333644486,
      "loss": 0.8158,
      "step": 2489
    },
    {
      "epoch": 0.1992,
      "grad_norm": 0.43707287311553955,
      "learning_rate": 0.00018674489931990934,
      "loss": 0.6482,
      "step": 2490
    },
    {
      "epoch": 0.19928,
      "grad_norm": 0.3207494616508484,
      "learning_rate": 0.00018673956527537005,
      "loss": 0.6697,
      "step": 2491
    },
    {
      "epoch": 0.19936,
      "grad_norm": 0.2679714262485504,
      "learning_rate": 0.0001867342312308308,
      "loss": 0.7188,
      "step": 2492
    },
    {
      "epoch": 0.19944,
      "grad_norm": 0.348784863948822,
      "learning_rate": 0.0001867288971862915,
      "loss": 0.985,
      "step": 2493
    },
    {
      "epoch": 0.19952,
      "grad_norm": 0.36435216665267944,
      "learning_rate": 0.00018672356314175225,
      "loss": 0.9941,
      "step": 2494
    },
    {
      "epoch": 0.1996,
      "grad_norm": 0.41115641593933105,
      "learning_rate": 0.00018671822909721296,
      "loss": 0.7231,
      "step": 2495
    },
    {
      "epoch": 0.19968,
      "grad_norm": 0.37459731101989746,
      "learning_rate": 0.0001867128950526737,
      "loss": 0.9063,
      "step": 2496
    },
    {
      "epoch": 0.19976,
      "grad_norm": 0.3226015269756317,
      "learning_rate": 0.0001867075610081344,
      "loss": 0.5421,
      "step": 2497
    },
    {
      "epoch": 0.19984,
      "grad_norm": 0.29669055342674255,
      "learning_rate": 0.00018670222696359515,
      "loss": 1.0472,
      "step": 2498
    },
    {
      "epoch": 0.19992,
      "grad_norm": 0.4682901203632355,
      "learning_rate": 0.00018669689291905586,
      "loss": 0.8047,
      "step": 2499
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.2922195494174957,
      "learning_rate": 0.0001866915588745166,
      "loss": 0.4707,
      "step": 2500
    },
    {
      "epoch": 0.20008,
      "grad_norm": 0.3932326138019562,
      "learning_rate": 0.00018668622482997734,
      "loss": 0.8082,
      "step": 2501
    },
    {
      "epoch": 0.20016,
      "grad_norm": 0.3507368862628937,
      "learning_rate": 0.00018668089078543806,
      "loss": 0.4915,
      "step": 2502
    },
    {
      "epoch": 0.20024,
      "grad_norm": 0.2743152976036072,
      "learning_rate": 0.0001866755567408988,
      "loss": 0.7851,
      "step": 2503
    },
    {
      "epoch": 0.20032,
      "grad_norm": 0.46947768330574036,
      "learning_rate": 0.0001866702226963595,
      "loss": 0.7247,
      "step": 2504
    },
    {
      "epoch": 0.2004,
      "grad_norm": 0.3291948139667511,
      "learning_rate": 0.00018666488865182025,
      "loss": 0.9064,
      "step": 2505
    },
    {
      "epoch": 0.20048,
      "grad_norm": 0.36447733640670776,
      "learning_rate": 0.00018665955460728096,
      "loss": 0.963,
      "step": 2506
    },
    {
      "epoch": 0.20056,
      "grad_norm": 0.3001290559768677,
      "learning_rate": 0.0001866542205627417,
      "loss": 0.8908,
      "step": 2507
    },
    {
      "epoch": 0.20064,
      "grad_norm": 0.3758256137371063,
      "learning_rate": 0.00018664888651820241,
      "loss": 0.7174,
      "step": 2508
    },
    {
      "epoch": 0.20072,
      "grad_norm": 0.32031282782554626,
      "learning_rate": 0.00018664355247366315,
      "loss": 0.5836,
      "step": 2509
    },
    {
      "epoch": 0.2008,
      "grad_norm": 0.3231675922870636,
      "learning_rate": 0.0001866382184291239,
      "loss": 0.8707,
      "step": 2510
    },
    {
      "epoch": 0.20088,
      "grad_norm": 0.2575037479400635,
      "learning_rate": 0.0001866328843845846,
      "loss": 0.4323,
      "step": 2511
    },
    {
      "epoch": 0.20096,
      "grad_norm": 0.39037206768989563,
      "learning_rate": 0.00018662755034004535,
      "loss": 0.7357,
      "step": 2512
    },
    {
      "epoch": 0.20104,
      "grad_norm": 0.39769408106803894,
      "learning_rate": 0.00018662221629550606,
      "loss": 0.7701,
      "step": 2513
    },
    {
      "epoch": 0.20112,
      "grad_norm": 0.48201704025268555,
      "learning_rate": 0.0001866168822509668,
      "loss": 1.0427,
      "step": 2514
    },
    {
      "epoch": 0.2012,
      "grad_norm": 0.3345050811767578,
      "learning_rate": 0.0001866115482064275,
      "loss": 0.6297,
      "step": 2515
    },
    {
      "epoch": 0.20128,
      "grad_norm": 0.3063221275806427,
      "learning_rate": 0.00018660621416188825,
      "loss": 0.543,
      "step": 2516
    },
    {
      "epoch": 0.20136,
      "grad_norm": 0.33751845359802246,
      "learning_rate": 0.000186600880117349,
      "loss": 0.8886,
      "step": 2517
    },
    {
      "epoch": 0.20144,
      "grad_norm": 0.40160804986953735,
      "learning_rate": 0.0001865955460728097,
      "loss": 0.7717,
      "step": 2518
    },
    {
      "epoch": 0.20152,
      "grad_norm": 0.3399900794029236,
      "learning_rate": 0.00018659021202827044,
      "loss": 0.8796,
      "step": 2519
    },
    {
      "epoch": 0.2016,
      "grad_norm": 0.3573977053165436,
      "learning_rate": 0.00018658487798373116,
      "loss": 0.5998,
      "step": 2520
    },
    {
      "epoch": 0.20168,
      "grad_norm": 0.3178727626800537,
      "learning_rate": 0.0001865795439391919,
      "loss": 0.9669,
      "step": 2521
    },
    {
      "epoch": 0.20176,
      "grad_norm": 0.2731646001338959,
      "learning_rate": 0.0001865742098946526,
      "loss": 0.6495,
      "step": 2522
    },
    {
      "epoch": 0.20184,
      "grad_norm": 0.3311046063899994,
      "learning_rate": 0.00018656887585011335,
      "loss": 1.0081,
      "step": 2523
    },
    {
      "epoch": 0.20192,
      "grad_norm": 0.2925266623497009,
      "learning_rate": 0.0001865635418055741,
      "loss": 0.8996,
      "step": 2524
    },
    {
      "epoch": 0.202,
      "grad_norm": 0.32407093048095703,
      "learning_rate": 0.0001865582077610348,
      "loss": 0.5209,
      "step": 2525
    },
    {
      "epoch": 0.20208,
      "grad_norm": 0.3703291714191437,
      "learning_rate": 0.00018655287371649554,
      "loss": 0.8164,
      "step": 2526
    },
    {
      "epoch": 0.20216,
      "grad_norm": 0.4389185309410095,
      "learning_rate": 0.00018654753967195625,
      "loss": 0.7768,
      "step": 2527
    },
    {
      "epoch": 0.20224,
      "grad_norm": 0.34351977705955505,
      "learning_rate": 0.000186542205627417,
      "loss": 0.8711,
      "step": 2528
    },
    {
      "epoch": 0.20232,
      "grad_norm": 0.33638647198677063,
      "learning_rate": 0.00018653687158287773,
      "loss": 0.7865,
      "step": 2529
    },
    {
      "epoch": 0.2024,
      "grad_norm": 0.3713456988334656,
      "learning_rate": 0.00018653153753833845,
      "loss": 0.8887,
      "step": 2530
    },
    {
      "epoch": 0.20248,
      "grad_norm": 0.45480671525001526,
      "learning_rate": 0.00018652620349379919,
      "loss": 0.8988,
      "step": 2531
    },
    {
      "epoch": 0.20256,
      "grad_norm": 0.3526155948638916,
      "learning_rate": 0.0001865208694492599,
      "loss": 0.9066,
      "step": 2532
    },
    {
      "epoch": 0.20264,
      "grad_norm": 0.4500781297683716,
      "learning_rate": 0.00018651553540472064,
      "loss": 0.9856,
      "step": 2533
    },
    {
      "epoch": 0.20272,
      "grad_norm": 0.3920963406562805,
      "learning_rate": 0.00018651020136018135,
      "loss": 0.6146,
      "step": 2534
    },
    {
      "epoch": 0.2028,
      "grad_norm": 0.47045254707336426,
      "learning_rate": 0.0001865048673156421,
      "loss": 1.0905,
      "step": 2535
    },
    {
      "epoch": 0.20288,
      "grad_norm": 0.23084591329097748,
      "learning_rate": 0.00018649953327110283,
      "loss": 0.7651,
      "step": 2536
    },
    {
      "epoch": 0.20296,
      "grad_norm": 0.3844718039035797,
      "learning_rate": 0.00018649419922656354,
      "loss": 0.6666,
      "step": 2537
    },
    {
      "epoch": 0.20304,
      "grad_norm": 0.3455267548561096,
      "learning_rate": 0.00018648886518202428,
      "loss": 0.7007,
      "step": 2538
    },
    {
      "epoch": 0.20312,
      "grad_norm": 0.42276838421821594,
      "learning_rate": 0.000186483531137485,
      "loss": 0.9544,
      "step": 2539
    },
    {
      "epoch": 0.2032,
      "grad_norm": 0.31138357520103455,
      "learning_rate": 0.00018647819709294574,
      "loss": 0.7301,
      "step": 2540
    },
    {
      "epoch": 0.20328,
      "grad_norm": 0.3842490315437317,
      "learning_rate": 0.00018647286304840645,
      "loss": 0.6911,
      "step": 2541
    },
    {
      "epoch": 0.20336,
      "grad_norm": 0.40021899342536926,
      "learning_rate": 0.0001864675290038672,
      "loss": 0.8226,
      "step": 2542
    },
    {
      "epoch": 0.20344,
      "grad_norm": 0.3432733118534088,
      "learning_rate": 0.00018646219495932793,
      "loss": 0.639,
      "step": 2543
    },
    {
      "epoch": 0.20352,
      "grad_norm": 0.3781810700893402,
      "learning_rate": 0.00018645686091478864,
      "loss": 1.0608,
      "step": 2544
    },
    {
      "epoch": 0.2036,
      "grad_norm": 0.2756880521774292,
      "learning_rate": 0.00018645152687024938,
      "loss": 0.8206,
      "step": 2545
    },
    {
      "epoch": 0.20368,
      "grad_norm": 0.28214043378829956,
      "learning_rate": 0.0001864461928257101,
      "loss": 0.943,
      "step": 2546
    },
    {
      "epoch": 0.20376,
      "grad_norm": 0.3789740800857544,
      "learning_rate": 0.00018644085878117083,
      "loss": 1.1052,
      "step": 2547
    },
    {
      "epoch": 0.20384,
      "grad_norm": 0.4201670289039612,
      "learning_rate": 0.00018643552473663155,
      "loss": 0.8067,
      "step": 2548
    },
    {
      "epoch": 0.20392,
      "grad_norm": 0.3956574499607086,
      "learning_rate": 0.0001864301906920923,
      "loss": 0.6816,
      "step": 2549
    },
    {
      "epoch": 0.204,
      "grad_norm": 0.3220095932483673,
      "learning_rate": 0.00018642485664755303,
      "loss": 0.7924,
      "step": 2550
    },
    {
      "epoch": 0.20408,
      "grad_norm": 0.2710227966308594,
      "learning_rate": 0.00018641952260301374,
      "loss": 0.8945,
      "step": 2551
    },
    {
      "epoch": 0.20416,
      "grad_norm": 0.41229385137557983,
      "learning_rate": 0.00018641418855847448,
      "loss": 0.7931,
      "step": 2552
    },
    {
      "epoch": 0.20424,
      "grad_norm": 0.41337960958480835,
      "learning_rate": 0.0001864088545139352,
      "loss": 0.8403,
      "step": 2553
    },
    {
      "epoch": 0.20432,
      "grad_norm": 0.25426575541496277,
      "learning_rate": 0.00018640352046939593,
      "loss": 0.9064,
      "step": 2554
    },
    {
      "epoch": 0.2044,
      "grad_norm": 0.45759809017181396,
      "learning_rate": 0.00018639818642485664,
      "loss": 0.8419,
      "step": 2555
    },
    {
      "epoch": 0.20448,
      "grad_norm": 0.4058608114719391,
      "learning_rate": 0.00018639285238031738,
      "loss": 0.7905,
      "step": 2556
    },
    {
      "epoch": 0.20456,
      "grad_norm": 0.34989696741104126,
      "learning_rate": 0.00018638751833577812,
      "loss": 0.8187,
      "step": 2557
    },
    {
      "epoch": 0.20464,
      "grad_norm": 0.3080486059188843,
      "learning_rate": 0.00018638218429123884,
      "loss": 0.5477,
      "step": 2558
    },
    {
      "epoch": 0.20472,
      "grad_norm": 0.29526522755622864,
      "learning_rate": 0.00018637685024669958,
      "loss": 0.5896,
      "step": 2559
    },
    {
      "epoch": 0.2048,
      "grad_norm": 0.3673998713493347,
      "learning_rate": 0.0001863715162021603,
      "loss": 0.7346,
      "step": 2560
    },
    {
      "epoch": 0.20488,
      "grad_norm": 0.3155829608440399,
      "learning_rate": 0.00018636618215762103,
      "loss": 0.6107,
      "step": 2561
    },
    {
      "epoch": 0.20496,
      "grad_norm": 0.3241114616394043,
      "learning_rate": 0.00018636084811308174,
      "loss": 0.6014,
      "step": 2562
    },
    {
      "epoch": 0.20504,
      "grad_norm": 0.44009119272232056,
      "learning_rate": 0.00018635551406854248,
      "loss": 0.8893,
      "step": 2563
    },
    {
      "epoch": 0.20512,
      "grad_norm": 0.39313575625419617,
      "learning_rate": 0.00018635018002400322,
      "loss": 0.9394,
      "step": 2564
    },
    {
      "epoch": 0.2052,
      "grad_norm": 0.36469683051109314,
      "learning_rate": 0.00018634484597946393,
      "loss": 0.7301,
      "step": 2565
    },
    {
      "epoch": 0.20528,
      "grad_norm": 0.42303723096847534,
      "learning_rate": 0.00018633951193492467,
      "loss": 1.1005,
      "step": 2566
    },
    {
      "epoch": 0.20536,
      "grad_norm": 0.3974556028842926,
      "learning_rate": 0.0001863341778903854,
      "loss": 0.9024,
      "step": 2567
    },
    {
      "epoch": 0.20544,
      "grad_norm": 0.3264044523239136,
      "learning_rate": 0.00018632884384584613,
      "loss": 0.6604,
      "step": 2568
    },
    {
      "epoch": 0.20552,
      "grad_norm": 0.4457472860813141,
      "learning_rate": 0.00018632350980130684,
      "loss": 0.892,
      "step": 2569
    },
    {
      "epoch": 0.2056,
      "grad_norm": 0.32185253500938416,
      "learning_rate": 0.00018631817575676758,
      "loss": 1.0396,
      "step": 2570
    },
    {
      "epoch": 0.20568,
      "grad_norm": 0.3899078667163849,
      "learning_rate": 0.00018631284171222832,
      "loss": 0.7223,
      "step": 2571
    },
    {
      "epoch": 0.20576,
      "grad_norm": 0.3351409137248993,
      "learning_rate": 0.00018630750766768903,
      "loss": 1.1706,
      "step": 2572
    },
    {
      "epoch": 0.20584,
      "grad_norm": 0.46078789234161377,
      "learning_rate": 0.00018630217362314977,
      "loss": 0.8866,
      "step": 2573
    },
    {
      "epoch": 0.20592,
      "grad_norm": 0.39954617619514465,
      "learning_rate": 0.00018629683957861048,
      "loss": 0.7641,
      "step": 2574
    },
    {
      "epoch": 0.206,
      "grad_norm": 0.35873913764953613,
      "learning_rate": 0.00018629150553407122,
      "loss": 0.6419,
      "step": 2575
    },
    {
      "epoch": 0.20608,
      "grad_norm": 0.39123085141181946,
      "learning_rate": 0.00018628617148953194,
      "loss": 0.5652,
      "step": 2576
    },
    {
      "epoch": 0.20616,
      "grad_norm": 0.4009503424167633,
      "learning_rate": 0.00018628083744499268,
      "loss": 0.8617,
      "step": 2577
    },
    {
      "epoch": 0.20624,
      "grad_norm": 0.380422443151474,
      "learning_rate": 0.00018627550340045342,
      "loss": 0.9865,
      "step": 2578
    },
    {
      "epoch": 0.20632,
      "grad_norm": 0.38935261964797974,
      "learning_rate": 0.00018627016935591413,
      "loss": 0.7948,
      "step": 2579
    },
    {
      "epoch": 0.2064,
      "grad_norm": 0.34312236309051514,
      "learning_rate": 0.00018626483531137487,
      "loss": 0.639,
      "step": 2580
    },
    {
      "epoch": 0.20648,
      "grad_norm": 0.33276069164276123,
      "learning_rate": 0.00018625950126683558,
      "loss": 0.6254,
      "step": 2581
    },
    {
      "epoch": 0.20656,
      "grad_norm": 0.3267866373062134,
      "learning_rate": 0.00018625416722229632,
      "loss": 1.0029,
      "step": 2582
    },
    {
      "epoch": 0.20664,
      "grad_norm": 0.2703337073326111,
      "learning_rate": 0.00018624883317775706,
      "loss": 0.6152,
      "step": 2583
    },
    {
      "epoch": 0.20672,
      "grad_norm": 0.29695671796798706,
      "learning_rate": 0.00018624349913321777,
      "loss": 0.8857,
      "step": 2584
    },
    {
      "epoch": 0.2068,
      "grad_norm": 0.41375601291656494,
      "learning_rate": 0.00018623816508867851,
      "loss": 0.9636,
      "step": 2585
    },
    {
      "epoch": 0.20688,
      "grad_norm": 0.36358359456062317,
      "learning_rate": 0.00018623283104413923,
      "loss": 0.6613,
      "step": 2586
    },
    {
      "epoch": 0.20696,
      "grad_norm": 0.3241100311279297,
      "learning_rate": 0.00018622749699959997,
      "loss": 0.9801,
      "step": 2587
    },
    {
      "epoch": 0.20704,
      "grad_norm": 0.2854384183883667,
      "learning_rate": 0.00018622216295506068,
      "loss": 0.8424,
      "step": 2588
    },
    {
      "epoch": 0.20712,
      "grad_norm": 0.42014434933662415,
      "learning_rate": 0.00018621682891052142,
      "loss": 0.669,
      "step": 2589
    },
    {
      "epoch": 0.2072,
      "grad_norm": 0.34534531831741333,
      "learning_rate": 0.00018621149486598216,
      "loss": 1.0452,
      "step": 2590
    },
    {
      "epoch": 0.20728,
      "grad_norm": 0.3488130569458008,
      "learning_rate": 0.00018620616082144287,
      "loss": 0.6731,
      "step": 2591
    },
    {
      "epoch": 0.20736,
      "grad_norm": 0.3315920829772949,
      "learning_rate": 0.0001862008267769036,
      "loss": 0.5886,
      "step": 2592
    },
    {
      "epoch": 0.20744,
      "grad_norm": 0.301697701215744,
      "learning_rate": 0.00018619549273236432,
      "loss": 0.8599,
      "step": 2593
    },
    {
      "epoch": 0.20752,
      "grad_norm": 0.48622792959213257,
      "learning_rate": 0.00018619015868782506,
      "loss": 1.2195,
      "step": 2594
    },
    {
      "epoch": 0.2076,
      "grad_norm": 0.3840795159339905,
      "learning_rate": 0.00018618482464328578,
      "loss": 0.7685,
      "step": 2595
    },
    {
      "epoch": 0.20768,
      "grad_norm": 0.4286309778690338,
      "learning_rate": 0.00018617949059874652,
      "loss": 0.9598,
      "step": 2596
    },
    {
      "epoch": 0.20776,
      "grad_norm": 0.34539976716041565,
      "learning_rate": 0.00018617415655420726,
      "loss": 0.6888,
      "step": 2597
    },
    {
      "epoch": 0.20784,
      "grad_norm": 0.39126908779144287,
      "learning_rate": 0.00018616882250966797,
      "loss": 0.7483,
      "step": 2598
    },
    {
      "epoch": 0.20792,
      "grad_norm": 0.2803817689418793,
      "learning_rate": 0.0001861634884651287,
      "loss": 1.1143,
      "step": 2599
    },
    {
      "epoch": 0.208,
      "grad_norm": 0.31638577580451965,
      "learning_rate": 0.00018615815442058942,
      "loss": 0.6792,
      "step": 2600
    },
    {
      "epoch": 0.20808,
      "grad_norm": 0.30729037523269653,
      "learning_rate": 0.00018615282037605016,
      "loss": 1.1406,
      "step": 2601
    },
    {
      "epoch": 0.20816,
      "grad_norm": 0.32770898938179016,
      "learning_rate": 0.00018614748633151088,
      "loss": 0.986,
      "step": 2602
    },
    {
      "epoch": 0.20824,
      "grad_norm": 0.43326452374458313,
      "learning_rate": 0.00018614215228697162,
      "loss": 0.7874,
      "step": 2603
    },
    {
      "epoch": 0.20832,
      "grad_norm": 0.40520238876342773,
      "learning_rate": 0.00018613681824243235,
      "loss": 0.8853,
      "step": 2604
    },
    {
      "epoch": 0.2084,
      "grad_norm": 0.2871444821357727,
      "learning_rate": 0.00018613148419789307,
      "loss": 0.9234,
      "step": 2605
    },
    {
      "epoch": 0.20848,
      "grad_norm": 0.42375943064689636,
      "learning_rate": 0.0001861261501533538,
      "loss": 0.6794,
      "step": 2606
    },
    {
      "epoch": 0.20856,
      "grad_norm": 0.3599814772605896,
      "learning_rate": 0.00018612081610881452,
      "loss": 0.6701,
      "step": 2607
    },
    {
      "epoch": 0.20864,
      "grad_norm": 0.30061468482017517,
      "learning_rate": 0.00018611548206427526,
      "loss": 0.5584,
      "step": 2608
    },
    {
      "epoch": 0.20872,
      "grad_norm": 0.3911711573600769,
      "learning_rate": 0.00018611014801973597,
      "loss": 0.8507,
      "step": 2609
    },
    {
      "epoch": 0.2088,
      "grad_norm": 0.30835357308387756,
      "learning_rate": 0.0001861048139751967,
      "loss": 0.8381,
      "step": 2610
    },
    {
      "epoch": 0.20888,
      "grad_norm": 0.3419218957424164,
      "learning_rate": 0.00018609947993065743,
      "loss": 0.7369,
      "step": 2611
    },
    {
      "epoch": 0.20896,
      "grad_norm": 0.365052193403244,
      "learning_rate": 0.00018609414588611817,
      "loss": 0.9358,
      "step": 2612
    },
    {
      "epoch": 0.20904,
      "grad_norm": 0.47742342948913574,
      "learning_rate": 0.0001860888118415789,
      "loss": 0.9091,
      "step": 2613
    },
    {
      "epoch": 0.20912,
      "grad_norm": 0.5324476361274719,
      "learning_rate": 0.00018608347779703962,
      "loss": 0.9196,
      "step": 2614
    },
    {
      "epoch": 0.2092,
      "grad_norm": 0.3750551640987396,
      "learning_rate": 0.00018607814375250036,
      "loss": 0.9127,
      "step": 2615
    },
    {
      "epoch": 0.20928,
      "grad_norm": 0.31658080220222473,
      "learning_rate": 0.00018607280970796107,
      "loss": 0.8058,
      "step": 2616
    },
    {
      "epoch": 0.20936,
      "grad_norm": 0.3229559361934662,
      "learning_rate": 0.0001860674756634218,
      "loss": 0.8363,
      "step": 2617
    },
    {
      "epoch": 0.20944,
      "grad_norm": 0.4309985339641571,
      "learning_rate": 0.00018606214161888252,
      "loss": 0.6795,
      "step": 2618
    },
    {
      "epoch": 0.20952,
      "grad_norm": 0.41647911071777344,
      "learning_rate": 0.00018605680757434326,
      "loss": 0.8164,
      "step": 2619
    },
    {
      "epoch": 0.2096,
      "grad_norm": 0.3158038854598999,
      "learning_rate": 0.00018605147352980398,
      "loss": 0.6955,
      "step": 2620
    },
    {
      "epoch": 0.20968,
      "grad_norm": 0.3893124759197235,
      "learning_rate": 0.00018604613948526472,
      "loss": 0.811,
      "step": 2621
    },
    {
      "epoch": 0.20976,
      "grad_norm": 0.36063864827156067,
      "learning_rate": 0.00018604080544072543,
      "loss": 1.2764,
      "step": 2622
    },
    {
      "epoch": 0.20984,
      "grad_norm": 0.32236266136169434,
      "learning_rate": 0.00018603547139618617,
      "loss": 0.4578,
      "step": 2623
    },
    {
      "epoch": 0.20992,
      "grad_norm": 0.42837393283843994,
      "learning_rate": 0.00018603013735164688,
      "loss": 1.0129,
      "step": 2624
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.38000303506851196,
      "learning_rate": 0.00018602480330710762,
      "loss": 0.6642,
      "step": 2625
    },
    {
      "epoch": 0.21008,
      "grad_norm": 0.3761065900325775,
      "learning_rate": 0.00018601946926256833,
      "loss": 0.8575,
      "step": 2626
    },
    {
      "epoch": 0.21016,
      "grad_norm": 0.3994998335838318,
      "learning_rate": 0.00018601413521802907,
      "loss": 0.9704,
      "step": 2627
    },
    {
      "epoch": 0.21024,
      "grad_norm": 0.3095439672470093,
      "learning_rate": 0.00018600880117348979,
      "loss": 0.7748,
      "step": 2628
    },
    {
      "epoch": 0.21032,
      "grad_norm": 0.37208452820777893,
      "learning_rate": 0.00018600346712895053,
      "loss": 1.1961,
      "step": 2629
    },
    {
      "epoch": 0.2104,
      "grad_norm": 0.431285560131073,
      "learning_rate": 0.00018599813308441127,
      "loss": 1.1093,
      "step": 2630
    },
    {
      "epoch": 0.21048,
      "grad_norm": 0.35497450828552246,
      "learning_rate": 0.00018599279903987198,
      "loss": 1.0041,
      "step": 2631
    },
    {
      "epoch": 0.21056,
      "grad_norm": 0.3947904407978058,
      "learning_rate": 0.00018598746499533272,
      "loss": 0.7704,
      "step": 2632
    },
    {
      "epoch": 0.21064,
      "grad_norm": 0.316914826631546,
      "learning_rate": 0.00018598213095079343,
      "loss": 0.7942,
      "step": 2633
    },
    {
      "epoch": 0.21072,
      "grad_norm": 0.40029558539390564,
      "learning_rate": 0.00018597679690625417,
      "loss": 0.8622,
      "step": 2634
    },
    {
      "epoch": 0.2108,
      "grad_norm": 0.28867843747138977,
      "learning_rate": 0.00018597146286171488,
      "loss": 0.5594,
      "step": 2635
    },
    {
      "epoch": 0.21088,
      "grad_norm": 0.2736422121524811,
      "learning_rate": 0.00018596612881717562,
      "loss": 0.9713,
      "step": 2636
    },
    {
      "epoch": 0.21096,
      "grad_norm": 0.3606787919998169,
      "learning_rate": 0.00018596079477263636,
      "loss": 0.9207,
      "step": 2637
    },
    {
      "epoch": 0.21104,
      "grad_norm": 0.39053046703338623,
      "learning_rate": 0.00018595546072809708,
      "loss": 0.8578,
      "step": 2638
    },
    {
      "epoch": 0.21112,
      "grad_norm": 0.387140691280365,
      "learning_rate": 0.00018595012668355782,
      "loss": 0.8678,
      "step": 2639
    },
    {
      "epoch": 0.2112,
      "grad_norm": 0.27044564485549927,
      "learning_rate": 0.00018594479263901853,
      "loss": 0.7695,
      "step": 2640
    },
    {
      "epoch": 0.21128,
      "grad_norm": 0.3820771872997284,
      "learning_rate": 0.00018593945859447927,
      "loss": 0.5821,
      "step": 2641
    },
    {
      "epoch": 0.21136,
      "grad_norm": 0.38025590777397156,
      "learning_rate": 0.00018593412454993998,
      "loss": 0.7673,
      "step": 2642
    },
    {
      "epoch": 0.21144,
      "grad_norm": 0.36183416843414307,
      "learning_rate": 0.00018592879050540072,
      "loss": 1.1086,
      "step": 2643
    },
    {
      "epoch": 0.21152,
      "grad_norm": 0.34312814474105835,
      "learning_rate": 0.00018592345646086146,
      "loss": 0.8664,
      "step": 2644
    },
    {
      "epoch": 0.2116,
      "grad_norm": 0.3613312244415283,
      "learning_rate": 0.00018591812241632217,
      "loss": 0.8107,
      "step": 2645
    },
    {
      "epoch": 0.21168,
      "grad_norm": 0.3415832817554474,
      "learning_rate": 0.0001859127883717829,
      "loss": 0.6383,
      "step": 2646
    },
    {
      "epoch": 0.21176,
      "grad_norm": 0.30090662837028503,
      "learning_rate": 0.00018590745432724363,
      "loss": 0.7241,
      "step": 2647
    },
    {
      "epoch": 0.21184,
      "grad_norm": 0.36669236421585083,
      "learning_rate": 0.00018590212028270437,
      "loss": 0.6992,
      "step": 2648
    },
    {
      "epoch": 0.21192,
      "grad_norm": 0.43294602632522583,
      "learning_rate": 0.00018589678623816508,
      "loss": 0.7929,
      "step": 2649
    },
    {
      "epoch": 0.212,
      "grad_norm": 0.3064103424549103,
      "learning_rate": 0.00018589145219362582,
      "loss": 0.8852,
      "step": 2650
    },
    {
      "epoch": 0.21208,
      "grad_norm": 0.39075541496276855,
      "learning_rate": 0.00018588611814908656,
      "loss": 0.9124,
      "step": 2651
    },
    {
      "epoch": 0.21216,
      "grad_norm": 0.41245803236961365,
      "learning_rate": 0.00018588078410454727,
      "loss": 0.6694,
      "step": 2652
    },
    {
      "epoch": 0.21224,
      "grad_norm": 0.3579048216342926,
      "learning_rate": 0.000185875450060008,
      "loss": 0.7846,
      "step": 2653
    },
    {
      "epoch": 0.21232,
      "grad_norm": 0.31646376848220825,
      "learning_rate": 0.00018587011601546872,
      "loss": 0.5565,
      "step": 2654
    },
    {
      "epoch": 0.2124,
      "grad_norm": 0.4871835708618164,
      "learning_rate": 0.00018586478197092946,
      "loss": 1.1528,
      "step": 2655
    },
    {
      "epoch": 0.21248,
      "grad_norm": 0.37775856256484985,
      "learning_rate": 0.00018585944792639018,
      "loss": 0.6867,
      "step": 2656
    },
    {
      "epoch": 0.21256,
      "grad_norm": 0.34866178035736084,
      "learning_rate": 0.00018585411388185092,
      "loss": 0.7734,
      "step": 2657
    },
    {
      "epoch": 0.21264,
      "grad_norm": 0.3607982099056244,
      "learning_rate": 0.00018584877983731166,
      "loss": 0.6546,
      "step": 2658
    },
    {
      "epoch": 0.21272,
      "grad_norm": 0.2810268700122833,
      "learning_rate": 0.00018584344579277237,
      "loss": 0.441,
      "step": 2659
    },
    {
      "epoch": 0.2128,
      "grad_norm": 0.3269822895526886,
      "learning_rate": 0.0001858381117482331,
      "loss": 0.9025,
      "step": 2660
    },
    {
      "epoch": 0.21288,
      "grad_norm": 0.3793186545372009,
      "learning_rate": 0.00018583277770369382,
      "loss": 0.6737,
      "step": 2661
    },
    {
      "epoch": 0.21296,
      "grad_norm": 0.5532060265541077,
      "learning_rate": 0.00018582744365915456,
      "loss": 1.0157,
      "step": 2662
    },
    {
      "epoch": 0.21304,
      "grad_norm": 0.32630062103271484,
      "learning_rate": 0.00018582210961461527,
      "loss": 0.7888,
      "step": 2663
    },
    {
      "epoch": 0.21312,
      "grad_norm": 0.25159740447998047,
      "learning_rate": 0.00018581677557007601,
      "loss": 0.9453,
      "step": 2664
    },
    {
      "epoch": 0.2132,
      "grad_norm": 0.40175768733024597,
      "learning_rate": 0.00018581144152553675,
      "loss": 0.947,
      "step": 2665
    },
    {
      "epoch": 0.21328,
      "grad_norm": 0.43491342663764954,
      "learning_rate": 0.00018580610748099747,
      "loss": 0.6397,
      "step": 2666
    },
    {
      "epoch": 0.21336,
      "grad_norm": 0.31227728724479675,
      "learning_rate": 0.0001858007734364582,
      "loss": 1.0357,
      "step": 2667
    },
    {
      "epoch": 0.21344,
      "grad_norm": 0.289044588804245,
      "learning_rate": 0.00018579543939191892,
      "loss": 0.7369,
      "step": 2668
    },
    {
      "epoch": 0.21352,
      "grad_norm": 0.3255469799041748,
      "learning_rate": 0.00018579010534737966,
      "loss": 0.9148,
      "step": 2669
    },
    {
      "epoch": 0.2136,
      "grad_norm": 0.40343499183654785,
      "learning_rate": 0.00018578477130284037,
      "loss": 0.8702,
      "step": 2670
    },
    {
      "epoch": 0.21368,
      "grad_norm": 0.3108302652835846,
      "learning_rate": 0.0001857794372583011,
      "loss": 0.7416,
      "step": 2671
    },
    {
      "epoch": 0.21376,
      "grad_norm": 0.30782029032707214,
      "learning_rate": 0.00018577410321376185,
      "loss": 1.0225,
      "step": 2672
    },
    {
      "epoch": 0.21384,
      "grad_norm": 0.4085750877857208,
      "learning_rate": 0.00018576876916922256,
      "loss": 0.7696,
      "step": 2673
    },
    {
      "epoch": 0.21392,
      "grad_norm": 0.40948039293289185,
      "learning_rate": 0.0001857634351246833,
      "loss": 0.949,
      "step": 2674
    },
    {
      "epoch": 0.214,
      "grad_norm": 0.3986865282058716,
      "learning_rate": 0.00018575810108014402,
      "loss": 0.8791,
      "step": 2675
    },
    {
      "epoch": 0.21408,
      "grad_norm": 0.3640252947807312,
      "learning_rate": 0.00018575276703560476,
      "loss": 0.7826,
      "step": 2676
    },
    {
      "epoch": 0.21416,
      "grad_norm": 0.3647235929965973,
      "learning_rate": 0.0001857474329910655,
      "loss": 0.8536,
      "step": 2677
    },
    {
      "epoch": 0.21424,
      "grad_norm": 0.28981849551200867,
      "learning_rate": 0.0001857420989465262,
      "loss": 0.6535,
      "step": 2678
    },
    {
      "epoch": 0.21432,
      "grad_norm": 0.42926597595214844,
      "learning_rate": 0.00018573676490198695,
      "loss": 0.9017,
      "step": 2679
    },
    {
      "epoch": 0.2144,
      "grad_norm": 0.44450774788856506,
      "learning_rate": 0.00018573143085744766,
      "loss": 0.8485,
      "step": 2680
    },
    {
      "epoch": 0.21448,
      "grad_norm": 0.38605430722236633,
      "learning_rate": 0.0001857260968129084,
      "loss": 0.8833,
      "step": 2681
    },
    {
      "epoch": 0.21456,
      "grad_norm": 0.2707291543483734,
      "learning_rate": 0.00018572076276836911,
      "loss": 0.3852,
      "step": 2682
    },
    {
      "epoch": 0.21464,
      "grad_norm": 0.37574535608291626,
      "learning_rate": 0.00018571542872382985,
      "loss": 0.7364,
      "step": 2683
    },
    {
      "epoch": 0.21472,
      "grad_norm": 0.3254890739917755,
      "learning_rate": 0.0001857100946792906,
      "loss": 1.1414,
      "step": 2684
    },
    {
      "epoch": 0.2148,
      "grad_norm": 0.3598012328147888,
      "learning_rate": 0.0001857047606347513,
      "loss": 0.8735,
      "step": 2685
    },
    {
      "epoch": 0.21488,
      "grad_norm": 0.3628118932247162,
      "learning_rate": 0.00018569942659021205,
      "loss": 0.7153,
      "step": 2686
    },
    {
      "epoch": 0.21496,
      "grad_norm": 0.41205498576164246,
      "learning_rate": 0.00018569409254567276,
      "loss": 1.1881,
      "step": 2687
    },
    {
      "epoch": 0.21504,
      "grad_norm": 0.3078036606311798,
      "learning_rate": 0.0001856887585011335,
      "loss": 0.6696,
      "step": 2688
    },
    {
      "epoch": 0.21512,
      "grad_norm": 0.48149654269218445,
      "learning_rate": 0.0001856834244565942,
      "loss": 0.759,
      "step": 2689
    },
    {
      "epoch": 0.2152,
      "grad_norm": 0.3667354881763458,
      "learning_rate": 0.00018567809041205495,
      "loss": 0.8255,
      "step": 2690
    },
    {
      "epoch": 0.21528,
      "grad_norm": 0.368341863155365,
      "learning_rate": 0.0001856727563675157,
      "loss": 0.6459,
      "step": 2691
    },
    {
      "epoch": 0.21536,
      "grad_norm": 0.35981762409210205,
      "learning_rate": 0.0001856674223229764,
      "loss": 0.696,
      "step": 2692
    },
    {
      "epoch": 0.21544,
      "grad_norm": 0.3309798538684845,
      "learning_rate": 0.00018566208827843714,
      "loss": 0.8923,
      "step": 2693
    },
    {
      "epoch": 0.21552,
      "grad_norm": 0.3235883414745331,
      "learning_rate": 0.00018565675423389786,
      "loss": 0.7014,
      "step": 2694
    },
    {
      "epoch": 0.2156,
      "grad_norm": 0.3939706087112427,
      "learning_rate": 0.0001856514201893586,
      "loss": 0.9213,
      "step": 2695
    },
    {
      "epoch": 0.21568,
      "grad_norm": 0.3641951084136963,
      "learning_rate": 0.0001856460861448193,
      "loss": 0.5698,
      "step": 2696
    },
    {
      "epoch": 0.21576,
      "grad_norm": 0.3241788446903229,
      "learning_rate": 0.00018564075210028005,
      "loss": 0.6671,
      "step": 2697
    },
    {
      "epoch": 0.21584,
      "grad_norm": 0.356711208820343,
      "learning_rate": 0.0001856354180557408,
      "loss": 0.6244,
      "step": 2698
    },
    {
      "epoch": 0.21592,
      "grad_norm": 0.26688653230667114,
      "learning_rate": 0.0001856300840112015,
      "loss": 0.8001,
      "step": 2699
    },
    {
      "epoch": 0.216,
      "grad_norm": 0.3668361306190491,
      "learning_rate": 0.00018562474996666224,
      "loss": 0.8791,
      "step": 2700
    },
    {
      "epoch": 0.21608,
      "grad_norm": 0.40062975883483887,
      "learning_rate": 0.00018561941592212295,
      "loss": 1.0359,
      "step": 2701
    },
    {
      "epoch": 0.21616,
      "grad_norm": 0.3238976299762726,
      "learning_rate": 0.0001856140818775837,
      "loss": 1.0202,
      "step": 2702
    },
    {
      "epoch": 0.21624,
      "grad_norm": 0.4310655891895294,
      "learning_rate": 0.0001856087478330444,
      "loss": 0.8574,
      "step": 2703
    },
    {
      "epoch": 0.21632,
      "grad_norm": 0.4183971583843231,
      "learning_rate": 0.00018560341378850515,
      "loss": 0.7352,
      "step": 2704
    },
    {
      "epoch": 0.2164,
      "grad_norm": 0.3540393114089966,
      "learning_rate": 0.0001855980797439659,
      "loss": 0.7093,
      "step": 2705
    },
    {
      "epoch": 0.21648,
      "grad_norm": 0.3773742914199829,
      "learning_rate": 0.0001855927456994266,
      "loss": 0.7925,
      "step": 2706
    },
    {
      "epoch": 0.21656,
      "grad_norm": 0.3330729007720947,
      "learning_rate": 0.00018558741165488734,
      "loss": 0.8335,
      "step": 2707
    },
    {
      "epoch": 0.21664,
      "grad_norm": 0.3137441575527191,
      "learning_rate": 0.00018558207761034805,
      "loss": 0.7718,
      "step": 2708
    },
    {
      "epoch": 0.21672,
      "grad_norm": 0.37971457839012146,
      "learning_rate": 0.0001855767435658088,
      "loss": 0.8208,
      "step": 2709
    },
    {
      "epoch": 0.2168,
      "grad_norm": 0.40219739079475403,
      "learning_rate": 0.0001855714095212695,
      "loss": 1.178,
      "step": 2710
    },
    {
      "epoch": 0.21688,
      "grad_norm": 0.3421786427497864,
      "learning_rate": 0.00018556607547673024,
      "loss": 0.5697,
      "step": 2711
    },
    {
      "epoch": 0.21696,
      "grad_norm": 0.3477880358695984,
      "learning_rate": 0.00018556074143219098,
      "loss": 0.8237,
      "step": 2712
    },
    {
      "epoch": 0.21704,
      "grad_norm": 0.4104522466659546,
      "learning_rate": 0.0001855554073876517,
      "loss": 0.6921,
      "step": 2713
    },
    {
      "epoch": 0.21712,
      "grad_norm": 0.4164716303348541,
      "learning_rate": 0.00018555007334311244,
      "loss": 0.9084,
      "step": 2714
    },
    {
      "epoch": 0.2172,
      "grad_norm": 0.3574422001838684,
      "learning_rate": 0.00018554473929857315,
      "loss": 0.6026,
      "step": 2715
    },
    {
      "epoch": 0.21728,
      "grad_norm": 0.3996809720993042,
      "learning_rate": 0.0001855394052540339,
      "loss": 1.3686,
      "step": 2716
    },
    {
      "epoch": 0.21736,
      "grad_norm": 0.3635849356651306,
      "learning_rate": 0.0001855340712094946,
      "loss": 1.0827,
      "step": 2717
    },
    {
      "epoch": 0.21744,
      "grad_norm": 0.38369137048721313,
      "learning_rate": 0.00018552873716495534,
      "loss": 0.6601,
      "step": 2718
    },
    {
      "epoch": 0.21752,
      "grad_norm": 0.3737106919288635,
      "learning_rate": 0.00018552340312041608,
      "loss": 0.7535,
      "step": 2719
    },
    {
      "epoch": 0.2176,
      "grad_norm": 0.3093884289264679,
      "learning_rate": 0.0001855180690758768,
      "loss": 1.0482,
      "step": 2720
    },
    {
      "epoch": 0.21768,
      "grad_norm": 0.3826128840446472,
      "learning_rate": 0.00018551273503133753,
      "loss": 0.7481,
      "step": 2721
    },
    {
      "epoch": 0.21776,
      "grad_norm": 0.5495442748069763,
      "learning_rate": 0.00018550740098679825,
      "loss": 0.8785,
      "step": 2722
    },
    {
      "epoch": 0.21784,
      "grad_norm": 0.3447541296482086,
      "learning_rate": 0.000185502066942259,
      "loss": 0.6977,
      "step": 2723
    },
    {
      "epoch": 0.21792,
      "grad_norm": 0.32002612948417664,
      "learning_rate": 0.0001854967328977197,
      "loss": 0.6209,
      "step": 2724
    },
    {
      "epoch": 0.218,
      "grad_norm": 0.388234406709671,
      "learning_rate": 0.00018549139885318044,
      "loss": 0.9245,
      "step": 2725
    },
    {
      "epoch": 0.21808,
      "grad_norm": 0.46651536226272583,
      "learning_rate": 0.00018548606480864118,
      "loss": 0.9243,
      "step": 2726
    },
    {
      "epoch": 0.21816,
      "grad_norm": 0.4148041307926178,
      "learning_rate": 0.0001854807307641019,
      "loss": 0.7819,
      "step": 2727
    },
    {
      "epoch": 0.21824,
      "grad_norm": 0.43181395530700684,
      "learning_rate": 0.00018547539671956263,
      "loss": 0.8517,
      "step": 2728
    },
    {
      "epoch": 0.21832,
      "grad_norm": 0.45558589696884155,
      "learning_rate": 0.00018547006267502335,
      "loss": 0.8294,
      "step": 2729
    },
    {
      "epoch": 0.2184,
      "grad_norm": 0.3322737216949463,
      "learning_rate": 0.00018546472863048408,
      "loss": 1.026,
      "step": 2730
    },
    {
      "epoch": 0.21848,
      "grad_norm": 0.3633175194263458,
      "learning_rate": 0.00018545939458594482,
      "loss": 0.6574,
      "step": 2731
    },
    {
      "epoch": 0.21856,
      "grad_norm": 0.4816407263278961,
      "learning_rate": 0.00018545406054140554,
      "loss": 0.9725,
      "step": 2732
    },
    {
      "epoch": 0.21864,
      "grad_norm": 0.29350942373275757,
      "learning_rate": 0.00018544872649686628,
      "loss": 0.8643,
      "step": 2733
    },
    {
      "epoch": 0.21872,
      "grad_norm": 0.29408013820648193,
      "learning_rate": 0.000185443392452327,
      "loss": 1.0631,
      "step": 2734
    },
    {
      "epoch": 0.2188,
      "grad_norm": 0.4078834652900696,
      "learning_rate": 0.00018543805840778773,
      "loss": 0.7788,
      "step": 2735
    },
    {
      "epoch": 0.21888,
      "grad_norm": 0.3657394349575043,
      "learning_rate": 0.00018543272436324844,
      "loss": 0.889,
      "step": 2736
    },
    {
      "epoch": 0.21896,
      "grad_norm": 0.3531434237957001,
      "learning_rate": 0.00018542739031870918,
      "loss": 0.9855,
      "step": 2737
    },
    {
      "epoch": 0.21904,
      "grad_norm": 0.3441162109375,
      "learning_rate": 0.0001854220562741699,
      "loss": 0.9793,
      "step": 2738
    },
    {
      "epoch": 0.21912,
      "grad_norm": 0.4480980634689331,
      "learning_rate": 0.00018541672222963064,
      "loss": 0.7185,
      "step": 2739
    },
    {
      "epoch": 0.2192,
      "grad_norm": 0.7388961315155029,
      "learning_rate": 0.00018541138818509135,
      "loss": 1.3786,
      "step": 2740
    },
    {
      "epoch": 0.21928,
      "grad_norm": 0.36072275042533875,
      "learning_rate": 0.0001854060541405521,
      "loss": 0.8531,
      "step": 2741
    },
    {
      "epoch": 0.21936,
      "grad_norm": 0.5200390219688416,
      "learning_rate": 0.00018540072009601283,
      "loss": 0.7439,
      "step": 2742
    },
    {
      "epoch": 0.21944,
      "grad_norm": 0.4100026488304138,
      "learning_rate": 0.00018539538605147354,
      "loss": 0.7803,
      "step": 2743
    },
    {
      "epoch": 0.21952,
      "grad_norm": 0.2881898581981659,
      "learning_rate": 0.00018539005200693428,
      "loss": 0.8955,
      "step": 2744
    },
    {
      "epoch": 0.2196,
      "grad_norm": 0.4263027608394623,
      "learning_rate": 0.000185384717962395,
      "loss": 0.874,
      "step": 2745
    },
    {
      "epoch": 0.21968,
      "grad_norm": 0.42018184065818787,
      "learning_rate": 0.00018537938391785573,
      "loss": 0.8142,
      "step": 2746
    },
    {
      "epoch": 0.21976,
      "grad_norm": 0.2691200077533722,
      "learning_rate": 0.00018537404987331645,
      "loss": 0.6992,
      "step": 2747
    },
    {
      "epoch": 0.21984,
      "grad_norm": 0.34238481521606445,
      "learning_rate": 0.00018536871582877719,
      "loss": 0.676,
      "step": 2748
    },
    {
      "epoch": 0.21992,
      "grad_norm": 0.3132726848125458,
      "learning_rate": 0.0001853633817842379,
      "loss": 0.5561,
      "step": 2749
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.4532400071620941,
      "learning_rate": 0.00018535804773969864,
      "loss": 0.7944,
      "step": 2750
    },
    {
      "epoch": 0.22008,
      "grad_norm": 0.3769896924495697,
      "learning_rate": 0.00018535271369515935,
      "loss": 0.5594,
      "step": 2751
    },
    {
      "epoch": 0.22016,
      "grad_norm": 0.3865346610546112,
      "learning_rate": 0.0001853473796506201,
      "loss": 0.7613,
      "step": 2752
    },
    {
      "epoch": 0.22024,
      "grad_norm": 0.4804060757160187,
      "learning_rate": 0.0001853420456060808,
      "loss": 1.1414,
      "step": 2753
    },
    {
      "epoch": 0.22032,
      "grad_norm": 0.3749472200870514,
      "learning_rate": 0.00018533671156154154,
      "loss": 0.5606,
      "step": 2754
    },
    {
      "epoch": 0.2204,
      "grad_norm": 0.43654629588127136,
      "learning_rate": 0.00018533137751700226,
      "loss": 0.6059,
      "step": 2755
    },
    {
      "epoch": 0.22048,
      "grad_norm": 0.38895851373672485,
      "learning_rate": 0.000185326043472463,
      "loss": 0.7242,
      "step": 2756
    },
    {
      "epoch": 0.22056,
      "grad_norm": 0.31893521547317505,
      "learning_rate": 0.00018532070942792374,
      "loss": 0.7983,
      "step": 2757
    },
    {
      "epoch": 0.22064,
      "grad_norm": 0.3860068917274475,
      "learning_rate": 0.00018531537538338445,
      "loss": 0.8427,
      "step": 2758
    },
    {
      "epoch": 0.22072,
      "grad_norm": 0.3431363105773926,
      "learning_rate": 0.0001853100413388452,
      "loss": 0.9926,
      "step": 2759
    },
    {
      "epoch": 0.2208,
      "grad_norm": 0.2781359851360321,
      "learning_rate": 0.0001853047072943059,
      "loss": 0.7596,
      "step": 2760
    },
    {
      "epoch": 0.22088,
      "grad_norm": 0.5284501910209656,
      "learning_rate": 0.00018529937324976664,
      "loss": 1.0542,
      "step": 2761
    },
    {
      "epoch": 0.22096,
      "grad_norm": 0.5033907294273376,
      "learning_rate": 0.00018529403920522735,
      "loss": 0.8577,
      "step": 2762
    },
    {
      "epoch": 0.22104,
      "grad_norm": 0.3731304705142975,
      "learning_rate": 0.0001852887051606881,
      "loss": 0.6257,
      "step": 2763
    },
    {
      "epoch": 0.22112,
      "grad_norm": 0.298905611038208,
      "learning_rate": 0.0001852833711161488,
      "loss": 0.9984,
      "step": 2764
    },
    {
      "epoch": 0.2212,
      "grad_norm": 0.6335195899009705,
      "learning_rate": 0.00018527803707160955,
      "loss": 1.2228,
      "step": 2765
    },
    {
      "epoch": 0.22128,
      "grad_norm": 0.31760647892951965,
      "learning_rate": 0.00018527270302707029,
      "loss": 0.7047,
      "step": 2766
    },
    {
      "epoch": 0.22136,
      "grad_norm": 0.2996101677417755,
      "learning_rate": 0.000185267368982531,
      "loss": 1.0614,
      "step": 2767
    },
    {
      "epoch": 0.22144,
      "grad_norm": 0.32367223501205444,
      "learning_rate": 0.00018526203493799174,
      "loss": 0.8792,
      "step": 2768
    },
    {
      "epoch": 0.22152,
      "grad_norm": 0.37115001678466797,
      "learning_rate": 0.00018525670089345245,
      "loss": 0.879,
      "step": 2769
    },
    {
      "epoch": 0.2216,
      "grad_norm": 0.39635488390922546,
      "learning_rate": 0.0001852513668489132,
      "loss": 0.9009,
      "step": 2770
    },
    {
      "epoch": 0.22168,
      "grad_norm": 0.3302095830440521,
      "learning_rate": 0.0001852460328043739,
      "loss": 0.7449,
      "step": 2771
    },
    {
      "epoch": 0.22176,
      "grad_norm": 0.38706085085868835,
      "learning_rate": 0.00018524069875983464,
      "loss": 1.0172,
      "step": 2772
    },
    {
      "epoch": 0.22184,
      "grad_norm": 0.36175838112831116,
      "learning_rate": 0.00018523536471529538,
      "loss": 0.7217,
      "step": 2773
    },
    {
      "epoch": 0.22192,
      "grad_norm": 0.4514438807964325,
      "learning_rate": 0.0001852300306707561,
      "loss": 1.0904,
      "step": 2774
    },
    {
      "epoch": 0.222,
      "grad_norm": 0.49136000871658325,
      "learning_rate": 0.00018522469662621684,
      "loss": 0.8107,
      "step": 2775
    },
    {
      "epoch": 0.22208,
      "grad_norm": 0.36344054341316223,
      "learning_rate": 0.00018521936258167755,
      "loss": 0.8849,
      "step": 2776
    },
    {
      "epoch": 0.22216,
      "grad_norm": 0.3208533227443695,
      "learning_rate": 0.0001852140285371383,
      "loss": 0.8276,
      "step": 2777
    },
    {
      "epoch": 0.22224,
      "grad_norm": 0.3601326644420624,
      "learning_rate": 0.00018520869449259903,
      "loss": 0.9017,
      "step": 2778
    },
    {
      "epoch": 0.22232,
      "grad_norm": 0.34593045711517334,
      "learning_rate": 0.00018520336044805974,
      "loss": 1.0002,
      "step": 2779
    },
    {
      "epoch": 0.2224,
      "grad_norm": 0.3839644491672516,
      "learning_rate": 0.00018519802640352048,
      "loss": 0.7035,
      "step": 2780
    },
    {
      "epoch": 0.22248,
      "grad_norm": 0.38406234979629517,
      "learning_rate": 0.0001851926923589812,
      "loss": 0.807,
      "step": 2781
    },
    {
      "epoch": 0.22256,
      "grad_norm": 0.31161147356033325,
      "learning_rate": 0.00018518735831444193,
      "loss": 0.586,
      "step": 2782
    },
    {
      "epoch": 0.22264,
      "grad_norm": 0.4960293769836426,
      "learning_rate": 0.00018518202426990265,
      "loss": 0.6846,
      "step": 2783
    },
    {
      "epoch": 0.22272,
      "grad_norm": 0.37496307492256165,
      "learning_rate": 0.00018517669022536339,
      "loss": 0.7033,
      "step": 2784
    },
    {
      "epoch": 0.2228,
      "grad_norm": 0.32773473858833313,
      "learning_rate": 0.00018517135618082413,
      "loss": 0.8947,
      "step": 2785
    },
    {
      "epoch": 0.22288,
      "grad_norm": 0.4123581051826477,
      "learning_rate": 0.00018516602213628484,
      "loss": 0.8762,
      "step": 2786
    },
    {
      "epoch": 0.22296,
      "grad_norm": 0.3885829448699951,
      "learning_rate": 0.00018516068809174558,
      "loss": 1.3217,
      "step": 2787
    },
    {
      "epoch": 0.22304,
      "grad_norm": 0.39982789754867554,
      "learning_rate": 0.0001851553540472063,
      "loss": 0.7104,
      "step": 2788
    },
    {
      "epoch": 0.22312,
      "grad_norm": 0.3391502797603607,
      "learning_rate": 0.00018515002000266703,
      "loss": 1.1304,
      "step": 2789
    },
    {
      "epoch": 0.2232,
      "grad_norm": 0.3459549844264984,
      "learning_rate": 0.00018514468595812774,
      "loss": 0.6621,
      "step": 2790
    },
    {
      "epoch": 0.22328,
      "grad_norm": 0.3347040116786957,
      "learning_rate": 0.00018513935191358848,
      "loss": 0.8608,
      "step": 2791
    },
    {
      "epoch": 0.22336,
      "grad_norm": 0.4152931869029999,
      "learning_rate": 0.00018513401786904922,
      "loss": 0.7674,
      "step": 2792
    },
    {
      "epoch": 0.22344,
      "grad_norm": 0.36374759674072266,
      "learning_rate": 0.00018512868382450994,
      "loss": 0.6587,
      "step": 2793
    },
    {
      "epoch": 0.22352,
      "grad_norm": 0.40232375264167786,
      "learning_rate": 0.00018512334977997068,
      "loss": 0.9883,
      "step": 2794
    },
    {
      "epoch": 0.2236,
      "grad_norm": 0.36695554852485657,
      "learning_rate": 0.0001851180157354314,
      "loss": 0.6302,
      "step": 2795
    },
    {
      "epoch": 0.22368,
      "grad_norm": 0.46754440665245056,
      "learning_rate": 0.00018511268169089213,
      "loss": 0.8447,
      "step": 2796
    },
    {
      "epoch": 0.22376,
      "grad_norm": 0.4633704721927643,
      "learning_rate": 0.00018510734764635284,
      "loss": 1.0014,
      "step": 2797
    },
    {
      "epoch": 0.22384,
      "grad_norm": 0.37576135993003845,
      "learning_rate": 0.00018510201360181358,
      "loss": 0.7891,
      "step": 2798
    },
    {
      "epoch": 0.22392,
      "grad_norm": 0.3646031320095062,
      "learning_rate": 0.00018509667955727432,
      "loss": 0.538,
      "step": 2799
    },
    {
      "epoch": 0.224,
      "grad_norm": 0.28648483753204346,
      "learning_rate": 0.00018509134551273503,
      "loss": 0.5225,
      "step": 2800
    },
    {
      "epoch": 0.22408,
      "grad_norm": 0.28914082050323486,
      "learning_rate": 0.00018508601146819577,
      "loss": 0.6127,
      "step": 2801
    },
    {
      "epoch": 0.22416,
      "grad_norm": 0.44563546776771545,
      "learning_rate": 0.0001850806774236565,
      "loss": 1.0378,
      "step": 2802
    },
    {
      "epoch": 0.22424,
      "grad_norm": 0.3680175244808197,
      "learning_rate": 0.00018507534337911723,
      "loss": 0.7821,
      "step": 2803
    },
    {
      "epoch": 0.22432,
      "grad_norm": 0.42360517382621765,
      "learning_rate": 0.00018507000933457794,
      "loss": 1.0828,
      "step": 2804
    },
    {
      "epoch": 0.2244,
      "grad_norm": 0.2905283570289612,
      "learning_rate": 0.00018506467529003868,
      "loss": 1.1807,
      "step": 2805
    },
    {
      "epoch": 0.22448,
      "grad_norm": 0.2516339421272278,
      "learning_rate": 0.00018505934124549942,
      "loss": 0.7409,
      "step": 2806
    },
    {
      "epoch": 0.22456,
      "grad_norm": 0.4017474055290222,
      "learning_rate": 0.00018505400720096013,
      "loss": 1.0161,
      "step": 2807
    },
    {
      "epoch": 0.22464,
      "grad_norm": 0.41921359300613403,
      "learning_rate": 0.00018504867315642087,
      "loss": 0.815,
      "step": 2808
    },
    {
      "epoch": 0.22472,
      "grad_norm": 0.42960235476493835,
      "learning_rate": 0.00018504333911188158,
      "loss": 0.8477,
      "step": 2809
    },
    {
      "epoch": 0.2248,
      "grad_norm": 0.4563048183917999,
      "learning_rate": 0.00018503800506734232,
      "loss": 0.9702,
      "step": 2810
    },
    {
      "epoch": 0.22488,
      "grad_norm": 0.3960815668106079,
      "learning_rate": 0.00018503267102280304,
      "loss": 0.9749,
      "step": 2811
    },
    {
      "epoch": 0.22496,
      "grad_norm": 0.48038721084594727,
      "learning_rate": 0.00018502733697826378,
      "loss": 0.6826,
      "step": 2812
    },
    {
      "epoch": 0.22504,
      "grad_norm": 0.2729700803756714,
      "learning_rate": 0.00018502200293372452,
      "loss": 0.674,
      "step": 2813
    },
    {
      "epoch": 0.22512,
      "grad_norm": 0.3830656409263611,
      "learning_rate": 0.00018501666888918523,
      "loss": 0.8746,
      "step": 2814
    },
    {
      "epoch": 0.2252,
      "grad_norm": 0.33126458525657654,
      "learning_rate": 0.00018501133484464597,
      "loss": 1.1128,
      "step": 2815
    },
    {
      "epoch": 0.22528,
      "grad_norm": 0.371901273727417,
      "learning_rate": 0.00018500600080010668,
      "loss": 0.7489,
      "step": 2816
    },
    {
      "epoch": 0.22536,
      "grad_norm": 0.46264445781707764,
      "learning_rate": 0.00018500066675556742,
      "loss": 0.8987,
      "step": 2817
    },
    {
      "epoch": 0.22544,
      "grad_norm": 0.3625706136226654,
      "learning_rate": 0.00018499533271102813,
      "loss": 0.7581,
      "step": 2818
    },
    {
      "epoch": 0.22552,
      "grad_norm": 0.35658907890319824,
      "learning_rate": 0.00018498999866648887,
      "loss": 0.9521,
      "step": 2819
    },
    {
      "epoch": 0.2256,
      "grad_norm": 0.4261765480041504,
      "learning_rate": 0.00018498466462194961,
      "loss": 0.8541,
      "step": 2820
    },
    {
      "epoch": 0.22568,
      "grad_norm": 0.45653924345970154,
      "learning_rate": 0.00018497933057741033,
      "loss": 0.8431,
      "step": 2821
    },
    {
      "epoch": 0.22576,
      "grad_norm": 0.4182834029197693,
      "learning_rate": 0.00018497399653287107,
      "loss": 0.9983,
      "step": 2822
    },
    {
      "epoch": 0.22584,
      "grad_norm": 0.2981782853603363,
      "learning_rate": 0.00018496866248833178,
      "loss": 0.8716,
      "step": 2823
    },
    {
      "epoch": 0.22592,
      "grad_norm": 0.3830348253250122,
      "learning_rate": 0.00018496332844379252,
      "loss": 0.7393,
      "step": 2824
    },
    {
      "epoch": 0.226,
      "grad_norm": 0.3370184302330017,
      "learning_rate": 0.00018495799439925323,
      "loss": 0.9614,
      "step": 2825
    },
    {
      "epoch": 0.22608,
      "grad_norm": 0.3398837149143219,
      "learning_rate": 0.00018495266035471397,
      "loss": 0.9932,
      "step": 2826
    },
    {
      "epoch": 0.22616,
      "grad_norm": 0.45504629611968994,
      "learning_rate": 0.0001849473263101747,
      "loss": 0.957,
      "step": 2827
    },
    {
      "epoch": 0.22624,
      "grad_norm": 0.4618874490261078,
      "learning_rate": 0.00018494199226563542,
      "loss": 0.9811,
      "step": 2828
    },
    {
      "epoch": 0.22632,
      "grad_norm": 0.47731924057006836,
      "learning_rate": 0.00018493665822109616,
      "loss": 0.8088,
      "step": 2829
    },
    {
      "epoch": 0.2264,
      "grad_norm": 0.30558890104293823,
      "learning_rate": 0.00018493132417655688,
      "loss": 0.4497,
      "step": 2830
    },
    {
      "epoch": 0.22648,
      "grad_norm": 0.33723679184913635,
      "learning_rate": 0.00018492599013201762,
      "loss": 0.7838,
      "step": 2831
    },
    {
      "epoch": 0.22656,
      "grad_norm": 0.3848159611225128,
      "learning_rate": 0.00018492065608747836,
      "loss": 0.8401,
      "step": 2832
    },
    {
      "epoch": 0.22664,
      "grad_norm": 0.3578875958919525,
      "learning_rate": 0.00018491532204293907,
      "loss": 1.0015,
      "step": 2833
    },
    {
      "epoch": 0.22672,
      "grad_norm": 0.3756813704967499,
      "learning_rate": 0.0001849099879983998,
      "loss": 0.6035,
      "step": 2834
    },
    {
      "epoch": 0.2268,
      "grad_norm": 0.3178797960281372,
      "learning_rate": 0.00018490465395386052,
      "loss": 0.6711,
      "step": 2835
    },
    {
      "epoch": 0.22688,
      "grad_norm": 0.4510040879249573,
      "learning_rate": 0.00018489931990932126,
      "loss": 0.4685,
      "step": 2836
    },
    {
      "epoch": 0.22696,
      "grad_norm": 0.5144806504249573,
      "learning_rate": 0.00018489398586478197,
      "loss": 0.971,
      "step": 2837
    },
    {
      "epoch": 0.22704,
      "grad_norm": 0.36941245198249817,
      "learning_rate": 0.00018488865182024271,
      "loss": 1.0769,
      "step": 2838
    },
    {
      "epoch": 0.22712,
      "grad_norm": 0.35280099511146545,
      "learning_rate": 0.00018488331777570345,
      "loss": 0.9964,
      "step": 2839
    },
    {
      "epoch": 0.2272,
      "grad_norm": 0.354162335395813,
      "learning_rate": 0.00018487798373116417,
      "loss": 0.8922,
      "step": 2840
    },
    {
      "epoch": 0.22728,
      "grad_norm": 0.3256154954433441,
      "learning_rate": 0.0001848726496866249,
      "loss": 0.901,
      "step": 2841
    },
    {
      "epoch": 0.22736,
      "grad_norm": 0.3579793870449066,
      "learning_rate": 0.00018486731564208562,
      "loss": 1.1864,
      "step": 2842
    },
    {
      "epoch": 0.22744,
      "grad_norm": 0.44448062777519226,
      "learning_rate": 0.00018486198159754636,
      "loss": 0.5785,
      "step": 2843
    },
    {
      "epoch": 0.22752,
      "grad_norm": 0.371354877948761,
      "learning_rate": 0.00018485664755300707,
      "loss": 0.8949,
      "step": 2844
    },
    {
      "epoch": 0.2276,
      "grad_norm": 0.29469600319862366,
      "learning_rate": 0.0001848513135084678,
      "loss": 0.9657,
      "step": 2845
    },
    {
      "epoch": 0.22768,
      "grad_norm": 0.34760376811027527,
      "learning_rate": 0.00018484597946392855,
      "loss": 0.8534,
      "step": 2846
    },
    {
      "epoch": 0.22776,
      "grad_norm": 0.4048604965209961,
      "learning_rate": 0.00018484064541938926,
      "loss": 0.8672,
      "step": 2847
    },
    {
      "epoch": 0.22784,
      "grad_norm": 0.3673498332500458,
      "learning_rate": 0.00018483531137485,
      "loss": 0.7222,
      "step": 2848
    },
    {
      "epoch": 0.22792,
      "grad_norm": 0.30287203192710876,
      "learning_rate": 0.00018482997733031072,
      "loss": 0.6341,
      "step": 2849
    },
    {
      "epoch": 0.228,
      "grad_norm": 0.3938645124435425,
      "learning_rate": 0.00018482464328577146,
      "loss": 0.7639,
      "step": 2850
    },
    {
      "epoch": 0.22808,
      "grad_norm": 0.332778662443161,
      "learning_rate": 0.00018481930924123217,
      "loss": 0.5604,
      "step": 2851
    },
    {
      "epoch": 0.22816,
      "grad_norm": 0.291119247674942,
      "learning_rate": 0.0001848139751966929,
      "loss": 0.8823,
      "step": 2852
    },
    {
      "epoch": 0.22824,
      "grad_norm": 0.3546055257320404,
      "learning_rate": 0.00018480864115215365,
      "loss": 0.7973,
      "step": 2853
    },
    {
      "epoch": 0.22832,
      "grad_norm": 0.3424427807331085,
      "learning_rate": 0.00018480330710761436,
      "loss": 1.2372,
      "step": 2854
    },
    {
      "epoch": 0.2284,
      "grad_norm": 0.326478511095047,
      "learning_rate": 0.0001847979730630751,
      "loss": 1.0268,
      "step": 2855
    },
    {
      "epoch": 0.22848,
      "grad_norm": 0.373977929353714,
      "learning_rate": 0.00018479263901853582,
      "loss": 0.9521,
      "step": 2856
    },
    {
      "epoch": 0.22856,
      "grad_norm": 0.3096753656864166,
      "learning_rate": 0.00018478730497399655,
      "loss": 0.5019,
      "step": 2857
    },
    {
      "epoch": 0.22864,
      "grad_norm": 0.38050615787506104,
      "learning_rate": 0.00018478197092945727,
      "loss": 0.4883,
      "step": 2858
    },
    {
      "epoch": 0.22872,
      "grad_norm": 0.3639630377292633,
      "learning_rate": 0.000184776636884918,
      "loss": 1.0016,
      "step": 2859
    },
    {
      "epoch": 0.2288,
      "grad_norm": 0.33880066871643066,
      "learning_rate": 0.00018477130284037875,
      "loss": 0.8238,
      "step": 2860
    },
    {
      "epoch": 0.22888,
      "grad_norm": 0.3346184492111206,
      "learning_rate": 0.00018476596879583946,
      "loss": 0.8367,
      "step": 2861
    },
    {
      "epoch": 0.22896,
      "grad_norm": 0.3524725139141083,
      "learning_rate": 0.0001847606347513002,
      "loss": 0.8003,
      "step": 2862
    },
    {
      "epoch": 0.22904,
      "grad_norm": 0.4449487328529358,
      "learning_rate": 0.0001847553007067609,
      "loss": 0.6873,
      "step": 2863
    },
    {
      "epoch": 0.22912,
      "grad_norm": 0.43164512515068054,
      "learning_rate": 0.00018474996666222165,
      "loss": 1.0657,
      "step": 2864
    },
    {
      "epoch": 0.2292,
      "grad_norm": 0.371642529964447,
      "learning_rate": 0.00018474463261768237,
      "loss": 0.8374,
      "step": 2865
    },
    {
      "epoch": 0.22928,
      "grad_norm": 0.4103771150112152,
      "learning_rate": 0.0001847392985731431,
      "loss": 1.3095,
      "step": 2866
    },
    {
      "epoch": 0.22936,
      "grad_norm": 0.3268072307109833,
      "learning_rate": 0.00018473396452860382,
      "loss": 0.7547,
      "step": 2867
    },
    {
      "epoch": 0.22944,
      "grad_norm": 0.44302091002464294,
      "learning_rate": 0.00018472863048406456,
      "loss": 0.974,
      "step": 2868
    },
    {
      "epoch": 0.22952,
      "grad_norm": 0.46934759616851807,
      "learning_rate": 0.0001847232964395253,
      "loss": 0.9568,
      "step": 2869
    },
    {
      "epoch": 0.2296,
      "grad_norm": 0.3219108283519745,
      "learning_rate": 0.000184717962394986,
      "loss": 0.9652,
      "step": 2870
    },
    {
      "epoch": 0.22968,
      "grad_norm": 0.595262348651886,
      "learning_rate": 0.00018471262835044675,
      "loss": 0.8914,
      "step": 2871
    },
    {
      "epoch": 0.22976,
      "grad_norm": 0.4837474226951599,
      "learning_rate": 0.00018470729430590746,
      "loss": 1.2224,
      "step": 2872
    },
    {
      "epoch": 0.22984,
      "grad_norm": 0.31753408908843994,
      "learning_rate": 0.0001847019602613682,
      "loss": 0.6267,
      "step": 2873
    },
    {
      "epoch": 0.22992,
      "grad_norm": 0.3745806813240051,
      "learning_rate": 0.00018469662621682892,
      "loss": 0.8923,
      "step": 2874
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.5788217782974243,
      "learning_rate": 0.00018469129217228966,
      "loss": 1.1522,
      "step": 2875
    },
    {
      "epoch": 0.23008,
      "grad_norm": 0.3748234212398529,
      "learning_rate": 0.00018468595812775037,
      "loss": 0.8574,
      "step": 2876
    },
    {
      "epoch": 0.23016,
      "grad_norm": 0.4369124472141266,
      "learning_rate": 0.0001846806240832111,
      "loss": 0.8203,
      "step": 2877
    },
    {
      "epoch": 0.23024,
      "grad_norm": 0.2575186491012573,
      "learning_rate": 0.00018467529003867182,
      "loss": 0.7063,
      "step": 2878
    },
    {
      "epoch": 0.23032,
      "grad_norm": 0.3453243374824524,
      "learning_rate": 0.00018466995599413256,
      "loss": 0.7631,
      "step": 2879
    },
    {
      "epoch": 0.2304,
      "grad_norm": 0.32757192850112915,
      "learning_rate": 0.00018466462194959327,
      "loss": 1.2284,
      "step": 2880
    },
    {
      "epoch": 0.23048,
      "grad_norm": 0.42556703090667725,
      "learning_rate": 0.000184659287905054,
      "loss": 0.7805,
      "step": 2881
    },
    {
      "epoch": 0.23056,
      "grad_norm": 0.6362683773040771,
      "learning_rate": 0.00018465395386051473,
      "loss": 1.2733,
      "step": 2882
    },
    {
      "epoch": 0.23064,
      "grad_norm": 0.4073600471019745,
      "learning_rate": 0.00018464861981597547,
      "loss": 0.8855,
      "step": 2883
    },
    {
      "epoch": 0.23072,
      "grad_norm": 0.28967738151550293,
      "learning_rate": 0.00018464328577143618,
      "loss": 0.7703,
      "step": 2884
    },
    {
      "epoch": 0.2308,
      "grad_norm": 0.31592968106269836,
      "learning_rate": 0.00018463795172689692,
      "loss": 1.0665,
      "step": 2885
    },
    {
      "epoch": 0.23088,
      "grad_norm": 0.3214568495750427,
      "learning_rate": 0.00018463261768235766,
      "loss": 1.2054,
      "step": 2886
    },
    {
      "epoch": 0.23096,
      "grad_norm": 0.4409612715244293,
      "learning_rate": 0.00018462728363781837,
      "loss": 0.9774,
      "step": 2887
    },
    {
      "epoch": 0.23104,
      "grad_norm": 0.3708629012107849,
      "learning_rate": 0.0001846219495932791,
      "loss": 0.7599,
      "step": 2888
    },
    {
      "epoch": 0.23112,
      "grad_norm": 0.49571457505226135,
      "learning_rate": 0.00018461661554873982,
      "loss": 0.9398,
      "step": 2889
    },
    {
      "epoch": 0.2312,
      "grad_norm": 0.3643234968185425,
      "learning_rate": 0.00018461128150420056,
      "loss": 0.8561,
      "step": 2890
    },
    {
      "epoch": 0.23128,
      "grad_norm": 0.37378156185150146,
      "learning_rate": 0.00018460594745966128,
      "loss": 1.0097,
      "step": 2891
    },
    {
      "epoch": 0.23136,
      "grad_norm": 0.35507115721702576,
      "learning_rate": 0.00018460061341512202,
      "loss": 0.6241,
      "step": 2892
    },
    {
      "epoch": 0.23144,
      "grad_norm": 0.37438279390335083,
      "learning_rate": 0.00018459527937058276,
      "loss": 1.1189,
      "step": 2893
    },
    {
      "epoch": 0.23152,
      "grad_norm": 0.28132742643356323,
      "learning_rate": 0.00018458994532604347,
      "loss": 0.6261,
      "step": 2894
    },
    {
      "epoch": 0.2316,
      "grad_norm": 0.3656398057937622,
      "learning_rate": 0.0001845846112815042,
      "loss": 0.6093,
      "step": 2895
    },
    {
      "epoch": 0.23168,
      "grad_norm": 0.34507840871810913,
      "learning_rate": 0.00018457927723696492,
      "loss": 0.7743,
      "step": 2896
    },
    {
      "epoch": 0.23176,
      "grad_norm": 0.32157307863235474,
      "learning_rate": 0.00018457394319242566,
      "loss": 0.8551,
      "step": 2897
    },
    {
      "epoch": 0.23184,
      "grad_norm": 0.27489563822746277,
      "learning_rate": 0.00018456860914788637,
      "loss": 0.5605,
      "step": 2898
    },
    {
      "epoch": 0.23192,
      "grad_norm": 0.383437842130661,
      "learning_rate": 0.00018456327510334711,
      "loss": 1.2582,
      "step": 2899
    },
    {
      "epoch": 0.232,
      "grad_norm": 0.27356019616127014,
      "learning_rate": 0.00018455794105880785,
      "loss": 0.5018,
      "step": 2900
    },
    {
      "epoch": 0.23208,
      "grad_norm": 0.3141789138317108,
      "learning_rate": 0.00018455260701426857,
      "loss": 0.7805,
      "step": 2901
    },
    {
      "epoch": 0.23216,
      "grad_norm": 0.27555516362190247,
      "learning_rate": 0.0001845472729697293,
      "loss": 0.4748,
      "step": 2902
    },
    {
      "epoch": 0.23224,
      "grad_norm": 0.4905822277069092,
      "learning_rate": 0.00018454193892519002,
      "loss": 1.0724,
      "step": 2903
    },
    {
      "epoch": 0.23232,
      "grad_norm": 0.37467506527900696,
      "learning_rate": 0.00018453660488065076,
      "loss": 0.914,
      "step": 2904
    },
    {
      "epoch": 0.2324,
      "grad_norm": 0.3225557804107666,
      "learning_rate": 0.00018453127083611147,
      "loss": 0.9097,
      "step": 2905
    },
    {
      "epoch": 0.23248,
      "grad_norm": 0.4219561815261841,
      "learning_rate": 0.0001845259367915722,
      "loss": 0.6937,
      "step": 2906
    },
    {
      "epoch": 0.23256,
      "grad_norm": 0.4355461895465851,
      "learning_rate": 0.00018452060274703295,
      "loss": 1.029,
      "step": 2907
    },
    {
      "epoch": 0.23264,
      "grad_norm": 0.37349382042884827,
      "learning_rate": 0.00018451526870249366,
      "loss": 0.6889,
      "step": 2908
    },
    {
      "epoch": 0.23272,
      "grad_norm": 0.39852529764175415,
      "learning_rate": 0.0001845099346579544,
      "loss": 0.9505,
      "step": 2909
    },
    {
      "epoch": 0.2328,
      "grad_norm": 0.4029249846935272,
      "learning_rate": 0.00018450460061341512,
      "loss": 0.7821,
      "step": 2910
    },
    {
      "epoch": 0.23288,
      "grad_norm": 0.3825889527797699,
      "learning_rate": 0.00018449926656887586,
      "loss": 0.8785,
      "step": 2911
    },
    {
      "epoch": 0.23296,
      "grad_norm": 0.5121810436248779,
      "learning_rate": 0.00018449393252433657,
      "loss": 0.9662,
      "step": 2912
    },
    {
      "epoch": 0.23304,
      "grad_norm": 0.3931678831577301,
      "learning_rate": 0.0001844885984797973,
      "loss": 0.5671,
      "step": 2913
    },
    {
      "epoch": 0.23312,
      "grad_norm": 0.29358217120170593,
      "learning_rate": 0.00018448326443525805,
      "loss": 1.032,
      "step": 2914
    },
    {
      "epoch": 0.2332,
      "grad_norm": 0.3485839366912842,
      "learning_rate": 0.00018447793039071876,
      "loss": 0.6118,
      "step": 2915
    },
    {
      "epoch": 0.23328,
      "grad_norm": 0.31696102023124695,
      "learning_rate": 0.0001844725963461795,
      "loss": 0.8523,
      "step": 2916
    },
    {
      "epoch": 0.23336,
      "grad_norm": 0.36776024103164673,
      "learning_rate": 0.00018446726230164021,
      "loss": 0.9517,
      "step": 2917
    },
    {
      "epoch": 0.23344,
      "grad_norm": 0.44657444953918457,
      "learning_rate": 0.00018446192825710095,
      "loss": 0.874,
      "step": 2918
    },
    {
      "epoch": 0.23352,
      "grad_norm": 0.28557488322257996,
      "learning_rate": 0.00018445659421256167,
      "loss": 0.6777,
      "step": 2919
    },
    {
      "epoch": 0.2336,
      "grad_norm": 0.27433574199676514,
      "learning_rate": 0.0001844512601680224,
      "loss": 1.0422,
      "step": 2920
    },
    {
      "epoch": 0.23368,
      "grad_norm": 0.37376564741134644,
      "learning_rate": 0.00018444592612348315,
      "loss": 0.8717,
      "step": 2921
    },
    {
      "epoch": 0.23376,
      "grad_norm": 0.4251419007778168,
      "learning_rate": 0.00018444059207894386,
      "loss": 0.8992,
      "step": 2922
    },
    {
      "epoch": 0.23384,
      "grad_norm": 0.4674609303474426,
      "learning_rate": 0.0001844352580344046,
      "loss": 0.7853,
      "step": 2923
    },
    {
      "epoch": 0.23392,
      "grad_norm": 0.30649152398109436,
      "learning_rate": 0.0001844299239898653,
      "loss": 0.5786,
      "step": 2924
    },
    {
      "epoch": 0.234,
      "grad_norm": 0.40081319212913513,
      "learning_rate": 0.00018442458994532605,
      "loss": 0.8862,
      "step": 2925
    },
    {
      "epoch": 0.23408,
      "grad_norm": 0.3401438593864441,
      "learning_rate": 0.0001844192559007868,
      "loss": 0.8175,
      "step": 2926
    },
    {
      "epoch": 0.23416,
      "grad_norm": 0.3014959990978241,
      "learning_rate": 0.0001844139218562475,
      "loss": 0.6736,
      "step": 2927
    },
    {
      "epoch": 0.23424,
      "grad_norm": 0.5082734227180481,
      "learning_rate": 0.00018440858781170824,
      "loss": 0.9302,
      "step": 2928
    },
    {
      "epoch": 0.23432,
      "grad_norm": 0.2861156165599823,
      "learning_rate": 0.00018440325376716896,
      "loss": 0.7051,
      "step": 2929
    },
    {
      "epoch": 0.2344,
      "grad_norm": 0.3016742169857025,
      "learning_rate": 0.0001843979197226297,
      "loss": 0.5643,
      "step": 2930
    },
    {
      "epoch": 0.23448,
      "grad_norm": 0.38044053316116333,
      "learning_rate": 0.0001843925856780904,
      "loss": 0.8658,
      "step": 2931
    },
    {
      "epoch": 0.23456,
      "grad_norm": 0.352390855550766,
      "learning_rate": 0.00018438725163355115,
      "loss": 1.1625,
      "step": 2932
    },
    {
      "epoch": 0.23464,
      "grad_norm": 0.35091349482536316,
      "learning_rate": 0.0001843819175890119,
      "loss": 0.6792,
      "step": 2933
    },
    {
      "epoch": 0.23472,
      "grad_norm": 0.400483101606369,
      "learning_rate": 0.0001843765835444726,
      "loss": 0.8815,
      "step": 2934
    },
    {
      "epoch": 0.2348,
      "grad_norm": 0.37487268447875977,
      "learning_rate": 0.00018437124949993334,
      "loss": 0.9711,
      "step": 2935
    },
    {
      "epoch": 0.23488,
      "grad_norm": 0.4469069838523865,
      "learning_rate": 0.00018436591545539405,
      "loss": 0.8081,
      "step": 2936
    },
    {
      "epoch": 0.23496,
      "grad_norm": 0.41247624158859253,
      "learning_rate": 0.0001843605814108548,
      "loss": 0.9415,
      "step": 2937
    },
    {
      "epoch": 0.23504,
      "grad_norm": 0.4120856523513794,
      "learning_rate": 0.0001843552473663155,
      "loss": 1.1252,
      "step": 2938
    },
    {
      "epoch": 0.23512,
      "grad_norm": 0.4051932692527771,
      "learning_rate": 0.00018434991332177625,
      "loss": 1.0864,
      "step": 2939
    },
    {
      "epoch": 0.2352,
      "grad_norm": 0.37400659918785095,
      "learning_rate": 0.000184344579277237,
      "loss": 0.8616,
      "step": 2940
    },
    {
      "epoch": 0.23528,
      "grad_norm": 0.4692017734050751,
      "learning_rate": 0.0001843392452326977,
      "loss": 0.787,
      "step": 2941
    },
    {
      "epoch": 0.23536,
      "grad_norm": 0.4006150960922241,
      "learning_rate": 0.00018433391118815844,
      "loss": 0.909,
      "step": 2942
    },
    {
      "epoch": 0.23544,
      "grad_norm": 0.4964170455932617,
      "learning_rate": 0.00018432857714361915,
      "loss": 1.1163,
      "step": 2943
    },
    {
      "epoch": 0.23552,
      "grad_norm": 0.4511190354824066,
      "learning_rate": 0.0001843232430990799,
      "loss": 1.2733,
      "step": 2944
    },
    {
      "epoch": 0.2356,
      "grad_norm": 0.29346781969070435,
      "learning_rate": 0.0001843179090545406,
      "loss": 0.9439,
      "step": 2945
    },
    {
      "epoch": 0.23568,
      "grad_norm": 0.4097600281238556,
      "learning_rate": 0.00018431257501000134,
      "loss": 0.969,
      "step": 2946
    },
    {
      "epoch": 0.23576,
      "grad_norm": 0.43671715259552,
      "learning_rate": 0.00018430724096546208,
      "loss": 0.7945,
      "step": 2947
    },
    {
      "epoch": 0.23584,
      "grad_norm": 0.36985933780670166,
      "learning_rate": 0.0001843019069209228,
      "loss": 0.7475,
      "step": 2948
    },
    {
      "epoch": 0.23592,
      "grad_norm": 0.3444501459598541,
      "learning_rate": 0.00018429657287638354,
      "loss": 1.0628,
      "step": 2949
    },
    {
      "epoch": 0.236,
      "grad_norm": 0.32951536774635315,
      "learning_rate": 0.00018429123883184425,
      "loss": 0.5885,
      "step": 2950
    },
    {
      "epoch": 0.23608,
      "grad_norm": 0.3181474506855011,
      "learning_rate": 0.000184285904787305,
      "loss": 0.7114,
      "step": 2951
    },
    {
      "epoch": 0.23616,
      "grad_norm": 0.3552129566669464,
      "learning_rate": 0.0001842805707427657,
      "loss": 0.8062,
      "step": 2952
    },
    {
      "epoch": 0.23624,
      "grad_norm": 0.33749914169311523,
      "learning_rate": 0.00018427523669822644,
      "loss": 1.1158,
      "step": 2953
    },
    {
      "epoch": 0.23632,
      "grad_norm": 0.3260643780231476,
      "learning_rate": 0.00018426990265368718,
      "loss": 0.8164,
      "step": 2954
    },
    {
      "epoch": 0.2364,
      "grad_norm": 0.3536050617694855,
      "learning_rate": 0.0001842645686091479,
      "loss": 0.9375,
      "step": 2955
    },
    {
      "epoch": 0.23648,
      "grad_norm": 0.39418691396713257,
      "learning_rate": 0.00018425923456460863,
      "loss": 0.7472,
      "step": 2956
    },
    {
      "epoch": 0.23656,
      "grad_norm": 0.3029802739620209,
      "learning_rate": 0.00018425390052006935,
      "loss": 0.6162,
      "step": 2957
    },
    {
      "epoch": 0.23664,
      "grad_norm": 0.35969623923301697,
      "learning_rate": 0.0001842485664755301,
      "loss": 0.8997,
      "step": 2958
    },
    {
      "epoch": 0.23672,
      "grad_norm": 0.39365991950035095,
      "learning_rate": 0.0001842432324309908,
      "loss": 1.0295,
      "step": 2959
    },
    {
      "epoch": 0.2368,
      "grad_norm": 0.41595444083213806,
      "learning_rate": 0.00018423789838645154,
      "loss": 0.7793,
      "step": 2960
    },
    {
      "epoch": 0.23688,
      "grad_norm": 0.4426303505897522,
      "learning_rate": 0.00018423256434191228,
      "loss": 0.6819,
      "step": 2961
    },
    {
      "epoch": 0.23696,
      "grad_norm": 0.4097900688648224,
      "learning_rate": 0.000184227230297373,
      "loss": 0.8,
      "step": 2962
    },
    {
      "epoch": 0.23704,
      "grad_norm": 0.45994383096694946,
      "learning_rate": 0.00018422189625283373,
      "loss": 0.6981,
      "step": 2963
    },
    {
      "epoch": 0.23712,
      "grad_norm": 0.28848108649253845,
      "learning_rate": 0.00018421656220829444,
      "loss": 0.9755,
      "step": 2964
    },
    {
      "epoch": 0.2372,
      "grad_norm": 0.447552889585495,
      "learning_rate": 0.00018421122816375518,
      "loss": 0.8829,
      "step": 2965
    },
    {
      "epoch": 0.23728,
      "grad_norm": 0.41819122433662415,
      "learning_rate": 0.0001842058941192159,
      "loss": 0.9402,
      "step": 2966
    },
    {
      "epoch": 0.23736,
      "grad_norm": 0.4493754804134369,
      "learning_rate": 0.00018420056007467664,
      "loss": 0.8129,
      "step": 2967
    },
    {
      "epoch": 0.23744,
      "grad_norm": 0.2887533903121948,
      "learning_rate": 0.00018419522603013738,
      "loss": 0.7749,
      "step": 2968
    },
    {
      "epoch": 0.23752,
      "grad_norm": 0.37775710225105286,
      "learning_rate": 0.0001841898919855981,
      "loss": 0.7736,
      "step": 2969
    },
    {
      "epoch": 0.2376,
      "grad_norm": 0.38938412070274353,
      "learning_rate": 0.00018418455794105883,
      "loss": 1.0446,
      "step": 2970
    },
    {
      "epoch": 0.23768,
      "grad_norm": 0.27773037552833557,
      "learning_rate": 0.00018417922389651954,
      "loss": 0.5669,
      "step": 2971
    },
    {
      "epoch": 0.23776,
      "grad_norm": 0.35197293758392334,
      "learning_rate": 0.00018417388985198028,
      "loss": 0.8542,
      "step": 2972
    },
    {
      "epoch": 0.23784,
      "grad_norm": 0.3129669427871704,
      "learning_rate": 0.000184168555807441,
      "loss": 0.7708,
      "step": 2973
    },
    {
      "epoch": 0.23792,
      "grad_norm": 0.3399389684200287,
      "learning_rate": 0.00018416322176290173,
      "loss": 0.7566,
      "step": 2974
    },
    {
      "epoch": 0.238,
      "grad_norm": 0.4490354359149933,
      "learning_rate": 0.00018415788771836247,
      "loss": 1.0382,
      "step": 2975
    },
    {
      "epoch": 0.23808,
      "grad_norm": 0.3208099901676178,
      "learning_rate": 0.0001841525536738232,
      "loss": 0.6988,
      "step": 2976
    },
    {
      "epoch": 0.23816,
      "grad_norm": 0.49815115332603455,
      "learning_rate": 0.00018414721962928393,
      "loss": 1.1076,
      "step": 2977
    },
    {
      "epoch": 0.23824,
      "grad_norm": 0.24659687280654907,
      "learning_rate": 0.00018414188558474464,
      "loss": 0.5669,
      "step": 2978
    },
    {
      "epoch": 0.23832,
      "grad_norm": 0.32847195863723755,
      "learning_rate": 0.00018413655154020538,
      "loss": 1.1514,
      "step": 2979
    },
    {
      "epoch": 0.2384,
      "grad_norm": 0.35148924589157104,
      "learning_rate": 0.00018413121749566612,
      "loss": 0.7585,
      "step": 2980
    },
    {
      "epoch": 0.23848,
      "grad_norm": 0.38963237404823303,
      "learning_rate": 0.00018412588345112683,
      "loss": 0.776,
      "step": 2981
    },
    {
      "epoch": 0.23856,
      "grad_norm": 0.463874876499176,
      "learning_rate": 0.00018412054940658757,
      "loss": 1.1214,
      "step": 2982
    },
    {
      "epoch": 0.23864,
      "grad_norm": 0.42190611362457275,
      "learning_rate": 0.00018411521536204829,
      "loss": 1.0793,
      "step": 2983
    },
    {
      "epoch": 0.23872,
      "grad_norm": 0.44848188757896423,
      "learning_rate": 0.00018410988131750902,
      "loss": 1.3603,
      "step": 2984
    },
    {
      "epoch": 0.2388,
      "grad_norm": 0.33404096961021423,
      "learning_rate": 0.00018410454727296974,
      "loss": 1.3338,
      "step": 2985
    },
    {
      "epoch": 0.23888,
      "grad_norm": 0.31857773661613464,
      "learning_rate": 0.00018409921322843048,
      "loss": 0.9266,
      "step": 2986
    },
    {
      "epoch": 0.23896,
      "grad_norm": 0.3376084268093109,
      "learning_rate": 0.00018409387918389122,
      "loss": 0.8977,
      "step": 2987
    },
    {
      "epoch": 0.23904,
      "grad_norm": 0.327819287776947,
      "learning_rate": 0.00018408854513935193,
      "loss": 0.6186,
      "step": 2988
    },
    {
      "epoch": 0.23912,
      "grad_norm": 0.4444105923175812,
      "learning_rate": 0.00018408321109481267,
      "loss": 0.8433,
      "step": 2989
    },
    {
      "epoch": 0.2392,
      "grad_norm": 0.46479931473731995,
      "learning_rate": 0.00018407787705027338,
      "loss": 0.6164,
      "step": 2990
    },
    {
      "epoch": 0.23928,
      "grad_norm": 0.3288528025150299,
      "learning_rate": 0.00018407254300573412,
      "loss": 1.0808,
      "step": 2991
    },
    {
      "epoch": 0.23936,
      "grad_norm": 0.364033967256546,
      "learning_rate": 0.00018406720896119484,
      "loss": 0.6602,
      "step": 2992
    },
    {
      "epoch": 0.23944,
      "grad_norm": 0.3563976585865021,
      "learning_rate": 0.00018406187491665558,
      "loss": 0.6875,
      "step": 2993
    },
    {
      "epoch": 0.23952,
      "grad_norm": 0.4202221632003784,
      "learning_rate": 0.0001840565408721163,
      "loss": 0.6954,
      "step": 2994
    },
    {
      "epoch": 0.2396,
      "grad_norm": 0.27072155475616455,
      "learning_rate": 0.00018405120682757703,
      "loss": 0.9613,
      "step": 2995
    },
    {
      "epoch": 0.23968,
      "grad_norm": 0.35100165009498596,
      "learning_rate": 0.00018404587278303774,
      "loss": 0.7357,
      "step": 2996
    },
    {
      "epoch": 0.23976,
      "grad_norm": 0.3804149925708771,
      "learning_rate": 0.00018404053873849848,
      "loss": 0.8653,
      "step": 2997
    },
    {
      "epoch": 0.23984,
      "grad_norm": 0.4240065813064575,
      "learning_rate": 0.00018403520469395922,
      "loss": 1.122,
      "step": 2998
    },
    {
      "epoch": 0.23992,
      "grad_norm": 0.3658665716648102,
      "learning_rate": 0.00018402987064941993,
      "loss": 0.9033,
      "step": 2999
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.43510663509368896,
      "learning_rate": 0.00018402453660488067,
      "loss": 0.8528,
      "step": 3000
    },
    {
      "epoch": 0.24008,
      "grad_norm": 0.32861560583114624,
      "learning_rate": 0.00018401920256034139,
      "loss": 0.7772,
      "step": 3001
    },
    {
      "epoch": 0.24016,
      "grad_norm": 0.4225161075592041,
      "learning_rate": 0.00018401386851580213,
      "loss": 0.8557,
      "step": 3002
    },
    {
      "epoch": 0.24024,
      "grad_norm": 0.3537168502807617,
      "learning_rate": 0.00018400853447126284,
      "loss": 0.8237,
      "step": 3003
    },
    {
      "epoch": 0.24032,
      "grad_norm": 0.3824465870857239,
      "learning_rate": 0.00018400320042672358,
      "loss": 0.9958,
      "step": 3004
    },
    {
      "epoch": 0.2404,
      "grad_norm": 0.37313783168792725,
      "learning_rate": 0.0001839978663821843,
      "loss": 0.6331,
      "step": 3005
    },
    {
      "epoch": 0.24048,
      "grad_norm": 0.3892212510108948,
      "learning_rate": 0.00018399253233764503,
      "loss": 0.762,
      "step": 3006
    },
    {
      "epoch": 0.24056,
      "grad_norm": 0.36512696743011475,
      "learning_rate": 0.00018398719829310574,
      "loss": 0.6354,
      "step": 3007
    },
    {
      "epoch": 0.24064,
      "grad_norm": 0.38514944911003113,
      "learning_rate": 0.00018398186424856648,
      "loss": 0.981,
      "step": 3008
    },
    {
      "epoch": 0.24072,
      "grad_norm": 0.31444069743156433,
      "learning_rate": 0.0001839765302040272,
      "loss": 0.662,
      "step": 3009
    },
    {
      "epoch": 0.2408,
      "grad_norm": 0.3754315674304962,
      "learning_rate": 0.00018397119615948794,
      "loss": 0.7038,
      "step": 3010
    },
    {
      "epoch": 0.24088,
      "grad_norm": 0.3681086599826813,
      "learning_rate": 0.00018396586211494865,
      "loss": 0.892,
      "step": 3011
    },
    {
      "epoch": 0.24096,
      "grad_norm": 0.46016207337379456,
      "learning_rate": 0.0001839605280704094,
      "loss": 0.6382,
      "step": 3012
    },
    {
      "epoch": 0.24104,
      "grad_norm": 0.3772793412208557,
      "learning_rate": 0.0001839551940258701,
      "loss": 0.8395,
      "step": 3013
    },
    {
      "epoch": 0.24112,
      "grad_norm": 0.42608723044395447,
      "learning_rate": 0.00018394985998133084,
      "loss": 1.0107,
      "step": 3014
    },
    {
      "epoch": 0.2412,
      "grad_norm": 0.3630025386810303,
      "learning_rate": 0.00018394452593679158,
      "loss": 0.8888,
      "step": 3015
    },
    {
      "epoch": 0.24128,
      "grad_norm": 0.35749271512031555,
      "learning_rate": 0.0001839391918922523,
      "loss": 0.5793,
      "step": 3016
    },
    {
      "epoch": 0.24136,
      "grad_norm": 0.3009345531463623,
      "learning_rate": 0.00018393385784771303,
      "loss": 0.518,
      "step": 3017
    },
    {
      "epoch": 0.24144,
      "grad_norm": 0.3660731315612793,
      "learning_rate": 0.00018392852380317375,
      "loss": 1.139,
      "step": 3018
    },
    {
      "epoch": 0.24152,
      "grad_norm": 0.4834160804748535,
      "learning_rate": 0.00018392318975863449,
      "loss": 1.17,
      "step": 3019
    },
    {
      "epoch": 0.2416,
      "grad_norm": 0.47395139932632446,
      "learning_rate": 0.0001839178557140952,
      "loss": 0.8048,
      "step": 3020
    },
    {
      "epoch": 0.24168,
      "grad_norm": 0.4981497824192047,
      "learning_rate": 0.00018391252166955594,
      "loss": 0.9565,
      "step": 3021
    },
    {
      "epoch": 0.24176,
      "grad_norm": 0.3775985836982727,
      "learning_rate": 0.00018390718762501668,
      "loss": 0.8216,
      "step": 3022
    },
    {
      "epoch": 0.24184,
      "grad_norm": 0.39724427461624146,
      "learning_rate": 0.0001839018535804774,
      "loss": 0.8292,
      "step": 3023
    },
    {
      "epoch": 0.24192,
      "grad_norm": 0.3935433030128479,
      "learning_rate": 0.00018389651953593813,
      "loss": 1.0964,
      "step": 3024
    },
    {
      "epoch": 0.242,
      "grad_norm": 0.38426533341407776,
      "learning_rate": 0.00018389118549139884,
      "loss": 0.7502,
      "step": 3025
    },
    {
      "epoch": 0.24208,
      "grad_norm": 0.31021884083747864,
      "learning_rate": 0.00018388585144685958,
      "loss": 0.7796,
      "step": 3026
    },
    {
      "epoch": 0.24216,
      "grad_norm": 0.3602633774280548,
      "learning_rate": 0.00018388051740232032,
      "loss": 1.009,
      "step": 3027
    },
    {
      "epoch": 0.24224,
      "grad_norm": 0.3967696726322174,
      "learning_rate": 0.00018387518335778104,
      "loss": 0.6993,
      "step": 3028
    },
    {
      "epoch": 0.24232,
      "grad_norm": 0.4401779770851135,
      "learning_rate": 0.00018386984931324178,
      "loss": 0.8088,
      "step": 3029
    },
    {
      "epoch": 0.2424,
      "grad_norm": 0.35448381304740906,
      "learning_rate": 0.0001838645152687025,
      "loss": 1.0308,
      "step": 3030
    },
    {
      "epoch": 0.24248,
      "grad_norm": 0.2907843589782715,
      "learning_rate": 0.00018385918122416323,
      "loss": 0.5853,
      "step": 3031
    },
    {
      "epoch": 0.24256,
      "grad_norm": 0.4664362370967865,
      "learning_rate": 0.00018385384717962394,
      "loss": 0.7656,
      "step": 3032
    },
    {
      "epoch": 0.24264,
      "grad_norm": 0.29932063817977905,
      "learning_rate": 0.00018384851313508468,
      "loss": 1.1255,
      "step": 3033
    },
    {
      "epoch": 0.24272,
      "grad_norm": 0.4185304343700409,
      "learning_rate": 0.00018384317909054542,
      "loss": 0.8515,
      "step": 3034
    },
    {
      "epoch": 0.2428,
      "grad_norm": 0.3924599885940552,
      "learning_rate": 0.00018383784504600613,
      "loss": 0.8496,
      "step": 3035
    },
    {
      "epoch": 0.24288,
      "grad_norm": 0.3361336886882782,
      "learning_rate": 0.00018383251100146687,
      "loss": 0.7495,
      "step": 3036
    },
    {
      "epoch": 0.24296,
      "grad_norm": 0.526584804058075,
      "learning_rate": 0.00018382717695692759,
      "loss": 1.057,
      "step": 3037
    },
    {
      "epoch": 0.24304,
      "grad_norm": 0.4023538827896118,
      "learning_rate": 0.00018382184291238833,
      "loss": 0.6956,
      "step": 3038
    },
    {
      "epoch": 0.24312,
      "grad_norm": 0.3327654302120209,
      "learning_rate": 0.00018381650886784904,
      "loss": 0.5743,
      "step": 3039
    },
    {
      "epoch": 0.2432,
      "grad_norm": 0.3943735361099243,
      "learning_rate": 0.00018381117482330978,
      "loss": 0.6904,
      "step": 3040
    },
    {
      "epoch": 0.24328,
      "grad_norm": 0.3624627888202667,
      "learning_rate": 0.00018380584077877052,
      "loss": 0.7764,
      "step": 3041
    },
    {
      "epoch": 0.24336,
      "grad_norm": 0.44440630078315735,
      "learning_rate": 0.00018380050673423123,
      "loss": 0.8457,
      "step": 3042
    },
    {
      "epoch": 0.24344,
      "grad_norm": 0.34388285875320435,
      "learning_rate": 0.00018379517268969197,
      "loss": 1.059,
      "step": 3043
    },
    {
      "epoch": 0.24352,
      "grad_norm": 0.42962852120399475,
      "learning_rate": 0.00018378983864515268,
      "loss": 1.2802,
      "step": 3044
    },
    {
      "epoch": 0.2436,
      "grad_norm": 0.32233908772468567,
      "learning_rate": 0.00018378450460061342,
      "loss": 0.7719,
      "step": 3045
    },
    {
      "epoch": 0.24368,
      "grad_norm": 0.4157254099845886,
      "learning_rate": 0.00018377917055607414,
      "loss": 0.6186,
      "step": 3046
    },
    {
      "epoch": 0.24376,
      "grad_norm": 0.35154661536216736,
      "learning_rate": 0.00018377383651153488,
      "loss": 0.6698,
      "step": 3047
    },
    {
      "epoch": 0.24384,
      "grad_norm": 0.3683234453201294,
      "learning_rate": 0.00018376850246699562,
      "loss": 0.9459,
      "step": 3048
    },
    {
      "epoch": 0.24392,
      "grad_norm": 0.35164687037467957,
      "learning_rate": 0.00018376316842245633,
      "loss": 0.7815,
      "step": 3049
    },
    {
      "epoch": 0.244,
      "grad_norm": 0.41730979084968567,
      "learning_rate": 0.00018375783437791707,
      "loss": 0.9776,
      "step": 3050
    },
    {
      "epoch": 0.24408,
      "grad_norm": 0.37598279118537903,
      "learning_rate": 0.00018375250033337778,
      "loss": 0.9353,
      "step": 3051
    },
    {
      "epoch": 0.24416,
      "grad_norm": 0.37888529896736145,
      "learning_rate": 0.00018374716628883852,
      "loss": 0.7854,
      "step": 3052
    },
    {
      "epoch": 0.24424,
      "grad_norm": 0.45855584740638733,
      "learning_rate": 0.00018374183224429923,
      "loss": 0.8602,
      "step": 3053
    },
    {
      "epoch": 0.24432,
      "grad_norm": 0.3693571984767914,
      "learning_rate": 0.00018373649819975997,
      "loss": 0.6413,
      "step": 3054
    },
    {
      "epoch": 0.2444,
      "grad_norm": 0.3590332269668579,
      "learning_rate": 0.00018373116415522071,
      "loss": 0.8497,
      "step": 3055
    },
    {
      "epoch": 0.24448,
      "grad_norm": 0.44941064715385437,
      "learning_rate": 0.00018372583011068143,
      "loss": 1.0179,
      "step": 3056
    },
    {
      "epoch": 0.24456,
      "grad_norm": 0.32041460275650024,
      "learning_rate": 0.00018372049606614217,
      "loss": 0.9062,
      "step": 3057
    },
    {
      "epoch": 0.24464,
      "grad_norm": 0.29283416271209717,
      "learning_rate": 0.00018371516202160288,
      "loss": 0.5883,
      "step": 3058
    },
    {
      "epoch": 0.24472,
      "grad_norm": 0.4032481908798218,
      "learning_rate": 0.00018370982797706362,
      "loss": 0.8251,
      "step": 3059
    },
    {
      "epoch": 0.2448,
      "grad_norm": 0.41773754358291626,
      "learning_rate": 0.00018370449393252433,
      "loss": 0.8654,
      "step": 3060
    },
    {
      "epoch": 0.24488,
      "grad_norm": 0.36702650785446167,
      "learning_rate": 0.00018369915988798507,
      "loss": 0.6212,
      "step": 3061
    },
    {
      "epoch": 0.24496,
      "grad_norm": 0.34846892952919006,
      "learning_rate": 0.0001836938258434458,
      "loss": 1.0291,
      "step": 3062
    },
    {
      "epoch": 0.24504,
      "grad_norm": 0.3486909866333008,
      "learning_rate": 0.00018368849179890652,
      "loss": 1.0965,
      "step": 3063
    },
    {
      "epoch": 0.24512,
      "grad_norm": 0.377907395362854,
      "learning_rate": 0.00018368315775436726,
      "loss": 0.6877,
      "step": 3064
    },
    {
      "epoch": 0.2452,
      "grad_norm": 0.2603919506072998,
      "learning_rate": 0.00018367782370982798,
      "loss": 0.674,
      "step": 3065
    },
    {
      "epoch": 0.24528,
      "grad_norm": 0.4334401488304138,
      "learning_rate": 0.00018367248966528872,
      "loss": 0.8012,
      "step": 3066
    },
    {
      "epoch": 0.24536,
      "grad_norm": 0.4102981686592102,
      "learning_rate": 0.00018366715562074943,
      "loss": 0.8343,
      "step": 3067
    },
    {
      "epoch": 0.24544,
      "grad_norm": 0.30000731348991394,
      "learning_rate": 0.00018366182157621017,
      "loss": 0.6941,
      "step": 3068
    },
    {
      "epoch": 0.24552,
      "grad_norm": 0.2900865077972412,
      "learning_rate": 0.0001836564875316709,
      "loss": 0.5439,
      "step": 3069
    },
    {
      "epoch": 0.2456,
      "grad_norm": 0.4088614881038666,
      "learning_rate": 0.00018365115348713162,
      "loss": 0.6643,
      "step": 3070
    },
    {
      "epoch": 0.24568,
      "grad_norm": 0.3899538516998291,
      "learning_rate": 0.00018364581944259236,
      "loss": 1.069,
      "step": 3071
    },
    {
      "epoch": 0.24576,
      "grad_norm": 0.34597426652908325,
      "learning_rate": 0.00018364048539805307,
      "loss": 1.0263,
      "step": 3072
    },
    {
      "epoch": 0.24584,
      "grad_norm": 0.36671653389930725,
      "learning_rate": 0.00018363515135351381,
      "loss": 0.6967,
      "step": 3073
    },
    {
      "epoch": 0.24592,
      "grad_norm": 0.260097473859787,
      "learning_rate": 0.00018362981730897453,
      "loss": 0.3487,
      "step": 3074
    },
    {
      "epoch": 0.246,
      "grad_norm": 0.40071213245391846,
      "learning_rate": 0.00018362448326443527,
      "loss": 0.7733,
      "step": 3075
    },
    {
      "epoch": 0.24608,
      "grad_norm": 0.3396170139312744,
      "learning_rate": 0.000183619149219896,
      "loss": 0.8666,
      "step": 3076
    },
    {
      "epoch": 0.24616,
      "grad_norm": 0.3258328437805176,
      "learning_rate": 0.00018361381517535672,
      "loss": 0.7508,
      "step": 3077
    },
    {
      "epoch": 0.24624,
      "grad_norm": 0.482302188873291,
      "learning_rate": 0.00018360848113081746,
      "loss": 0.5644,
      "step": 3078
    },
    {
      "epoch": 0.24632,
      "grad_norm": 0.30067235231399536,
      "learning_rate": 0.00018360314708627817,
      "loss": 0.9203,
      "step": 3079
    },
    {
      "epoch": 0.2464,
      "grad_norm": 0.28897514939308167,
      "learning_rate": 0.0001835978130417389,
      "loss": 0.7286,
      "step": 3080
    },
    {
      "epoch": 0.24648,
      "grad_norm": 0.3945028483867645,
      "learning_rate": 0.00018359247899719965,
      "loss": 0.6808,
      "step": 3081
    },
    {
      "epoch": 0.24656,
      "grad_norm": 0.33228373527526855,
      "learning_rate": 0.00018358714495266036,
      "loss": 0.6464,
      "step": 3082
    },
    {
      "epoch": 0.24664,
      "grad_norm": 0.41575977206230164,
      "learning_rate": 0.0001835818109081211,
      "loss": 0.7392,
      "step": 3083
    },
    {
      "epoch": 0.24672,
      "grad_norm": 0.4240322709083557,
      "learning_rate": 0.00018357647686358182,
      "loss": 0.7875,
      "step": 3084
    },
    {
      "epoch": 0.2468,
      "grad_norm": 0.28228166699409485,
      "learning_rate": 0.00018357114281904256,
      "loss": 0.6083,
      "step": 3085
    },
    {
      "epoch": 0.24688,
      "grad_norm": 0.4063026010990143,
      "learning_rate": 0.00018356580877450327,
      "loss": 0.7302,
      "step": 3086
    },
    {
      "epoch": 0.24696,
      "grad_norm": 0.3665398359298706,
      "learning_rate": 0.000183560474729964,
      "loss": 0.9619,
      "step": 3087
    },
    {
      "epoch": 0.24704,
      "grad_norm": 0.2713681757450104,
      "learning_rate": 0.00018355514068542475,
      "loss": 0.6596,
      "step": 3088
    },
    {
      "epoch": 0.24712,
      "grad_norm": 0.32687464356422424,
      "learning_rate": 0.00018354980664088546,
      "loss": 0.7041,
      "step": 3089
    },
    {
      "epoch": 0.2472,
      "grad_norm": 0.3432777225971222,
      "learning_rate": 0.0001835444725963462,
      "loss": 0.8886,
      "step": 3090
    },
    {
      "epoch": 0.24728,
      "grad_norm": 0.4394380450248718,
      "learning_rate": 0.00018353913855180691,
      "loss": 0.8654,
      "step": 3091
    },
    {
      "epoch": 0.24736,
      "grad_norm": 0.37967973947525024,
      "learning_rate": 0.00018353380450726765,
      "loss": 0.6153,
      "step": 3092
    },
    {
      "epoch": 0.24744,
      "grad_norm": 0.44034063816070557,
      "learning_rate": 0.00018352847046272837,
      "loss": 0.666,
      "step": 3093
    },
    {
      "epoch": 0.24752,
      "grad_norm": 0.4401766359806061,
      "learning_rate": 0.0001835231364181891,
      "loss": 0.79,
      "step": 3094
    },
    {
      "epoch": 0.2476,
      "grad_norm": 0.30641013383865356,
      "learning_rate": 0.00018351780237364985,
      "loss": 0.5258,
      "step": 3095
    },
    {
      "epoch": 0.24768,
      "grad_norm": 0.37157323956489563,
      "learning_rate": 0.00018351246832911056,
      "loss": 0.9404,
      "step": 3096
    },
    {
      "epoch": 0.24776,
      "grad_norm": 0.3776952028274536,
      "learning_rate": 0.0001835071342845713,
      "loss": 0.8583,
      "step": 3097
    },
    {
      "epoch": 0.24784,
      "grad_norm": 0.3434705436229706,
      "learning_rate": 0.000183501800240032,
      "loss": 0.5232,
      "step": 3098
    },
    {
      "epoch": 0.24792,
      "grad_norm": 0.32688266038894653,
      "learning_rate": 0.00018349646619549275,
      "loss": 1.0032,
      "step": 3099
    },
    {
      "epoch": 0.248,
      "grad_norm": 0.3000677525997162,
      "learning_rate": 0.00018349113215095346,
      "loss": 0.5349,
      "step": 3100
    },
    {
      "epoch": 0.24808,
      "grad_norm": 0.3042260408401489,
      "learning_rate": 0.0001834857981064142,
      "loss": 0.6497,
      "step": 3101
    },
    {
      "epoch": 0.24816,
      "grad_norm": 0.5619574189186096,
      "learning_rate": 0.00018348046406187494,
      "loss": 1.0639,
      "step": 3102
    },
    {
      "epoch": 0.24824,
      "grad_norm": 0.4657951295375824,
      "learning_rate": 0.00018347513001733566,
      "loss": 0.7377,
      "step": 3103
    },
    {
      "epoch": 0.24832,
      "grad_norm": 0.36056435108184814,
      "learning_rate": 0.0001834697959727964,
      "loss": 1.0293,
      "step": 3104
    },
    {
      "epoch": 0.2484,
      "grad_norm": 0.3660931885242462,
      "learning_rate": 0.0001834644619282571,
      "loss": 1.2317,
      "step": 3105
    },
    {
      "epoch": 0.24848,
      "grad_norm": 0.39662814140319824,
      "learning_rate": 0.00018345912788371785,
      "loss": 0.7589,
      "step": 3106
    },
    {
      "epoch": 0.24856,
      "grad_norm": 0.42527860403060913,
      "learning_rate": 0.00018345379383917856,
      "loss": 0.9933,
      "step": 3107
    },
    {
      "epoch": 0.24864,
      "grad_norm": 0.37802860140800476,
      "learning_rate": 0.0001834484597946393,
      "loss": 0.7348,
      "step": 3108
    },
    {
      "epoch": 0.24872,
      "grad_norm": 0.36881715059280396,
      "learning_rate": 0.00018344312575010004,
      "loss": 0.681,
      "step": 3109
    },
    {
      "epoch": 0.2488,
      "grad_norm": 0.5641381144523621,
      "learning_rate": 0.00018343779170556076,
      "loss": 0.675,
      "step": 3110
    },
    {
      "epoch": 0.24888,
      "grad_norm": 0.36509570479393005,
      "learning_rate": 0.0001834324576610215,
      "loss": 0.9318,
      "step": 3111
    },
    {
      "epoch": 0.24896,
      "grad_norm": 0.3487088084220886,
      "learning_rate": 0.0001834271236164822,
      "loss": 0.6135,
      "step": 3112
    },
    {
      "epoch": 0.24904,
      "grad_norm": 0.33367136120796204,
      "learning_rate": 0.00018342178957194295,
      "loss": 0.6564,
      "step": 3113
    },
    {
      "epoch": 0.24912,
      "grad_norm": 0.3842438757419586,
      "learning_rate": 0.00018341645552740366,
      "loss": 0.5742,
      "step": 3114
    },
    {
      "epoch": 0.2492,
      "grad_norm": 0.3559231758117676,
      "learning_rate": 0.0001834111214828644,
      "loss": 0.9542,
      "step": 3115
    },
    {
      "epoch": 0.24928,
      "grad_norm": 0.42528995871543884,
      "learning_rate": 0.00018340578743832514,
      "loss": 1.0047,
      "step": 3116
    },
    {
      "epoch": 0.24936,
      "grad_norm": 0.3297552168369293,
      "learning_rate": 0.00018340045339378585,
      "loss": 0.5893,
      "step": 3117
    },
    {
      "epoch": 0.24944,
      "grad_norm": 0.44836997985839844,
      "learning_rate": 0.0001833951193492466,
      "loss": 0.893,
      "step": 3118
    },
    {
      "epoch": 0.24952,
      "grad_norm": 0.37678831815719604,
      "learning_rate": 0.0001833897853047073,
      "loss": 0.9102,
      "step": 3119
    },
    {
      "epoch": 0.2496,
      "grad_norm": 0.46461233496665955,
      "learning_rate": 0.00018338445126016805,
      "loss": 0.9238,
      "step": 3120
    },
    {
      "epoch": 0.24968,
      "grad_norm": 0.40949225425720215,
      "learning_rate": 0.00018337911721562876,
      "loss": 0.9237,
      "step": 3121
    },
    {
      "epoch": 0.24976,
      "grad_norm": 0.5259213447570801,
      "learning_rate": 0.0001833737831710895,
      "loss": 0.9333,
      "step": 3122
    },
    {
      "epoch": 0.24984,
      "grad_norm": 0.4438653290271759,
      "learning_rate": 0.0001833684491265502,
      "loss": 1.1614,
      "step": 3123
    },
    {
      "epoch": 0.24992,
      "grad_norm": 0.33146965503692627,
      "learning_rate": 0.00018336311508201095,
      "loss": 1.0626,
      "step": 3124
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.427120178937912,
      "learning_rate": 0.00018335778103747166,
      "loss": 0.9373,
      "step": 3125
    },
    {
      "epoch": 0.25008,
      "grad_norm": 0.3429846465587616,
      "learning_rate": 0.0001833524469929324,
      "loss": 0.9949,
      "step": 3126
    },
    {
      "epoch": 0.25016,
      "grad_norm": 0.34712541103363037,
      "learning_rate": 0.00018334711294839314,
      "loss": 0.834,
      "step": 3127
    },
    {
      "epoch": 0.25024,
      "grad_norm": 0.4618915915489197,
      "learning_rate": 0.00018334177890385386,
      "loss": 0.9662,
      "step": 3128
    },
    {
      "epoch": 0.25032,
      "grad_norm": 0.3278549015522003,
      "learning_rate": 0.0001833364448593146,
      "loss": 0.6098,
      "step": 3129
    },
    {
      "epoch": 0.2504,
      "grad_norm": 0.31705179810523987,
      "learning_rate": 0.0001833311108147753,
      "loss": 0.6381,
      "step": 3130
    },
    {
      "epoch": 0.25048,
      "grad_norm": 0.385272353887558,
      "learning_rate": 0.00018332577677023605,
      "loss": 0.6007,
      "step": 3131
    },
    {
      "epoch": 0.25056,
      "grad_norm": 0.39489227533340454,
      "learning_rate": 0.00018332044272569676,
      "loss": 0.6163,
      "step": 3132
    },
    {
      "epoch": 0.25064,
      "grad_norm": 0.34145429730415344,
      "learning_rate": 0.0001833151086811575,
      "loss": 0.5595,
      "step": 3133
    },
    {
      "epoch": 0.25072,
      "grad_norm": 0.37567809224128723,
      "learning_rate": 0.0001833097746366182,
      "loss": 1.0983,
      "step": 3134
    },
    {
      "epoch": 0.2508,
      "grad_norm": 0.4364280104637146,
      "learning_rate": 0.00018330444059207895,
      "loss": 0.8656,
      "step": 3135
    },
    {
      "epoch": 0.25088,
      "grad_norm": 0.43293845653533936,
      "learning_rate": 0.00018329910654753967,
      "loss": 1.0104,
      "step": 3136
    },
    {
      "epoch": 0.25096,
      "grad_norm": 0.39869028329849243,
      "learning_rate": 0.0001832937725030004,
      "loss": 0.9663,
      "step": 3137
    },
    {
      "epoch": 0.25104,
      "grad_norm": 0.3218260705471039,
      "learning_rate": 0.00018328843845846112,
      "loss": 0.5216,
      "step": 3138
    },
    {
      "epoch": 0.25112,
      "grad_norm": 0.5606906414031982,
      "learning_rate": 0.00018328310441392186,
      "loss": 0.8359,
      "step": 3139
    },
    {
      "epoch": 0.2512,
      "grad_norm": 0.3113768994808197,
      "learning_rate": 0.00018327777036938257,
      "loss": 0.8874,
      "step": 3140
    },
    {
      "epoch": 0.25128,
      "grad_norm": 0.4149511158466339,
      "learning_rate": 0.0001832724363248433,
      "loss": 0.6194,
      "step": 3141
    },
    {
      "epoch": 0.25136,
      "grad_norm": 0.42241939902305603,
      "learning_rate": 0.00018326710228030405,
      "loss": 0.732,
      "step": 3142
    },
    {
      "epoch": 0.25144,
      "grad_norm": 0.41358256340026855,
      "learning_rate": 0.00018326176823576476,
      "loss": 0.8014,
      "step": 3143
    },
    {
      "epoch": 0.25152,
      "grad_norm": 0.25904715061187744,
      "learning_rate": 0.0001832564341912255,
      "loss": 0.8385,
      "step": 3144
    },
    {
      "epoch": 0.2516,
      "grad_norm": 0.39050668478012085,
      "learning_rate": 0.00018325110014668622,
      "loss": 0.7343,
      "step": 3145
    },
    {
      "epoch": 0.25168,
      "grad_norm": 0.38022658228874207,
      "learning_rate": 0.00018324576610214696,
      "loss": 0.7763,
      "step": 3146
    },
    {
      "epoch": 0.25176,
      "grad_norm": 0.41996368765830994,
      "learning_rate": 0.00018324043205760767,
      "loss": 0.7352,
      "step": 3147
    },
    {
      "epoch": 0.25184,
      "grad_norm": 0.38394126296043396,
      "learning_rate": 0.0001832350980130684,
      "loss": 0.6332,
      "step": 3148
    },
    {
      "epoch": 0.25192,
      "grad_norm": 0.36942246556282043,
      "learning_rate": 0.00018322976396852915,
      "loss": 0.8703,
      "step": 3149
    },
    {
      "epoch": 0.252,
      "grad_norm": 0.3543492257595062,
      "learning_rate": 0.00018322442992398986,
      "loss": 0.6553,
      "step": 3150
    },
    {
      "epoch": 0.25208,
      "grad_norm": 0.3981013894081116,
      "learning_rate": 0.0001832190958794506,
      "loss": 0.7994,
      "step": 3151
    },
    {
      "epoch": 0.25216,
      "grad_norm": 0.35442009568214417,
      "learning_rate": 0.00018321376183491131,
      "loss": 0.6649,
      "step": 3152
    },
    {
      "epoch": 0.25224,
      "grad_norm": 0.40118497610092163,
      "learning_rate": 0.00018320842779037205,
      "loss": 0.6713,
      "step": 3153
    },
    {
      "epoch": 0.25232,
      "grad_norm": 0.2900014817714691,
      "learning_rate": 0.00018320309374583277,
      "loss": 0.847,
      "step": 3154
    },
    {
      "epoch": 0.2524,
      "grad_norm": 0.45439285039901733,
      "learning_rate": 0.0001831977597012935,
      "loss": 0.8943,
      "step": 3155
    },
    {
      "epoch": 0.25248,
      "grad_norm": 0.2999001145362854,
      "learning_rate": 0.00018319242565675425,
      "loss": 1.1212,
      "step": 3156
    },
    {
      "epoch": 0.25256,
      "grad_norm": 0.3040474057197571,
      "learning_rate": 0.00018318709161221496,
      "loss": 0.6174,
      "step": 3157
    },
    {
      "epoch": 0.25264,
      "grad_norm": 0.2461688369512558,
      "learning_rate": 0.0001831817575676757,
      "loss": 0.663,
      "step": 3158
    },
    {
      "epoch": 0.25272,
      "grad_norm": 0.35557886958122253,
      "learning_rate": 0.0001831764235231364,
      "loss": 0.6593,
      "step": 3159
    },
    {
      "epoch": 0.2528,
      "grad_norm": 0.31042051315307617,
      "learning_rate": 0.00018317108947859715,
      "loss": 0.8,
      "step": 3160
    },
    {
      "epoch": 0.25288,
      "grad_norm": 0.40123096108436584,
      "learning_rate": 0.00018316575543405786,
      "loss": 0.9734,
      "step": 3161
    },
    {
      "epoch": 0.25296,
      "grad_norm": 0.41457971930503845,
      "learning_rate": 0.0001831604213895186,
      "loss": 1.0331,
      "step": 3162
    },
    {
      "epoch": 0.25304,
      "grad_norm": 0.4007902443408966,
      "learning_rate": 0.00018315508734497934,
      "loss": 1.0086,
      "step": 3163
    },
    {
      "epoch": 0.25312,
      "grad_norm": 0.38481298089027405,
      "learning_rate": 0.00018314975330044006,
      "loss": 1.267,
      "step": 3164
    },
    {
      "epoch": 0.2532,
      "grad_norm": 0.4074561893939972,
      "learning_rate": 0.0001831444192559008,
      "loss": 0.7142,
      "step": 3165
    },
    {
      "epoch": 0.25328,
      "grad_norm": 0.47525471448898315,
      "learning_rate": 0.0001831390852113615,
      "loss": 0.711,
      "step": 3166
    },
    {
      "epoch": 0.25336,
      "grad_norm": 0.29694828391075134,
      "learning_rate": 0.00018313375116682225,
      "loss": 0.7089,
      "step": 3167
    },
    {
      "epoch": 0.25344,
      "grad_norm": 0.37252527475357056,
      "learning_rate": 0.00018312841712228296,
      "loss": 0.6578,
      "step": 3168
    },
    {
      "epoch": 0.25352,
      "grad_norm": 0.4276595115661621,
      "learning_rate": 0.0001831230830777437,
      "loss": 0.75,
      "step": 3169
    },
    {
      "epoch": 0.2536,
      "grad_norm": 0.4162988066673279,
      "learning_rate": 0.00018311774903320444,
      "loss": 0.7921,
      "step": 3170
    },
    {
      "epoch": 0.25368,
      "grad_norm": 0.3895846903324127,
      "learning_rate": 0.00018311241498866515,
      "loss": 1.1852,
      "step": 3171
    },
    {
      "epoch": 0.25376,
      "grad_norm": 0.24738438427448273,
      "learning_rate": 0.0001831070809441259,
      "loss": 1.0993,
      "step": 3172
    },
    {
      "epoch": 0.25384,
      "grad_norm": 0.43673279881477356,
      "learning_rate": 0.0001831017468995866,
      "loss": 0.9344,
      "step": 3173
    },
    {
      "epoch": 0.25392,
      "grad_norm": 0.36294588446617126,
      "learning_rate": 0.00018309641285504735,
      "loss": 0.6957,
      "step": 3174
    },
    {
      "epoch": 0.254,
      "grad_norm": 0.37559887766838074,
      "learning_rate": 0.00018309107881050806,
      "loss": 0.8599,
      "step": 3175
    },
    {
      "epoch": 0.25408,
      "grad_norm": 0.4249666631221771,
      "learning_rate": 0.0001830857447659688,
      "loss": 0.8443,
      "step": 3176
    },
    {
      "epoch": 0.25416,
      "grad_norm": 0.4620836675167084,
      "learning_rate": 0.00018308041072142954,
      "loss": 0.795,
      "step": 3177
    },
    {
      "epoch": 0.25424,
      "grad_norm": 0.5189161896705627,
      "learning_rate": 0.00018307507667689025,
      "loss": 1.0339,
      "step": 3178
    },
    {
      "epoch": 0.25432,
      "grad_norm": 0.42195212841033936,
      "learning_rate": 0.000183069742632351,
      "loss": 0.6499,
      "step": 3179
    },
    {
      "epoch": 0.2544,
      "grad_norm": 0.34053459763526917,
      "learning_rate": 0.0001830644085878117,
      "loss": 1.0009,
      "step": 3180
    },
    {
      "epoch": 0.25448,
      "grad_norm": 0.4313647150993347,
      "learning_rate": 0.00018305907454327244,
      "loss": 0.8102,
      "step": 3181
    },
    {
      "epoch": 0.25456,
      "grad_norm": 0.45203548669815063,
      "learning_rate": 0.00018305374049873318,
      "loss": 0.8595,
      "step": 3182
    },
    {
      "epoch": 0.25464,
      "grad_norm": 0.323804646730423,
      "learning_rate": 0.0001830484064541939,
      "loss": 0.79,
      "step": 3183
    },
    {
      "epoch": 0.25472,
      "grad_norm": 0.31004786491394043,
      "learning_rate": 0.00018304307240965464,
      "loss": 0.6628,
      "step": 3184
    },
    {
      "epoch": 0.2548,
      "grad_norm": 0.43842601776123047,
      "learning_rate": 0.00018303773836511535,
      "loss": 0.8725,
      "step": 3185
    },
    {
      "epoch": 0.25488,
      "grad_norm": 0.37320011854171753,
      "learning_rate": 0.0001830324043205761,
      "loss": 0.7518,
      "step": 3186
    },
    {
      "epoch": 0.25496,
      "grad_norm": 0.4360743761062622,
      "learning_rate": 0.0001830270702760368,
      "loss": 0.8422,
      "step": 3187
    },
    {
      "epoch": 0.25504,
      "grad_norm": 0.3853626847267151,
      "learning_rate": 0.00018302173623149754,
      "loss": 0.7106,
      "step": 3188
    },
    {
      "epoch": 0.25512,
      "grad_norm": 0.4389369785785675,
      "learning_rate": 0.00018301640218695828,
      "loss": 0.8531,
      "step": 3189
    },
    {
      "epoch": 0.2552,
      "grad_norm": 0.3322226405143738,
      "learning_rate": 0.000183011068142419,
      "loss": 0.7223,
      "step": 3190
    },
    {
      "epoch": 0.25528,
      "grad_norm": 0.3049136698246002,
      "learning_rate": 0.00018300573409787973,
      "loss": 0.8911,
      "step": 3191
    },
    {
      "epoch": 0.25536,
      "grad_norm": 0.3547949194908142,
      "learning_rate": 0.00018300040005334045,
      "loss": 1.0938,
      "step": 3192
    },
    {
      "epoch": 0.25544,
      "grad_norm": 0.35256627202033997,
      "learning_rate": 0.0001829950660088012,
      "loss": 1.0936,
      "step": 3193
    },
    {
      "epoch": 0.25552,
      "grad_norm": 0.36090975999832153,
      "learning_rate": 0.0001829897319642619,
      "loss": 0.8427,
      "step": 3194
    },
    {
      "epoch": 0.2556,
      "grad_norm": 0.42602622509002686,
      "learning_rate": 0.00018298439791972264,
      "loss": 0.9552,
      "step": 3195
    },
    {
      "epoch": 0.25568,
      "grad_norm": 0.40302079916000366,
      "learning_rate": 0.00018297906387518338,
      "loss": 0.7625,
      "step": 3196
    },
    {
      "epoch": 0.25576,
      "grad_norm": 0.309515118598938,
      "learning_rate": 0.0001829737298306441,
      "loss": 0.8802,
      "step": 3197
    },
    {
      "epoch": 0.25584,
      "grad_norm": 0.4446898102760315,
      "learning_rate": 0.00018296839578610483,
      "loss": 0.7665,
      "step": 3198
    },
    {
      "epoch": 0.25592,
      "grad_norm": 0.45616334676742554,
      "learning_rate": 0.00018296306174156554,
      "loss": 1.0038,
      "step": 3199
    },
    {
      "epoch": 0.256,
      "grad_norm": 0.36814796924591064,
      "learning_rate": 0.00018295772769702628,
      "loss": 0.6905,
      "step": 3200
    },
    {
      "epoch": 0.25608,
      "grad_norm": 0.4648812711238861,
      "learning_rate": 0.000182952393652487,
      "loss": 0.945,
      "step": 3201
    },
    {
      "epoch": 0.25616,
      "grad_norm": 0.4150943160057068,
      "learning_rate": 0.00018294705960794774,
      "loss": 0.6513,
      "step": 3202
    },
    {
      "epoch": 0.25624,
      "grad_norm": 0.36359500885009766,
      "learning_rate": 0.00018294172556340848,
      "loss": 1.0046,
      "step": 3203
    },
    {
      "epoch": 0.25632,
      "grad_norm": 0.3557576835155487,
      "learning_rate": 0.0001829363915188692,
      "loss": 0.6229,
      "step": 3204
    },
    {
      "epoch": 0.2564,
      "grad_norm": 0.3364204168319702,
      "learning_rate": 0.00018293105747432993,
      "loss": 0.7215,
      "step": 3205
    },
    {
      "epoch": 0.25648,
      "grad_norm": 0.37458446621894836,
      "learning_rate": 0.00018292572342979064,
      "loss": 1.0301,
      "step": 3206
    },
    {
      "epoch": 0.25656,
      "grad_norm": 0.45122092962265015,
      "learning_rate": 0.00018292038938525138,
      "loss": 0.7883,
      "step": 3207
    },
    {
      "epoch": 0.25664,
      "grad_norm": 0.3867919445037842,
      "learning_rate": 0.0001829150553407121,
      "loss": 0.8336,
      "step": 3208
    },
    {
      "epoch": 0.25672,
      "grad_norm": 0.33519092202186584,
      "learning_rate": 0.00018290972129617283,
      "loss": 0.6586,
      "step": 3209
    },
    {
      "epoch": 0.2568,
      "grad_norm": 0.5743834972381592,
      "learning_rate": 0.00018290438725163357,
      "loss": 0.9335,
      "step": 3210
    },
    {
      "epoch": 0.25688,
      "grad_norm": 0.4216298758983612,
      "learning_rate": 0.0001828990532070943,
      "loss": 0.8873,
      "step": 3211
    },
    {
      "epoch": 0.25696,
      "grad_norm": 0.21895508468151093,
      "learning_rate": 0.00018289371916255503,
      "loss": 0.7542,
      "step": 3212
    },
    {
      "epoch": 0.25704,
      "grad_norm": 0.35771802067756653,
      "learning_rate": 0.00018288838511801574,
      "loss": 0.8383,
      "step": 3213
    },
    {
      "epoch": 0.25712,
      "grad_norm": 0.3083484470844269,
      "learning_rate": 0.00018288305107347648,
      "loss": 0.6936,
      "step": 3214
    },
    {
      "epoch": 0.2572,
      "grad_norm": 0.343662291765213,
      "learning_rate": 0.0001828777170289372,
      "loss": 0.8106,
      "step": 3215
    },
    {
      "epoch": 0.25728,
      "grad_norm": 0.35170555114746094,
      "learning_rate": 0.00018287238298439793,
      "loss": 1.0518,
      "step": 3216
    },
    {
      "epoch": 0.25736,
      "grad_norm": 0.39832860231399536,
      "learning_rate": 0.00018286704893985867,
      "loss": 0.6012,
      "step": 3217
    },
    {
      "epoch": 0.25744,
      "grad_norm": 0.4290561079978943,
      "learning_rate": 0.00018286171489531938,
      "loss": 0.752,
      "step": 3218
    },
    {
      "epoch": 0.25752,
      "grad_norm": 0.37138688564300537,
      "learning_rate": 0.00018285638085078012,
      "loss": 0.6136,
      "step": 3219
    },
    {
      "epoch": 0.2576,
      "grad_norm": 0.4337923526763916,
      "learning_rate": 0.00018285104680624084,
      "loss": 0.8965,
      "step": 3220
    },
    {
      "epoch": 0.25768,
      "grad_norm": 0.4117496907711029,
      "learning_rate": 0.00018284571276170158,
      "loss": 0.5885,
      "step": 3221
    },
    {
      "epoch": 0.25776,
      "grad_norm": 0.3767995238304138,
      "learning_rate": 0.0001828403787171623,
      "loss": 1.0316,
      "step": 3222
    },
    {
      "epoch": 0.25784,
      "grad_norm": 0.4639240503311157,
      "learning_rate": 0.00018283504467262303,
      "loss": 1.0492,
      "step": 3223
    },
    {
      "epoch": 0.25792,
      "grad_norm": 0.34343814849853516,
      "learning_rate": 0.00018282971062808377,
      "loss": 0.9116,
      "step": 3224
    },
    {
      "epoch": 0.258,
      "grad_norm": 0.3982933461666107,
      "learning_rate": 0.00018282437658354448,
      "loss": 0.9189,
      "step": 3225
    },
    {
      "epoch": 0.25808,
      "grad_norm": 0.5269227623939514,
      "learning_rate": 0.00018281904253900522,
      "loss": 0.9907,
      "step": 3226
    },
    {
      "epoch": 0.25816,
      "grad_norm": 0.3934669494628906,
      "learning_rate": 0.00018281370849446593,
      "loss": 0.8405,
      "step": 3227
    },
    {
      "epoch": 0.25824,
      "grad_norm": 0.4641466736793518,
      "learning_rate": 0.00018280837444992667,
      "loss": 0.7058,
      "step": 3228
    },
    {
      "epoch": 0.25832,
      "grad_norm": 0.37153851985931396,
      "learning_rate": 0.00018280304040538741,
      "loss": 1.0196,
      "step": 3229
    },
    {
      "epoch": 0.2584,
      "grad_norm": 0.3894196152687073,
      "learning_rate": 0.00018279770636084813,
      "loss": 0.6227,
      "step": 3230
    },
    {
      "epoch": 0.25848,
      "grad_norm": 0.4641660451889038,
      "learning_rate": 0.00018279237231630887,
      "loss": 0.7783,
      "step": 3231
    },
    {
      "epoch": 0.25856,
      "grad_norm": 0.33670899271965027,
      "learning_rate": 0.00018278703827176958,
      "loss": 0.8686,
      "step": 3232
    },
    {
      "epoch": 0.25864,
      "grad_norm": 0.32339388132095337,
      "learning_rate": 0.00018278170422723032,
      "loss": 1.2958,
      "step": 3233
    },
    {
      "epoch": 0.25872,
      "grad_norm": 0.3931445777416229,
      "learning_rate": 0.00018277637018269103,
      "loss": 1.0168,
      "step": 3234
    },
    {
      "epoch": 0.2588,
      "grad_norm": 0.38567933440208435,
      "learning_rate": 0.00018277103613815177,
      "loss": 1.0526,
      "step": 3235
    },
    {
      "epoch": 0.25888,
      "grad_norm": 0.32861095666885376,
      "learning_rate": 0.0001827657020936125,
      "loss": 1.0966,
      "step": 3236
    },
    {
      "epoch": 0.25896,
      "grad_norm": 0.28135380148887634,
      "learning_rate": 0.00018276036804907322,
      "loss": 0.9078,
      "step": 3237
    },
    {
      "epoch": 0.25904,
      "grad_norm": 0.3587800860404968,
      "learning_rate": 0.00018275503400453396,
      "loss": 1.027,
      "step": 3238
    },
    {
      "epoch": 0.25912,
      "grad_norm": 0.3208921253681183,
      "learning_rate": 0.00018274969995999468,
      "loss": 0.5717,
      "step": 3239
    },
    {
      "epoch": 0.2592,
      "grad_norm": 0.3147180378437042,
      "learning_rate": 0.00018274436591545542,
      "loss": 0.5715,
      "step": 3240
    },
    {
      "epoch": 0.25928,
      "grad_norm": 0.39404892921447754,
      "learning_rate": 0.00018273903187091613,
      "loss": 0.7969,
      "step": 3241
    },
    {
      "epoch": 0.25936,
      "grad_norm": 0.3803761899471283,
      "learning_rate": 0.00018273369782637687,
      "loss": 0.8365,
      "step": 3242
    },
    {
      "epoch": 0.25944,
      "grad_norm": 0.34956738352775574,
      "learning_rate": 0.0001827283637818376,
      "loss": 0.6951,
      "step": 3243
    },
    {
      "epoch": 0.25952,
      "grad_norm": 0.4640432596206665,
      "learning_rate": 0.00018272302973729832,
      "loss": 0.5832,
      "step": 3244
    },
    {
      "epoch": 0.2596,
      "grad_norm": 0.3889409005641937,
      "learning_rate": 0.00018271769569275906,
      "loss": 0.9894,
      "step": 3245
    },
    {
      "epoch": 0.25968,
      "grad_norm": 0.36033108830451965,
      "learning_rate": 0.00018271236164821978,
      "loss": 0.6484,
      "step": 3246
    },
    {
      "epoch": 0.25976,
      "grad_norm": 0.4003011882305145,
      "learning_rate": 0.00018270702760368052,
      "loss": 0.7284,
      "step": 3247
    },
    {
      "epoch": 0.25984,
      "grad_norm": 0.3869228959083557,
      "learning_rate": 0.00018270169355914123,
      "loss": 0.8283,
      "step": 3248
    },
    {
      "epoch": 0.25992,
      "grad_norm": 0.37556642293930054,
      "learning_rate": 0.00018269635951460197,
      "loss": 1.0125,
      "step": 3249
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.3617725670337677,
      "learning_rate": 0.00018269102547006268,
      "loss": 0.894,
      "step": 3250
    },
    {
      "epoch": 0.26008,
      "grad_norm": 0.3325716555118561,
      "learning_rate": 0.00018268569142552342,
      "loss": 1.0344,
      "step": 3251
    },
    {
      "epoch": 0.26016,
      "grad_norm": 0.4561493992805481,
      "learning_rate": 0.00018268035738098413,
      "loss": 0.6955,
      "step": 3252
    },
    {
      "epoch": 0.26024,
      "grad_norm": 0.3268301486968994,
      "learning_rate": 0.00018267502333644487,
      "loss": 0.8921,
      "step": 3253
    },
    {
      "epoch": 0.26032,
      "grad_norm": 0.43434274196624756,
      "learning_rate": 0.0001826696892919056,
      "loss": 0.7439,
      "step": 3254
    },
    {
      "epoch": 0.2604,
      "grad_norm": 0.35543978214263916,
      "learning_rate": 0.00018266435524736633,
      "loss": 0.8147,
      "step": 3255
    },
    {
      "epoch": 0.26048,
      "grad_norm": 0.26002195477485657,
      "learning_rate": 0.00018265902120282707,
      "loss": 0.4828,
      "step": 3256
    },
    {
      "epoch": 0.26056,
      "grad_norm": 0.43573540449142456,
      "learning_rate": 0.00018265368715828778,
      "loss": 0.7958,
      "step": 3257
    },
    {
      "epoch": 0.26064,
      "grad_norm": 0.4271208345890045,
      "learning_rate": 0.00018264835311374852,
      "loss": 0.8598,
      "step": 3258
    },
    {
      "epoch": 0.26072,
      "grad_norm": 0.4205916225910187,
      "learning_rate": 0.00018264301906920923,
      "loss": 0.639,
      "step": 3259
    },
    {
      "epoch": 0.2608,
      "grad_norm": 0.42652463912963867,
      "learning_rate": 0.00018263768502466997,
      "loss": 0.8946,
      "step": 3260
    },
    {
      "epoch": 0.26088,
      "grad_norm": 0.3082747757434845,
      "learning_rate": 0.00018263235098013068,
      "loss": 0.5251,
      "step": 3261
    },
    {
      "epoch": 0.26096,
      "grad_norm": 0.3962138295173645,
      "learning_rate": 0.00018262701693559142,
      "loss": 0.9981,
      "step": 3262
    },
    {
      "epoch": 0.26104,
      "grad_norm": 0.3538567125797272,
      "learning_rate": 0.00018262168289105214,
      "loss": 0.6453,
      "step": 3263
    },
    {
      "epoch": 0.26112,
      "grad_norm": 0.4842335879802704,
      "learning_rate": 0.00018261634884651288,
      "loss": 0.6199,
      "step": 3264
    },
    {
      "epoch": 0.2612,
      "grad_norm": 0.3416118919849396,
      "learning_rate": 0.0001826110148019736,
      "loss": 1.1118,
      "step": 3265
    },
    {
      "epoch": 0.26128,
      "grad_norm": 0.37193426489830017,
      "learning_rate": 0.00018260568075743433,
      "loss": 0.6468,
      "step": 3266
    },
    {
      "epoch": 0.26136,
      "grad_norm": 0.4909122586250305,
      "learning_rate": 0.00018260034671289504,
      "loss": 0.7765,
      "step": 3267
    },
    {
      "epoch": 0.26144,
      "grad_norm": 0.4214268624782562,
      "learning_rate": 0.00018259501266835578,
      "loss": 1.0136,
      "step": 3268
    },
    {
      "epoch": 0.26152,
      "grad_norm": 0.3972180485725403,
      "learning_rate": 0.0001825896786238165,
      "loss": 0.8857,
      "step": 3269
    },
    {
      "epoch": 0.2616,
      "grad_norm": 0.3745947778224945,
      "learning_rate": 0.00018258434457927723,
      "loss": 0.7234,
      "step": 3270
    },
    {
      "epoch": 0.26168,
      "grad_norm": 0.4126085937023163,
      "learning_rate": 0.00018257901053473797,
      "loss": 0.9831,
      "step": 3271
    },
    {
      "epoch": 0.26176,
      "grad_norm": 0.3622739613056183,
      "learning_rate": 0.00018257367649019869,
      "loss": 0.4971,
      "step": 3272
    },
    {
      "epoch": 0.26184,
      "grad_norm": 0.3945655822753906,
      "learning_rate": 0.00018256834244565943,
      "loss": 1.1404,
      "step": 3273
    },
    {
      "epoch": 0.26192,
      "grad_norm": 0.39954861998558044,
      "learning_rate": 0.00018256300840112014,
      "loss": 0.7556,
      "step": 3274
    },
    {
      "epoch": 0.262,
      "grad_norm": 0.36522069573402405,
      "learning_rate": 0.00018255767435658088,
      "loss": 0.5504,
      "step": 3275
    },
    {
      "epoch": 0.26208,
      "grad_norm": 0.3904528319835663,
      "learning_rate": 0.00018255234031204162,
      "loss": 1.2348,
      "step": 3276
    },
    {
      "epoch": 0.26216,
      "grad_norm": 0.39129069447517395,
      "learning_rate": 0.00018254700626750233,
      "loss": 0.9669,
      "step": 3277
    },
    {
      "epoch": 0.26224,
      "grad_norm": 0.33075791597366333,
      "learning_rate": 0.00018254167222296307,
      "loss": 0.6549,
      "step": 3278
    },
    {
      "epoch": 0.26232,
      "grad_norm": 0.3988496959209442,
      "learning_rate": 0.00018253633817842378,
      "loss": 1.0748,
      "step": 3279
    },
    {
      "epoch": 0.2624,
      "grad_norm": 0.32689279317855835,
      "learning_rate": 0.00018253100413388452,
      "loss": 0.6132,
      "step": 3280
    },
    {
      "epoch": 0.26248,
      "grad_norm": 0.4050275683403015,
      "learning_rate": 0.00018252567008934524,
      "loss": 0.616,
      "step": 3281
    },
    {
      "epoch": 0.26256,
      "grad_norm": 0.3232203722000122,
      "learning_rate": 0.00018252033604480598,
      "loss": 0.6895,
      "step": 3282
    },
    {
      "epoch": 0.26264,
      "grad_norm": 0.4422891139984131,
      "learning_rate": 0.00018251500200026672,
      "loss": 0.7369,
      "step": 3283
    },
    {
      "epoch": 0.26272,
      "grad_norm": 0.2919315993785858,
      "learning_rate": 0.00018250966795572743,
      "loss": 0.6439,
      "step": 3284
    },
    {
      "epoch": 0.2628,
      "grad_norm": 0.33812063932418823,
      "learning_rate": 0.00018250433391118817,
      "loss": 1.0125,
      "step": 3285
    },
    {
      "epoch": 0.26288,
      "grad_norm": 0.3983783721923828,
      "learning_rate": 0.00018249899986664888,
      "loss": 0.866,
      "step": 3286
    },
    {
      "epoch": 0.26296,
      "grad_norm": 0.25639867782592773,
      "learning_rate": 0.00018249366582210962,
      "loss": 1.0943,
      "step": 3287
    },
    {
      "epoch": 0.26304,
      "grad_norm": 0.5024877786636353,
      "learning_rate": 0.00018248833177757033,
      "loss": 1.2004,
      "step": 3288
    },
    {
      "epoch": 0.26312,
      "grad_norm": 0.43176108598709106,
      "learning_rate": 0.00018248299773303107,
      "loss": 0.882,
      "step": 3289
    },
    {
      "epoch": 0.2632,
      "grad_norm": 0.30450060963630676,
      "learning_rate": 0.0001824776636884918,
      "loss": 0.7749,
      "step": 3290
    },
    {
      "epoch": 0.26328,
      "grad_norm": 0.35976237058639526,
      "learning_rate": 0.00018247232964395253,
      "loss": 0.7049,
      "step": 3291
    },
    {
      "epoch": 0.26336,
      "grad_norm": 0.3520564138889313,
      "learning_rate": 0.00018246699559941327,
      "loss": 0.5723,
      "step": 3292
    },
    {
      "epoch": 0.26344,
      "grad_norm": 0.33974340558052063,
      "learning_rate": 0.00018246166155487398,
      "loss": 0.7869,
      "step": 3293
    },
    {
      "epoch": 0.26352,
      "grad_norm": 0.3814125061035156,
      "learning_rate": 0.00018245632751033472,
      "loss": 0.6374,
      "step": 3294
    },
    {
      "epoch": 0.2636,
      "grad_norm": 0.38716068863868713,
      "learning_rate": 0.00018245099346579543,
      "loss": 0.7448,
      "step": 3295
    },
    {
      "epoch": 0.26368,
      "grad_norm": 0.26713159680366516,
      "learning_rate": 0.00018244565942125617,
      "loss": 0.6296,
      "step": 3296
    },
    {
      "epoch": 0.26376,
      "grad_norm": 0.40704795718193054,
      "learning_rate": 0.0001824403253767169,
      "loss": 0.8858,
      "step": 3297
    },
    {
      "epoch": 0.26384,
      "grad_norm": 0.45097172260284424,
      "learning_rate": 0.00018243499133217762,
      "loss": 1.0194,
      "step": 3298
    },
    {
      "epoch": 0.26392,
      "grad_norm": 0.36530569195747375,
      "learning_rate": 0.00018242965728763836,
      "loss": 0.6494,
      "step": 3299
    },
    {
      "epoch": 0.264,
      "grad_norm": 0.4840250015258789,
      "learning_rate": 0.00018242432324309908,
      "loss": 0.8593,
      "step": 3300
    },
    {
      "epoch": 0.26408,
      "grad_norm": 0.3051672577857971,
      "learning_rate": 0.00018241898919855982,
      "loss": 0.5065,
      "step": 3301
    },
    {
      "epoch": 0.26416,
      "grad_norm": 0.4087975323200226,
      "learning_rate": 0.00018241365515402053,
      "loss": 0.8384,
      "step": 3302
    },
    {
      "epoch": 0.26424,
      "grad_norm": 0.3500072956085205,
      "learning_rate": 0.00018240832110948127,
      "loss": 0.8337,
      "step": 3303
    },
    {
      "epoch": 0.26432,
      "grad_norm": 0.4227948486804962,
      "learning_rate": 0.000182402987064942,
      "loss": 0.6263,
      "step": 3304
    },
    {
      "epoch": 0.2644,
      "grad_norm": 0.38460177183151245,
      "learning_rate": 0.00018239765302040272,
      "loss": 0.8777,
      "step": 3305
    },
    {
      "epoch": 0.26448,
      "grad_norm": 0.33953243494033813,
      "learning_rate": 0.00018239231897586346,
      "loss": 0.5674,
      "step": 3306
    },
    {
      "epoch": 0.26456,
      "grad_norm": 0.36714300513267517,
      "learning_rate": 0.00018238698493132417,
      "loss": 0.8476,
      "step": 3307
    },
    {
      "epoch": 0.26464,
      "grad_norm": 0.3912853002548218,
      "learning_rate": 0.00018238165088678491,
      "loss": 0.9163,
      "step": 3308
    },
    {
      "epoch": 0.26472,
      "grad_norm": 0.4049445688724518,
      "learning_rate": 0.00018237631684224563,
      "loss": 0.7731,
      "step": 3309
    },
    {
      "epoch": 0.2648,
      "grad_norm": 0.41962969303131104,
      "learning_rate": 0.00018237098279770637,
      "loss": 0.6842,
      "step": 3310
    },
    {
      "epoch": 0.26488,
      "grad_norm": 0.30589842796325684,
      "learning_rate": 0.0001823656487531671,
      "loss": 0.5936,
      "step": 3311
    },
    {
      "epoch": 0.26496,
      "grad_norm": 0.5155957937240601,
      "learning_rate": 0.00018236031470862782,
      "loss": 0.9542,
      "step": 3312
    },
    {
      "epoch": 0.26504,
      "grad_norm": 0.38939887285232544,
      "learning_rate": 0.00018235498066408856,
      "loss": 1.1224,
      "step": 3313
    },
    {
      "epoch": 0.26512,
      "grad_norm": 0.3439973294734955,
      "learning_rate": 0.00018234964661954927,
      "loss": 0.8522,
      "step": 3314
    },
    {
      "epoch": 0.2652,
      "grad_norm": 0.34776031970977783,
      "learning_rate": 0.00018234431257501,
      "loss": 0.8924,
      "step": 3315
    },
    {
      "epoch": 0.26528,
      "grad_norm": 0.5221379399299622,
      "learning_rate": 0.00018233897853047072,
      "loss": 0.9071,
      "step": 3316
    },
    {
      "epoch": 0.26536,
      "grad_norm": 0.34853485226631165,
      "learning_rate": 0.00018233364448593146,
      "loss": 0.5632,
      "step": 3317
    },
    {
      "epoch": 0.26544,
      "grad_norm": 0.3167087137699127,
      "learning_rate": 0.0001823283104413922,
      "loss": 0.8769,
      "step": 3318
    },
    {
      "epoch": 0.26552,
      "grad_norm": 0.3224881589412689,
      "learning_rate": 0.00018232297639685292,
      "loss": 0.8393,
      "step": 3319
    },
    {
      "epoch": 0.2656,
      "grad_norm": 0.39333629608154297,
      "learning_rate": 0.00018231764235231366,
      "loss": 0.8793,
      "step": 3320
    },
    {
      "epoch": 0.26568,
      "grad_norm": 0.2925192415714264,
      "learning_rate": 0.00018231230830777437,
      "loss": 0.7452,
      "step": 3321
    },
    {
      "epoch": 0.26576,
      "grad_norm": 0.3580317497253418,
      "learning_rate": 0.0001823069742632351,
      "loss": 0.9678,
      "step": 3322
    },
    {
      "epoch": 0.26584,
      "grad_norm": 0.26459652185440063,
      "learning_rate": 0.00018230164021869582,
      "loss": 0.6487,
      "step": 3323
    },
    {
      "epoch": 0.26592,
      "grad_norm": 0.4008607268333435,
      "learning_rate": 0.00018229630617415656,
      "loss": 0.9076,
      "step": 3324
    },
    {
      "epoch": 0.266,
      "grad_norm": 0.4482276141643524,
      "learning_rate": 0.0001822909721296173,
      "loss": 0.87,
      "step": 3325
    },
    {
      "epoch": 0.26608,
      "grad_norm": 0.32641610503196716,
      "learning_rate": 0.00018228563808507801,
      "loss": 0.9441,
      "step": 3326
    },
    {
      "epoch": 0.26616,
      "grad_norm": 0.4752231538295746,
      "learning_rate": 0.00018228030404053875,
      "loss": 0.9151,
      "step": 3327
    },
    {
      "epoch": 0.26624,
      "grad_norm": 0.3644947111606598,
      "learning_rate": 0.00018227496999599947,
      "loss": 0.9594,
      "step": 3328
    },
    {
      "epoch": 0.26632,
      "grad_norm": 0.3213736414909363,
      "learning_rate": 0.0001822696359514602,
      "loss": 0.972,
      "step": 3329
    },
    {
      "epoch": 0.2664,
      "grad_norm": 0.44220858812332153,
      "learning_rate": 0.00018226430190692095,
      "loss": 0.7675,
      "step": 3330
    },
    {
      "epoch": 0.26648,
      "grad_norm": 0.38715991377830505,
      "learning_rate": 0.00018225896786238166,
      "loss": 0.9071,
      "step": 3331
    },
    {
      "epoch": 0.26656,
      "grad_norm": 0.3335495889186859,
      "learning_rate": 0.0001822536338178424,
      "loss": 0.8814,
      "step": 3332
    },
    {
      "epoch": 0.26664,
      "grad_norm": 0.37523552775382996,
      "learning_rate": 0.0001822482997733031,
      "loss": 0.6708,
      "step": 3333
    },
    {
      "epoch": 0.26672,
      "grad_norm": 0.3641968071460724,
      "learning_rate": 0.00018224296572876385,
      "loss": 0.8147,
      "step": 3334
    },
    {
      "epoch": 0.2668,
      "grad_norm": 0.4410063326358795,
      "learning_rate": 0.00018223763168422456,
      "loss": 0.7176,
      "step": 3335
    },
    {
      "epoch": 0.26688,
      "grad_norm": 0.33638694882392883,
      "learning_rate": 0.0001822322976396853,
      "loss": 0.6609,
      "step": 3336
    },
    {
      "epoch": 0.26696,
      "grad_norm": 0.3807513415813446,
      "learning_rate": 0.00018222696359514604,
      "loss": 0.892,
      "step": 3337
    },
    {
      "epoch": 0.26704,
      "grad_norm": 0.43198221921920776,
      "learning_rate": 0.00018222162955060676,
      "loss": 0.9947,
      "step": 3338
    },
    {
      "epoch": 0.26712,
      "grad_norm": 0.35775521397590637,
      "learning_rate": 0.0001822162955060675,
      "loss": 0.9915,
      "step": 3339
    },
    {
      "epoch": 0.2672,
      "grad_norm": 0.3397482931613922,
      "learning_rate": 0.0001822109614615282,
      "loss": 1.1223,
      "step": 3340
    },
    {
      "epoch": 0.26728,
      "grad_norm": 0.29715749621391296,
      "learning_rate": 0.00018220562741698895,
      "loss": 0.5837,
      "step": 3341
    },
    {
      "epoch": 0.26736,
      "grad_norm": 0.38626253604888916,
      "learning_rate": 0.00018220029337244966,
      "loss": 0.8165,
      "step": 3342
    },
    {
      "epoch": 0.26744,
      "grad_norm": 0.41793733835220337,
      "learning_rate": 0.0001821949593279104,
      "loss": 0.7814,
      "step": 3343
    },
    {
      "epoch": 0.26752,
      "grad_norm": 0.31596899032592773,
      "learning_rate": 0.00018218962528337114,
      "loss": 1.1248,
      "step": 3344
    },
    {
      "epoch": 0.2676,
      "grad_norm": 0.40706947445869446,
      "learning_rate": 0.00018218429123883185,
      "loss": 0.7227,
      "step": 3345
    },
    {
      "epoch": 0.26768,
      "grad_norm": 0.4146842658519745,
      "learning_rate": 0.0001821789571942926,
      "loss": 0.9959,
      "step": 3346
    },
    {
      "epoch": 0.26776,
      "grad_norm": 0.4305110275745392,
      "learning_rate": 0.0001821736231497533,
      "loss": 1.067,
      "step": 3347
    },
    {
      "epoch": 0.26784,
      "grad_norm": 0.38014525175094604,
      "learning_rate": 0.00018216828910521405,
      "loss": 0.8484,
      "step": 3348
    },
    {
      "epoch": 0.26792,
      "grad_norm": 0.43788906931877136,
      "learning_rate": 0.00018216295506067476,
      "loss": 0.9178,
      "step": 3349
    },
    {
      "epoch": 0.268,
      "grad_norm": 0.40777215361595154,
      "learning_rate": 0.0001821576210161355,
      "loss": 0.7308,
      "step": 3350
    },
    {
      "epoch": 0.26808,
      "grad_norm": 0.267497181892395,
      "learning_rate": 0.00018215228697159624,
      "loss": 0.62,
      "step": 3351
    },
    {
      "epoch": 0.26816,
      "grad_norm": 0.33628323674201965,
      "learning_rate": 0.00018214695292705695,
      "loss": 1.0971,
      "step": 3352
    },
    {
      "epoch": 0.26824,
      "grad_norm": 0.3931926190853119,
      "learning_rate": 0.0001821416188825177,
      "loss": 0.9756,
      "step": 3353
    },
    {
      "epoch": 0.26832,
      "grad_norm": 0.27995288372039795,
      "learning_rate": 0.0001821362848379784,
      "loss": 0.7377,
      "step": 3354
    },
    {
      "epoch": 0.2684,
      "grad_norm": 0.4059363305568695,
      "learning_rate": 0.00018213095079343914,
      "loss": 0.8335,
      "step": 3355
    },
    {
      "epoch": 0.26848,
      "grad_norm": 0.40156853199005127,
      "learning_rate": 0.00018212561674889986,
      "loss": 0.6109,
      "step": 3356
    },
    {
      "epoch": 0.26856,
      "grad_norm": 0.3196850121021271,
      "learning_rate": 0.0001821202827043606,
      "loss": 0.8419,
      "step": 3357
    },
    {
      "epoch": 0.26864,
      "grad_norm": 0.3352672755718231,
      "learning_rate": 0.00018211494865982134,
      "loss": 1.0142,
      "step": 3358
    },
    {
      "epoch": 0.26872,
      "grad_norm": 0.3454095721244812,
      "learning_rate": 0.00018210961461528205,
      "loss": 0.7179,
      "step": 3359
    },
    {
      "epoch": 0.2688,
      "grad_norm": 0.5203295350074768,
      "learning_rate": 0.0001821042805707428,
      "loss": 1.0567,
      "step": 3360
    },
    {
      "epoch": 0.26888,
      "grad_norm": 0.41651734709739685,
      "learning_rate": 0.0001820989465262035,
      "loss": 1.0042,
      "step": 3361
    },
    {
      "epoch": 0.26896,
      "grad_norm": 0.342889666557312,
      "learning_rate": 0.00018209361248166424,
      "loss": 0.7021,
      "step": 3362
    },
    {
      "epoch": 0.26904,
      "grad_norm": 0.3612578809261322,
      "learning_rate": 0.00018208827843712496,
      "loss": 0.7642,
      "step": 3363
    },
    {
      "epoch": 0.26912,
      "grad_norm": 0.43062928318977356,
      "learning_rate": 0.0001820829443925857,
      "loss": 0.8786,
      "step": 3364
    },
    {
      "epoch": 0.2692,
      "grad_norm": 0.31436848640441895,
      "learning_rate": 0.00018207761034804643,
      "loss": 0.8423,
      "step": 3365
    },
    {
      "epoch": 0.26928,
      "grad_norm": 0.25881004333496094,
      "learning_rate": 0.00018207227630350715,
      "loss": 0.4455,
      "step": 3366
    },
    {
      "epoch": 0.26936,
      "grad_norm": 0.4339641332626343,
      "learning_rate": 0.0001820669422589679,
      "loss": 0.9477,
      "step": 3367
    },
    {
      "epoch": 0.26944,
      "grad_norm": 0.3330751955509186,
      "learning_rate": 0.0001820616082144286,
      "loss": 0.9764,
      "step": 3368
    },
    {
      "epoch": 0.26952,
      "grad_norm": 0.3529162108898163,
      "learning_rate": 0.00018205627416988934,
      "loss": 1.0456,
      "step": 3369
    },
    {
      "epoch": 0.2696,
      "grad_norm": 0.4161982238292694,
      "learning_rate": 0.00018205094012535005,
      "loss": 0.9173,
      "step": 3370
    },
    {
      "epoch": 0.26968,
      "grad_norm": 0.4629668891429901,
      "learning_rate": 0.0001820456060808108,
      "loss": 0.8393,
      "step": 3371
    },
    {
      "epoch": 0.26976,
      "grad_norm": 0.29796043038368225,
      "learning_rate": 0.00018204027203627153,
      "loss": 0.7028,
      "step": 3372
    },
    {
      "epoch": 0.26984,
      "grad_norm": 0.35723868012428284,
      "learning_rate": 0.00018203493799173225,
      "loss": 0.7493,
      "step": 3373
    },
    {
      "epoch": 0.26992,
      "grad_norm": 0.39300432801246643,
      "learning_rate": 0.00018202960394719298,
      "loss": 0.9878,
      "step": 3374
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.3937944173812866,
      "learning_rate": 0.0001820242699026537,
      "loss": 0.6925,
      "step": 3375
    },
    {
      "epoch": 0.27008,
      "grad_norm": 0.35050109028816223,
      "learning_rate": 0.00018201893585811444,
      "loss": 1.1978,
      "step": 3376
    },
    {
      "epoch": 0.27016,
      "grad_norm": 0.5867938995361328,
      "learning_rate": 0.00018201360181357515,
      "loss": 1.1289,
      "step": 3377
    },
    {
      "epoch": 0.27024,
      "grad_norm": 0.4637928903102875,
      "learning_rate": 0.0001820082677690359,
      "loss": 0.9945,
      "step": 3378
    },
    {
      "epoch": 0.27032,
      "grad_norm": 0.36438238620758057,
      "learning_rate": 0.0001820029337244966,
      "loss": 1.1127,
      "step": 3379
    },
    {
      "epoch": 0.2704,
      "grad_norm": 0.30788731575012207,
      "learning_rate": 0.00018199759967995734,
      "loss": 0.5092,
      "step": 3380
    },
    {
      "epoch": 0.27048,
      "grad_norm": 0.30827081203460693,
      "learning_rate": 0.00018199226563541806,
      "loss": 0.7298,
      "step": 3381
    },
    {
      "epoch": 0.27056,
      "grad_norm": 0.4419446289539337,
      "learning_rate": 0.0001819869315908788,
      "loss": 0.8933,
      "step": 3382
    },
    {
      "epoch": 0.27064,
      "grad_norm": 0.3579989969730377,
      "learning_rate": 0.00018198159754633954,
      "loss": 1.0978,
      "step": 3383
    },
    {
      "epoch": 0.27072,
      "grad_norm": 0.26943475008010864,
      "learning_rate": 0.00018197626350180025,
      "loss": 0.794,
      "step": 3384
    },
    {
      "epoch": 0.2708,
      "grad_norm": 0.3566514253616333,
      "learning_rate": 0.000181970929457261,
      "loss": 0.9787,
      "step": 3385
    },
    {
      "epoch": 0.27088,
      "grad_norm": 0.35198724269866943,
      "learning_rate": 0.0001819655954127217,
      "loss": 0.7265,
      "step": 3386
    },
    {
      "epoch": 0.27096,
      "grad_norm": 0.45563411712646484,
      "learning_rate": 0.00018196026136818244,
      "loss": 0.7902,
      "step": 3387
    },
    {
      "epoch": 0.27104,
      "grad_norm": 0.35300880670547485,
      "learning_rate": 0.00018195492732364315,
      "loss": 0.7148,
      "step": 3388
    },
    {
      "epoch": 0.27112,
      "grad_norm": 0.35911932587623596,
      "learning_rate": 0.0001819495932791039,
      "loss": 0.6033,
      "step": 3389
    },
    {
      "epoch": 0.2712,
      "grad_norm": 0.44350898265838623,
      "learning_rate": 0.0001819442592345646,
      "loss": 0.8791,
      "step": 3390
    },
    {
      "epoch": 0.27128,
      "grad_norm": 0.3595385253429413,
      "learning_rate": 0.00018193892519002535,
      "loss": 0.5662,
      "step": 3391
    },
    {
      "epoch": 0.27136,
      "grad_norm": 0.4217603802680969,
      "learning_rate": 0.00018193359114548606,
      "loss": 0.745,
      "step": 3392
    },
    {
      "epoch": 0.27144,
      "grad_norm": 0.42866671085357666,
      "learning_rate": 0.0001819282571009468,
      "loss": 0.8257,
      "step": 3393
    },
    {
      "epoch": 0.27152,
      "grad_norm": 0.45654016733169556,
      "learning_rate": 0.0001819229230564075,
      "loss": 0.9334,
      "step": 3394
    },
    {
      "epoch": 0.2716,
      "grad_norm": 0.33175477385520935,
      "learning_rate": 0.00018191758901186825,
      "loss": 0.521,
      "step": 3395
    },
    {
      "epoch": 0.27168,
      "grad_norm": 0.3910754919052124,
      "learning_rate": 0.00018191225496732896,
      "loss": 0.7962,
      "step": 3396
    },
    {
      "epoch": 0.27176,
      "grad_norm": 0.34733346104621887,
      "learning_rate": 0.0001819069209227897,
      "loss": 0.4658,
      "step": 3397
    },
    {
      "epoch": 0.27184,
      "grad_norm": 0.3994654715061188,
      "learning_rate": 0.00018190158687825044,
      "loss": 0.6863,
      "step": 3398
    },
    {
      "epoch": 0.27192,
      "grad_norm": 0.37928009033203125,
      "learning_rate": 0.00018189625283371116,
      "loss": 0.6512,
      "step": 3399
    },
    {
      "epoch": 0.272,
      "grad_norm": 0.35878050327301025,
      "learning_rate": 0.0001818909187891719,
      "loss": 0.8536,
      "step": 3400
    },
    {
      "epoch": 0.27208,
      "grad_norm": 0.4187709391117096,
      "learning_rate": 0.0001818855847446326,
      "loss": 1.1926,
      "step": 3401
    },
    {
      "epoch": 0.27216,
      "grad_norm": 0.4826662242412567,
      "learning_rate": 0.00018188025070009335,
      "loss": 0.8335,
      "step": 3402
    },
    {
      "epoch": 0.27224,
      "grad_norm": 0.44094133377075195,
      "learning_rate": 0.00018187491665555406,
      "loss": 1.1357,
      "step": 3403
    },
    {
      "epoch": 0.27232,
      "grad_norm": 0.4378972053527832,
      "learning_rate": 0.0001818695826110148,
      "loss": 0.7255,
      "step": 3404
    },
    {
      "epoch": 0.2724,
      "grad_norm": 0.3258885145187378,
      "learning_rate": 0.00018186424856647554,
      "loss": 0.712,
      "step": 3405
    },
    {
      "epoch": 0.27248,
      "grad_norm": 0.3266525864601135,
      "learning_rate": 0.00018185891452193625,
      "loss": 0.5859,
      "step": 3406
    },
    {
      "epoch": 0.27256,
      "grad_norm": 0.5303004384040833,
      "learning_rate": 0.000181853580477397,
      "loss": 0.9325,
      "step": 3407
    },
    {
      "epoch": 0.27264,
      "grad_norm": 0.40112578868865967,
      "learning_rate": 0.0001818482464328577,
      "loss": 0.9679,
      "step": 3408
    },
    {
      "epoch": 0.27272,
      "grad_norm": 0.3906879127025604,
      "learning_rate": 0.00018184291238831845,
      "loss": 0.6196,
      "step": 3409
    },
    {
      "epoch": 0.2728,
      "grad_norm": 0.36771464347839355,
      "learning_rate": 0.00018183757834377916,
      "loss": 0.6912,
      "step": 3410
    },
    {
      "epoch": 0.27288,
      "grad_norm": 0.34171921014785767,
      "learning_rate": 0.0001818322442992399,
      "loss": 0.8955,
      "step": 3411
    },
    {
      "epoch": 0.27296,
      "grad_norm": 0.4446648061275482,
      "learning_rate": 0.00018182691025470064,
      "loss": 0.8454,
      "step": 3412
    },
    {
      "epoch": 0.27304,
      "grad_norm": 0.38570982217788696,
      "learning_rate": 0.00018182157621016135,
      "loss": 0.6593,
      "step": 3413
    },
    {
      "epoch": 0.27312,
      "grad_norm": 0.32190340757369995,
      "learning_rate": 0.0001818162421656221,
      "loss": 1.07,
      "step": 3414
    },
    {
      "epoch": 0.2732,
      "grad_norm": 0.3431558907032013,
      "learning_rate": 0.0001818109081210828,
      "loss": 0.779,
      "step": 3415
    },
    {
      "epoch": 0.27328,
      "grad_norm": 0.43782082200050354,
      "learning_rate": 0.00018180557407654354,
      "loss": 1.0173,
      "step": 3416
    },
    {
      "epoch": 0.27336,
      "grad_norm": 0.37647196650505066,
      "learning_rate": 0.00018180024003200426,
      "loss": 1.1102,
      "step": 3417
    },
    {
      "epoch": 0.27344,
      "grad_norm": 0.36100444197654724,
      "learning_rate": 0.000181794905987465,
      "loss": 0.763,
      "step": 3418
    },
    {
      "epoch": 0.27352,
      "grad_norm": 0.4007663130760193,
      "learning_rate": 0.00018178957194292574,
      "loss": 0.9227,
      "step": 3419
    },
    {
      "epoch": 0.2736,
      "grad_norm": 0.4308467209339142,
      "learning_rate": 0.00018178423789838645,
      "loss": 0.6888,
      "step": 3420
    },
    {
      "epoch": 0.27368,
      "grad_norm": 0.42071664333343506,
      "learning_rate": 0.0001817789038538472,
      "loss": 0.8514,
      "step": 3421
    },
    {
      "epoch": 0.27376,
      "grad_norm": 0.408782035112381,
      "learning_rate": 0.0001817735698093079,
      "loss": 0.9677,
      "step": 3422
    },
    {
      "epoch": 0.27384,
      "grad_norm": 0.30026042461395264,
      "learning_rate": 0.00018176823576476864,
      "loss": 0.8443,
      "step": 3423
    },
    {
      "epoch": 0.27392,
      "grad_norm": 0.4153466522693634,
      "learning_rate": 0.00018176290172022935,
      "loss": 0.7687,
      "step": 3424
    },
    {
      "epoch": 0.274,
      "grad_norm": 0.35634246468544006,
      "learning_rate": 0.0001817575676756901,
      "loss": 1.2111,
      "step": 3425
    },
    {
      "epoch": 0.27408,
      "grad_norm": 0.41460880637168884,
      "learning_rate": 0.00018175223363115083,
      "loss": 0.7581,
      "step": 3426
    },
    {
      "epoch": 0.27416,
      "grad_norm": 0.3766135275363922,
      "learning_rate": 0.00018174689958661155,
      "loss": 0.7249,
      "step": 3427
    },
    {
      "epoch": 0.27424,
      "grad_norm": 0.34491652250289917,
      "learning_rate": 0.00018174156554207229,
      "loss": 0.7979,
      "step": 3428
    },
    {
      "epoch": 0.27432,
      "grad_norm": 0.3807297945022583,
      "learning_rate": 0.000181736231497533,
      "loss": 0.6057,
      "step": 3429
    },
    {
      "epoch": 0.2744,
      "grad_norm": 0.32071179151535034,
      "learning_rate": 0.00018173089745299374,
      "loss": 0.4253,
      "step": 3430
    },
    {
      "epoch": 0.27448,
      "grad_norm": 0.4459855556488037,
      "learning_rate": 0.00018172556340845448,
      "loss": 1.1551,
      "step": 3431
    },
    {
      "epoch": 0.27456,
      "grad_norm": 0.32035014033317566,
      "learning_rate": 0.0001817202293639152,
      "loss": 0.7172,
      "step": 3432
    },
    {
      "epoch": 0.27464,
      "grad_norm": 0.45326122641563416,
      "learning_rate": 0.00018171489531937593,
      "loss": 0.7881,
      "step": 3433
    },
    {
      "epoch": 0.27472,
      "grad_norm": 0.42007771134376526,
      "learning_rate": 0.00018170956127483664,
      "loss": 1.2534,
      "step": 3434
    },
    {
      "epoch": 0.2748,
      "grad_norm": 0.4026753306388855,
      "learning_rate": 0.00018170422723029738,
      "loss": 0.8348,
      "step": 3435
    },
    {
      "epoch": 0.27488,
      "grad_norm": 0.31629759073257446,
      "learning_rate": 0.0001816988931857581,
      "loss": 0.5871,
      "step": 3436
    },
    {
      "epoch": 0.27496,
      "grad_norm": 0.46982261538505554,
      "learning_rate": 0.00018169355914121884,
      "loss": 0.9302,
      "step": 3437
    },
    {
      "epoch": 0.27504,
      "grad_norm": 0.33445125818252563,
      "learning_rate": 0.00018168822509667958,
      "loss": 1.3637,
      "step": 3438
    },
    {
      "epoch": 0.27512,
      "grad_norm": 0.319756418466568,
      "learning_rate": 0.0001816828910521403,
      "loss": 0.8539,
      "step": 3439
    },
    {
      "epoch": 0.2752,
      "grad_norm": 0.3333631753921509,
      "learning_rate": 0.00018167755700760103,
      "loss": 0.8156,
      "step": 3440
    },
    {
      "epoch": 0.27528,
      "grad_norm": 0.3920099437236786,
      "learning_rate": 0.00018167222296306174,
      "loss": 0.8134,
      "step": 3441
    },
    {
      "epoch": 0.27536,
      "grad_norm": 0.43117016553878784,
      "learning_rate": 0.00018166688891852248,
      "loss": 0.7629,
      "step": 3442
    },
    {
      "epoch": 0.27544,
      "grad_norm": 0.36223819851875305,
      "learning_rate": 0.0001816615548739832,
      "loss": 0.6828,
      "step": 3443
    },
    {
      "epoch": 0.27552,
      "grad_norm": 0.48217830061912537,
      "learning_rate": 0.00018165622082944393,
      "loss": 1.0898,
      "step": 3444
    },
    {
      "epoch": 0.2756,
      "grad_norm": 0.4225202798843384,
      "learning_rate": 0.00018165088678490467,
      "loss": 0.6426,
      "step": 3445
    },
    {
      "epoch": 0.27568,
      "grad_norm": 0.5284311175346375,
      "learning_rate": 0.0001816455527403654,
      "loss": 1.0666,
      "step": 3446
    },
    {
      "epoch": 0.27576,
      "grad_norm": 0.3574950695037842,
      "learning_rate": 0.00018164021869582613,
      "loss": 0.8305,
      "step": 3447
    },
    {
      "epoch": 0.27584,
      "grad_norm": 0.33532005548477173,
      "learning_rate": 0.00018163488465128684,
      "loss": 0.787,
      "step": 3448
    },
    {
      "epoch": 0.27592,
      "grad_norm": 0.3471525311470032,
      "learning_rate": 0.00018162955060674758,
      "loss": 0.4817,
      "step": 3449
    },
    {
      "epoch": 0.276,
      "grad_norm": 0.3460378050804138,
      "learning_rate": 0.0001816242165622083,
      "loss": 0.8608,
      "step": 3450
    },
    {
      "epoch": 0.27608,
      "grad_norm": 0.47717928886413574,
      "learning_rate": 0.00018161888251766903,
      "loss": 0.8248,
      "step": 3451
    },
    {
      "epoch": 0.27616,
      "grad_norm": 0.4375038743019104,
      "learning_rate": 0.00018161354847312977,
      "loss": 1.1732,
      "step": 3452
    },
    {
      "epoch": 0.27624,
      "grad_norm": 0.4658893048763275,
      "learning_rate": 0.00018160821442859048,
      "loss": 0.889,
      "step": 3453
    },
    {
      "epoch": 0.27632,
      "grad_norm": 0.3002007305622101,
      "learning_rate": 0.00018160288038405122,
      "loss": 0.8684,
      "step": 3454
    },
    {
      "epoch": 0.2764,
      "grad_norm": 0.258492648601532,
      "learning_rate": 0.00018159754633951194,
      "loss": 0.458,
      "step": 3455
    },
    {
      "epoch": 0.27648,
      "grad_norm": 0.3634514808654785,
      "learning_rate": 0.00018159221229497268,
      "loss": 1.0653,
      "step": 3456
    },
    {
      "epoch": 0.27656,
      "grad_norm": 0.29691675305366516,
      "learning_rate": 0.0001815868782504334,
      "loss": 0.7853,
      "step": 3457
    },
    {
      "epoch": 0.27664,
      "grad_norm": 0.34883975982666016,
      "learning_rate": 0.00018158154420589413,
      "loss": 0.6925,
      "step": 3458
    },
    {
      "epoch": 0.27672,
      "grad_norm": 0.25390878319740295,
      "learning_rate": 0.00018157621016135487,
      "loss": 0.757,
      "step": 3459
    },
    {
      "epoch": 0.2768,
      "grad_norm": 0.3065086305141449,
      "learning_rate": 0.00018157087611681558,
      "loss": 0.9609,
      "step": 3460
    },
    {
      "epoch": 0.27688,
      "grad_norm": 0.2874104380607605,
      "learning_rate": 0.00018156554207227632,
      "loss": 0.6026,
      "step": 3461
    },
    {
      "epoch": 0.27696,
      "grad_norm": 0.3384207487106323,
      "learning_rate": 0.00018156020802773703,
      "loss": 0.8651,
      "step": 3462
    },
    {
      "epoch": 0.27704,
      "grad_norm": 0.43009233474731445,
      "learning_rate": 0.00018155487398319777,
      "loss": 1.0543,
      "step": 3463
    },
    {
      "epoch": 0.27712,
      "grad_norm": 0.3704341948032379,
      "learning_rate": 0.0001815495399386585,
      "loss": 0.6779,
      "step": 3464
    },
    {
      "epoch": 0.2772,
      "grad_norm": 0.41024667024612427,
      "learning_rate": 0.00018154420589411923,
      "loss": 1.3274,
      "step": 3465
    },
    {
      "epoch": 0.27728,
      "grad_norm": 0.2787870168685913,
      "learning_rate": 0.00018153887184957997,
      "loss": 0.7225,
      "step": 3466
    },
    {
      "epoch": 0.27736,
      "grad_norm": 0.3860979676246643,
      "learning_rate": 0.00018153353780504068,
      "loss": 0.6547,
      "step": 3467
    },
    {
      "epoch": 0.27744,
      "grad_norm": 0.40938758850097656,
      "learning_rate": 0.00018152820376050142,
      "loss": 0.7665,
      "step": 3468
    },
    {
      "epoch": 0.27752,
      "grad_norm": 0.3075108528137207,
      "learning_rate": 0.00018152286971596213,
      "loss": 0.7198,
      "step": 3469
    },
    {
      "epoch": 0.2776,
      "grad_norm": 0.3726193904876709,
      "learning_rate": 0.00018151753567142287,
      "loss": 0.6447,
      "step": 3470
    },
    {
      "epoch": 0.27768,
      "grad_norm": 0.5775296688079834,
      "learning_rate": 0.00018151220162688358,
      "loss": 0.8591,
      "step": 3471
    },
    {
      "epoch": 0.27776,
      "grad_norm": 0.4280610978603363,
      "learning_rate": 0.00018150686758234432,
      "loss": 0.694,
      "step": 3472
    },
    {
      "epoch": 0.27784,
      "grad_norm": 0.41923993825912476,
      "learning_rate": 0.00018150153353780506,
      "loss": 0.9244,
      "step": 3473
    },
    {
      "epoch": 0.27792,
      "grad_norm": 0.3487941026687622,
      "learning_rate": 0.00018149619949326578,
      "loss": 0.7004,
      "step": 3474
    },
    {
      "epoch": 0.278,
      "grad_norm": 0.30872395634651184,
      "learning_rate": 0.00018149086544872652,
      "loss": 0.7794,
      "step": 3475
    },
    {
      "epoch": 0.27808,
      "grad_norm": 0.4276547431945801,
      "learning_rate": 0.00018148553140418723,
      "loss": 1.0474,
      "step": 3476
    },
    {
      "epoch": 0.27816,
      "grad_norm": 0.32616570591926575,
      "learning_rate": 0.00018148019735964797,
      "loss": 0.9382,
      "step": 3477
    },
    {
      "epoch": 0.27824,
      "grad_norm": 0.41633838415145874,
      "learning_rate": 0.0001814748633151087,
      "loss": 0.823,
      "step": 3478
    },
    {
      "epoch": 0.27832,
      "grad_norm": 0.4059199392795563,
      "learning_rate": 0.00018146952927056942,
      "loss": 0.8767,
      "step": 3479
    },
    {
      "epoch": 0.2784,
      "grad_norm": 0.4165717661380768,
      "learning_rate": 0.00018146419522603016,
      "loss": 0.9324,
      "step": 3480
    },
    {
      "epoch": 0.27848,
      "grad_norm": 0.4351743161678314,
      "learning_rate": 0.00018145886118149087,
      "loss": 0.8255,
      "step": 3481
    },
    {
      "epoch": 0.27856,
      "grad_norm": 0.3327210247516632,
      "learning_rate": 0.00018145352713695161,
      "loss": 1.0602,
      "step": 3482
    },
    {
      "epoch": 0.27864,
      "grad_norm": 0.459683358669281,
      "learning_rate": 0.00018144819309241233,
      "loss": 0.6374,
      "step": 3483
    },
    {
      "epoch": 0.27872,
      "grad_norm": 0.42024439573287964,
      "learning_rate": 0.00018144285904787307,
      "loss": 0.6607,
      "step": 3484
    },
    {
      "epoch": 0.2788,
      "grad_norm": 0.3495276868343353,
      "learning_rate": 0.0001814375250033338,
      "loss": 0.8023,
      "step": 3485
    },
    {
      "epoch": 0.27888,
      "grad_norm": 0.36922240257263184,
      "learning_rate": 0.00018143219095879452,
      "loss": 0.705,
      "step": 3486
    },
    {
      "epoch": 0.27896,
      "grad_norm": 0.41979852318763733,
      "learning_rate": 0.00018142685691425526,
      "loss": 0.9838,
      "step": 3487
    },
    {
      "epoch": 0.27904,
      "grad_norm": 0.40492895245552063,
      "learning_rate": 0.00018142152286971597,
      "loss": 0.6952,
      "step": 3488
    },
    {
      "epoch": 0.27912,
      "grad_norm": 0.33710137009620667,
      "learning_rate": 0.0001814161888251767,
      "loss": 0.9577,
      "step": 3489
    },
    {
      "epoch": 0.2792,
      "grad_norm": 0.2512166500091553,
      "learning_rate": 0.00018141085478063743,
      "loss": 0.6506,
      "step": 3490
    },
    {
      "epoch": 0.27928,
      "grad_norm": 0.3428705632686615,
      "learning_rate": 0.00018140552073609816,
      "loss": 0.6645,
      "step": 3491
    },
    {
      "epoch": 0.27936,
      "grad_norm": 0.29354000091552734,
      "learning_rate": 0.0001814001866915589,
      "loss": 0.7773,
      "step": 3492
    },
    {
      "epoch": 0.27944,
      "grad_norm": 0.334387868642807,
      "learning_rate": 0.00018139485264701962,
      "loss": 0.4781,
      "step": 3493
    },
    {
      "epoch": 0.27952,
      "grad_norm": 0.337155282497406,
      "learning_rate": 0.00018138951860248036,
      "loss": 0.892,
      "step": 3494
    },
    {
      "epoch": 0.2796,
      "grad_norm": 0.38212546706199646,
      "learning_rate": 0.00018138418455794107,
      "loss": 0.637,
      "step": 3495
    },
    {
      "epoch": 0.27968,
      "grad_norm": 0.4660007953643799,
      "learning_rate": 0.0001813788505134018,
      "loss": 0.7489,
      "step": 3496
    },
    {
      "epoch": 0.27976,
      "grad_norm": 0.39320072531700134,
      "learning_rate": 0.00018137351646886252,
      "loss": 0.9188,
      "step": 3497
    },
    {
      "epoch": 0.27984,
      "grad_norm": 0.39271238446235657,
      "learning_rate": 0.00018136818242432326,
      "loss": 0.8408,
      "step": 3498
    },
    {
      "epoch": 0.27992,
      "grad_norm": 0.40358421206474304,
      "learning_rate": 0.000181362848379784,
      "loss": 0.9286,
      "step": 3499
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.3471766710281372,
      "learning_rate": 0.00018135751433524472,
      "loss": 0.7177,
      "step": 3500
    },
    {
      "epoch": 0.28008,
      "grad_norm": 0.4491693079471588,
      "learning_rate": 0.00018135218029070545,
      "loss": 0.5893,
      "step": 3501
    },
    {
      "epoch": 0.28016,
      "grad_norm": 0.33958300948143005,
      "learning_rate": 0.00018134684624616617,
      "loss": 0.9491,
      "step": 3502
    },
    {
      "epoch": 0.28024,
      "grad_norm": 0.3546696603298187,
      "learning_rate": 0.0001813415122016269,
      "loss": 1.0115,
      "step": 3503
    },
    {
      "epoch": 0.28032,
      "grad_norm": 0.40390029549598694,
      "learning_rate": 0.00018133617815708762,
      "loss": 0.5252,
      "step": 3504
    },
    {
      "epoch": 0.2804,
      "grad_norm": 0.32697367668151855,
      "learning_rate": 0.00018133084411254836,
      "loss": 0.7071,
      "step": 3505
    },
    {
      "epoch": 0.28048,
      "grad_norm": 0.3655223548412323,
      "learning_rate": 0.00018132551006800907,
      "loss": 0.9715,
      "step": 3506
    },
    {
      "epoch": 0.28056,
      "grad_norm": 0.4652307331562042,
      "learning_rate": 0.0001813201760234698,
      "loss": 1.2953,
      "step": 3507
    },
    {
      "epoch": 0.28064,
      "grad_norm": 0.2644730806350708,
      "learning_rate": 0.00018131484197893053,
      "loss": 0.408,
      "step": 3508
    },
    {
      "epoch": 0.28072,
      "grad_norm": 0.4013657867908478,
      "learning_rate": 0.00018130950793439127,
      "loss": 0.9616,
      "step": 3509
    },
    {
      "epoch": 0.2808,
      "grad_norm": 0.2918160557746887,
      "learning_rate": 0.00018130417388985198,
      "loss": 0.9679,
      "step": 3510
    },
    {
      "epoch": 0.28088,
      "grad_norm": 0.359537273645401,
      "learning_rate": 0.00018129883984531272,
      "loss": 0.6889,
      "step": 3511
    },
    {
      "epoch": 0.28096,
      "grad_norm": 0.3160485029220581,
      "learning_rate": 0.00018129350580077346,
      "loss": 0.9216,
      "step": 3512
    },
    {
      "epoch": 0.28104,
      "grad_norm": 0.47201457619667053,
      "learning_rate": 0.00018128817175623417,
      "loss": 0.9278,
      "step": 3513
    },
    {
      "epoch": 0.28112,
      "grad_norm": 0.3513953983783722,
      "learning_rate": 0.0001812828377116949,
      "loss": 0.8566,
      "step": 3514
    },
    {
      "epoch": 0.2812,
      "grad_norm": 0.4751488268375397,
      "learning_rate": 0.00018127750366715562,
      "loss": 1.1493,
      "step": 3515
    },
    {
      "epoch": 0.28128,
      "grad_norm": 0.41679081320762634,
      "learning_rate": 0.00018127216962261636,
      "loss": 1.0505,
      "step": 3516
    },
    {
      "epoch": 0.28136,
      "grad_norm": 0.29617464542388916,
      "learning_rate": 0.00018126683557807708,
      "loss": 0.7643,
      "step": 3517
    },
    {
      "epoch": 0.28144,
      "grad_norm": 0.3535061180591583,
      "learning_rate": 0.00018126150153353782,
      "loss": 1.1318,
      "step": 3518
    },
    {
      "epoch": 0.28152,
      "grad_norm": 0.4129575490951538,
      "learning_rate": 0.00018125616748899853,
      "loss": 0.9492,
      "step": 3519
    },
    {
      "epoch": 0.2816,
      "grad_norm": 0.3745056986808777,
      "learning_rate": 0.00018125083344445927,
      "loss": 0.6878,
      "step": 3520
    },
    {
      "epoch": 0.28168,
      "grad_norm": 0.5321784019470215,
      "learning_rate": 0.00018124549939991998,
      "loss": 1.0279,
      "step": 3521
    },
    {
      "epoch": 0.28176,
      "grad_norm": 0.32215559482574463,
      "learning_rate": 0.00018124016535538072,
      "loss": 0.6173,
      "step": 3522
    },
    {
      "epoch": 0.28184,
      "grad_norm": 0.3650752604007721,
      "learning_rate": 0.00018123483131084143,
      "loss": 0.5843,
      "step": 3523
    },
    {
      "epoch": 0.28192,
      "grad_norm": 0.44681140780448914,
      "learning_rate": 0.00018122949726630217,
      "loss": 0.8178,
      "step": 3524
    },
    {
      "epoch": 0.282,
      "grad_norm": 0.42437171936035156,
      "learning_rate": 0.0001812241632217629,
      "loss": 0.7822,
      "step": 3525
    },
    {
      "epoch": 0.28208,
      "grad_norm": 0.3831315040588379,
      "learning_rate": 0.00018121882917722363,
      "loss": 0.7923,
      "step": 3526
    },
    {
      "epoch": 0.28216,
      "grad_norm": 0.31398218870162964,
      "learning_rate": 0.00018121349513268437,
      "loss": 0.7008,
      "step": 3527
    },
    {
      "epoch": 0.28224,
      "grad_norm": 0.39777863025665283,
      "learning_rate": 0.00018120816108814508,
      "loss": 0.8652,
      "step": 3528
    },
    {
      "epoch": 0.28232,
      "grad_norm": 0.4345971345901489,
      "learning_rate": 0.00018120282704360582,
      "loss": 0.7087,
      "step": 3529
    },
    {
      "epoch": 0.2824,
      "grad_norm": 0.3689999282360077,
      "learning_rate": 0.00018119749299906653,
      "loss": 1.1764,
      "step": 3530
    },
    {
      "epoch": 0.28248,
      "grad_norm": 0.35085710883140564,
      "learning_rate": 0.00018119215895452727,
      "loss": 1.0625,
      "step": 3531
    },
    {
      "epoch": 0.28256,
      "grad_norm": 0.3282683789730072,
      "learning_rate": 0.000181186824909988,
      "loss": 1.2053,
      "step": 3532
    },
    {
      "epoch": 0.28264,
      "grad_norm": 0.3804571032524109,
      "learning_rate": 0.00018118149086544872,
      "loss": 1.0109,
      "step": 3533
    },
    {
      "epoch": 0.28272,
      "grad_norm": 0.3030090928077698,
      "learning_rate": 0.00018117615682090946,
      "loss": 0.6694,
      "step": 3534
    },
    {
      "epoch": 0.2828,
      "grad_norm": 0.36359232664108276,
      "learning_rate": 0.00018117082277637018,
      "loss": 0.7808,
      "step": 3535
    },
    {
      "epoch": 0.28288,
      "grad_norm": 0.3837943375110626,
      "learning_rate": 0.00018116548873183092,
      "loss": 1.1164,
      "step": 3536
    },
    {
      "epoch": 0.28296,
      "grad_norm": 0.37057217955589294,
      "learning_rate": 0.00018116015468729163,
      "loss": 0.7924,
      "step": 3537
    },
    {
      "epoch": 0.28304,
      "grad_norm": 0.37284719944000244,
      "learning_rate": 0.00018115482064275237,
      "loss": 0.7436,
      "step": 3538
    },
    {
      "epoch": 0.28312,
      "grad_norm": 0.4343072474002838,
      "learning_rate": 0.0001811494865982131,
      "loss": 0.8123,
      "step": 3539
    },
    {
      "epoch": 0.2832,
      "grad_norm": 0.41807642579078674,
      "learning_rate": 0.00018114415255367382,
      "loss": 0.7094,
      "step": 3540
    },
    {
      "epoch": 0.28328,
      "grad_norm": 0.3293311595916748,
      "learning_rate": 0.00018113881850913456,
      "loss": 0.8504,
      "step": 3541
    },
    {
      "epoch": 0.28336,
      "grad_norm": 0.2964651584625244,
      "learning_rate": 0.00018113348446459527,
      "loss": 0.6124,
      "step": 3542
    },
    {
      "epoch": 0.28344,
      "grad_norm": 0.40068376064300537,
      "learning_rate": 0.000181128150420056,
      "loss": 1.002,
      "step": 3543
    },
    {
      "epoch": 0.28352,
      "grad_norm": 0.43495339155197144,
      "learning_rate": 0.00018112281637551673,
      "loss": 0.9492,
      "step": 3544
    },
    {
      "epoch": 0.2836,
      "grad_norm": 0.37543851137161255,
      "learning_rate": 0.00018111748233097747,
      "loss": 0.7542,
      "step": 3545
    },
    {
      "epoch": 0.28368,
      "grad_norm": 0.33220598101615906,
      "learning_rate": 0.0001811121482864382,
      "loss": 0.658,
      "step": 3546
    },
    {
      "epoch": 0.28376,
      "grad_norm": 0.41951295733451843,
      "learning_rate": 0.00018110681424189892,
      "loss": 1.0322,
      "step": 3547
    },
    {
      "epoch": 0.28384,
      "grad_norm": 0.3056257665157318,
      "learning_rate": 0.00018110148019735966,
      "loss": 0.5075,
      "step": 3548
    },
    {
      "epoch": 0.28392,
      "grad_norm": 0.30347245931625366,
      "learning_rate": 0.00018109614615282037,
      "loss": 0.4541,
      "step": 3549
    },
    {
      "epoch": 0.284,
      "grad_norm": 0.3036685883998871,
      "learning_rate": 0.0001810908121082811,
      "loss": 0.7407,
      "step": 3550
    },
    {
      "epoch": 0.28408,
      "grad_norm": 0.4122953414916992,
      "learning_rate": 0.00018108547806374182,
      "loss": 0.9112,
      "step": 3551
    },
    {
      "epoch": 0.28416,
      "grad_norm": 0.2542835772037506,
      "learning_rate": 0.00018108014401920256,
      "loss": 0.7715,
      "step": 3552
    },
    {
      "epoch": 0.28424,
      "grad_norm": 0.393767386674881,
      "learning_rate": 0.0001810748099746633,
      "loss": 0.8788,
      "step": 3553
    },
    {
      "epoch": 0.28432,
      "grad_norm": 0.4184936285018921,
      "learning_rate": 0.00018106947593012402,
      "loss": 0.8422,
      "step": 3554
    },
    {
      "epoch": 0.2844,
      "grad_norm": 0.30124425888061523,
      "learning_rate": 0.00018106414188558476,
      "loss": 0.7317,
      "step": 3555
    },
    {
      "epoch": 0.28448,
      "grad_norm": 0.3958966135978699,
      "learning_rate": 0.00018105880784104547,
      "loss": 0.4977,
      "step": 3556
    },
    {
      "epoch": 0.28456,
      "grad_norm": 0.4088621437549591,
      "learning_rate": 0.0001810534737965062,
      "loss": 1.1844,
      "step": 3557
    },
    {
      "epoch": 0.28464,
      "grad_norm": 0.37032565474510193,
      "learning_rate": 0.00018104813975196692,
      "loss": 0.863,
      "step": 3558
    },
    {
      "epoch": 0.28472,
      "grad_norm": 0.2757079303264618,
      "learning_rate": 0.00018104280570742766,
      "loss": 0.5213,
      "step": 3559
    },
    {
      "epoch": 0.2848,
      "grad_norm": 0.34021928906440735,
      "learning_rate": 0.0001810374716628884,
      "loss": 1.0156,
      "step": 3560
    },
    {
      "epoch": 0.28488,
      "grad_norm": 0.370288223028183,
      "learning_rate": 0.00018103213761834911,
      "loss": 0.9304,
      "step": 3561
    },
    {
      "epoch": 0.28496,
      "grad_norm": 0.3507746160030365,
      "learning_rate": 0.00018102680357380985,
      "loss": 0.6409,
      "step": 3562
    },
    {
      "epoch": 0.28504,
      "grad_norm": 0.3834478557109833,
      "learning_rate": 0.00018102146952927057,
      "loss": 0.8622,
      "step": 3563
    },
    {
      "epoch": 0.28512,
      "grad_norm": 0.38426515460014343,
      "learning_rate": 0.0001810161354847313,
      "loss": 0.6454,
      "step": 3564
    },
    {
      "epoch": 0.2852,
      "grad_norm": 0.4851312041282654,
      "learning_rate": 0.00018101080144019202,
      "loss": 0.9019,
      "step": 3565
    },
    {
      "epoch": 0.28528,
      "grad_norm": 0.36925414204597473,
      "learning_rate": 0.00018100546739565276,
      "loss": 1.0559,
      "step": 3566
    },
    {
      "epoch": 0.28536,
      "grad_norm": 0.3637241721153259,
      "learning_rate": 0.0001810001333511135,
      "loss": 0.7053,
      "step": 3567
    },
    {
      "epoch": 0.28544,
      "grad_norm": 0.4113733172416687,
      "learning_rate": 0.0001809947993065742,
      "loss": 1.052,
      "step": 3568
    },
    {
      "epoch": 0.28552,
      "grad_norm": 0.45649048686027527,
      "learning_rate": 0.00018098946526203495,
      "loss": 1.0041,
      "step": 3569
    },
    {
      "epoch": 0.2856,
      "grad_norm": 0.2875659763813019,
      "learning_rate": 0.00018098413121749566,
      "loss": 0.9504,
      "step": 3570
    },
    {
      "epoch": 0.28568,
      "grad_norm": 0.30822253227233887,
      "learning_rate": 0.0001809787971729564,
      "loss": 0.6669,
      "step": 3571
    },
    {
      "epoch": 0.28576,
      "grad_norm": 0.33570045232772827,
      "learning_rate": 0.00018097346312841712,
      "loss": 0.7078,
      "step": 3572
    },
    {
      "epoch": 0.28584,
      "grad_norm": 0.3591185212135315,
      "learning_rate": 0.00018096812908387786,
      "loss": 0.8811,
      "step": 3573
    },
    {
      "epoch": 0.28592,
      "grad_norm": 0.3527224063873291,
      "learning_rate": 0.0001809627950393386,
      "loss": 0.8125,
      "step": 3574
    },
    {
      "epoch": 0.286,
      "grad_norm": 0.3467195928096771,
      "learning_rate": 0.0001809574609947993,
      "loss": 0.6531,
      "step": 3575
    },
    {
      "epoch": 0.28608,
      "grad_norm": 0.4795660376548767,
      "learning_rate": 0.00018095212695026005,
      "loss": 1.1387,
      "step": 3576
    },
    {
      "epoch": 0.28616,
      "grad_norm": 0.36468306183815,
      "learning_rate": 0.00018094679290572076,
      "loss": 0.8782,
      "step": 3577
    },
    {
      "epoch": 0.28624,
      "grad_norm": 0.4207046926021576,
      "learning_rate": 0.0001809414588611815,
      "loss": 0.8658,
      "step": 3578
    },
    {
      "epoch": 0.28632,
      "grad_norm": 0.37665805220603943,
      "learning_rate": 0.00018093612481664224,
      "loss": 1.0954,
      "step": 3579
    },
    {
      "epoch": 0.2864,
      "grad_norm": 0.3946579098701477,
      "learning_rate": 0.00018093079077210295,
      "loss": 0.866,
      "step": 3580
    },
    {
      "epoch": 0.28648,
      "grad_norm": 0.3204071521759033,
      "learning_rate": 0.0001809254567275637,
      "loss": 0.9465,
      "step": 3581
    },
    {
      "epoch": 0.28656,
      "grad_norm": 0.43744200468063354,
      "learning_rate": 0.0001809201226830244,
      "loss": 0.9763,
      "step": 3582
    },
    {
      "epoch": 0.28664,
      "grad_norm": 0.3438713550567627,
      "learning_rate": 0.00018091478863848515,
      "loss": 0.9465,
      "step": 3583
    },
    {
      "epoch": 0.28672,
      "grad_norm": 0.3502422273159027,
      "learning_rate": 0.00018090945459394586,
      "loss": 0.5853,
      "step": 3584
    },
    {
      "epoch": 0.2868,
      "grad_norm": 0.44837966561317444,
      "learning_rate": 0.0001809041205494066,
      "loss": 0.8663,
      "step": 3585
    },
    {
      "epoch": 0.28688,
      "grad_norm": 0.3181641697883606,
      "learning_rate": 0.00018089878650486734,
      "loss": 1.2226,
      "step": 3586
    },
    {
      "epoch": 0.28696,
      "grad_norm": 0.42470037937164307,
      "learning_rate": 0.00018089345246032805,
      "loss": 0.9182,
      "step": 3587
    },
    {
      "epoch": 0.28704,
      "grad_norm": 0.3405546545982361,
      "learning_rate": 0.0001808881184157888,
      "loss": 0.6963,
      "step": 3588
    },
    {
      "epoch": 0.28712,
      "grad_norm": 0.37605249881744385,
      "learning_rate": 0.0001808827843712495,
      "loss": 0.8695,
      "step": 3589
    },
    {
      "epoch": 0.2872,
      "grad_norm": 0.45549190044403076,
      "learning_rate": 0.00018087745032671024,
      "loss": 1.109,
      "step": 3590
    },
    {
      "epoch": 0.28728,
      "grad_norm": 0.34182482957839966,
      "learning_rate": 0.00018087211628217096,
      "loss": 0.7571,
      "step": 3591
    },
    {
      "epoch": 0.28736,
      "grad_norm": 0.5207125544548035,
      "learning_rate": 0.0001808667822376317,
      "loss": 1.0367,
      "step": 3592
    },
    {
      "epoch": 0.28744,
      "grad_norm": 0.36489737033843994,
      "learning_rate": 0.00018086144819309244,
      "loss": 1.1281,
      "step": 3593
    },
    {
      "epoch": 0.28752,
      "grad_norm": 0.414013147354126,
      "learning_rate": 0.00018085611414855315,
      "loss": 0.8535,
      "step": 3594
    },
    {
      "epoch": 0.2876,
      "grad_norm": 0.30932340025901794,
      "learning_rate": 0.0001808507801040139,
      "loss": 0.8225,
      "step": 3595
    },
    {
      "epoch": 0.28768,
      "grad_norm": 0.3193642795085907,
      "learning_rate": 0.0001808454460594746,
      "loss": 1.0084,
      "step": 3596
    },
    {
      "epoch": 0.28776,
      "grad_norm": 0.29835331439971924,
      "learning_rate": 0.00018084011201493534,
      "loss": 1.0498,
      "step": 3597
    },
    {
      "epoch": 0.28784,
      "grad_norm": 0.3393803536891937,
      "learning_rate": 0.00018083477797039605,
      "loss": 0.7088,
      "step": 3598
    },
    {
      "epoch": 0.28792,
      "grad_norm": 0.3107988238334656,
      "learning_rate": 0.0001808294439258568,
      "loss": 0.6628,
      "step": 3599
    },
    {
      "epoch": 0.288,
      "grad_norm": 0.41693970561027527,
      "learning_rate": 0.00018082410988131753,
      "loss": 0.7,
      "step": 3600
    },
    {
      "epoch": 0.28808,
      "grad_norm": 0.3522041440010071,
      "learning_rate": 0.00018081877583677825,
      "loss": 1.0822,
      "step": 3601
    },
    {
      "epoch": 0.28816,
      "grad_norm": 0.31423065066337585,
      "learning_rate": 0.000180813441792239,
      "loss": 0.9316,
      "step": 3602
    },
    {
      "epoch": 0.28824,
      "grad_norm": 0.39239633083343506,
      "learning_rate": 0.0001808081077476997,
      "loss": 1.3976,
      "step": 3603
    },
    {
      "epoch": 0.28832,
      "grad_norm": 0.35589179396629333,
      "learning_rate": 0.00018080277370316044,
      "loss": 0.8371,
      "step": 3604
    },
    {
      "epoch": 0.2884,
      "grad_norm": 0.4227427840232849,
      "learning_rate": 0.00018079743965862115,
      "loss": 0.5263,
      "step": 3605
    },
    {
      "epoch": 0.28848,
      "grad_norm": 0.43660593032836914,
      "learning_rate": 0.0001807921056140819,
      "loss": 1.0739,
      "step": 3606
    },
    {
      "epoch": 0.28856,
      "grad_norm": 0.33474382758140564,
      "learning_rate": 0.00018078677156954263,
      "loss": 0.8557,
      "step": 3607
    },
    {
      "epoch": 0.28864,
      "grad_norm": 0.3890514075756073,
      "learning_rate": 0.00018078143752500334,
      "loss": 0.6711,
      "step": 3608
    },
    {
      "epoch": 0.28872,
      "grad_norm": 0.4379754960536957,
      "learning_rate": 0.00018077610348046408,
      "loss": 0.9298,
      "step": 3609
    },
    {
      "epoch": 0.2888,
      "grad_norm": 0.3950604498386383,
      "learning_rate": 0.0001807707694359248,
      "loss": 0.8824,
      "step": 3610
    },
    {
      "epoch": 0.28888,
      "grad_norm": 0.38138675689697266,
      "learning_rate": 0.00018076543539138554,
      "loss": 0.8029,
      "step": 3611
    },
    {
      "epoch": 0.28896,
      "grad_norm": 0.32160234451293945,
      "learning_rate": 0.00018076010134684625,
      "loss": 0.5333,
      "step": 3612
    },
    {
      "epoch": 0.28904,
      "grad_norm": 0.3719450831413269,
      "learning_rate": 0.000180754767302307,
      "loss": 0.693,
      "step": 3613
    },
    {
      "epoch": 0.28912,
      "grad_norm": 0.40836283564567566,
      "learning_rate": 0.00018074943325776773,
      "loss": 0.6378,
      "step": 3614
    },
    {
      "epoch": 0.2892,
      "grad_norm": 0.28601381182670593,
      "learning_rate": 0.00018074409921322844,
      "loss": 0.6328,
      "step": 3615
    },
    {
      "epoch": 0.28928,
      "grad_norm": 0.3437483310699463,
      "learning_rate": 0.00018073876516868918,
      "loss": 0.9108,
      "step": 3616
    },
    {
      "epoch": 0.28936,
      "grad_norm": 0.3850967288017273,
      "learning_rate": 0.0001807334311241499,
      "loss": 0.7825,
      "step": 3617
    },
    {
      "epoch": 0.28944,
      "grad_norm": 0.42962074279785156,
      "learning_rate": 0.00018072809707961063,
      "loss": 0.912,
      "step": 3618
    },
    {
      "epoch": 0.28952,
      "grad_norm": 0.3281242251396179,
      "learning_rate": 0.00018072276303507135,
      "loss": 0.4959,
      "step": 3619
    },
    {
      "epoch": 0.2896,
      "grad_norm": 0.3585922122001648,
      "learning_rate": 0.0001807174289905321,
      "loss": 0.7597,
      "step": 3620
    },
    {
      "epoch": 0.28968,
      "grad_norm": 0.4174376428127289,
      "learning_rate": 0.00018071209494599283,
      "loss": 0.9845,
      "step": 3621
    },
    {
      "epoch": 0.28976,
      "grad_norm": 0.2805998623371124,
      "learning_rate": 0.00018070676090145354,
      "loss": 0.8071,
      "step": 3622
    },
    {
      "epoch": 0.28984,
      "grad_norm": 0.4081929624080658,
      "learning_rate": 0.00018070142685691428,
      "loss": 0.9429,
      "step": 3623
    },
    {
      "epoch": 0.28992,
      "grad_norm": 0.41054174304008484,
      "learning_rate": 0.000180696092812375,
      "loss": 0.6419,
      "step": 3624
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.3341739773750305,
      "learning_rate": 0.00018069075876783573,
      "loss": 0.8303,
      "step": 3625
    },
    {
      "epoch": 0.29008,
      "grad_norm": 0.513130784034729,
      "learning_rate": 0.00018068542472329647,
      "loss": 0.6413,
      "step": 3626
    },
    {
      "epoch": 0.29016,
      "grad_norm": 0.3692682981491089,
      "learning_rate": 0.00018068009067875719,
      "loss": 1.2258,
      "step": 3627
    },
    {
      "epoch": 0.29024,
      "grad_norm": 0.29697924852371216,
      "learning_rate": 0.00018067475663421792,
      "loss": 1.0139,
      "step": 3628
    },
    {
      "epoch": 0.29032,
      "grad_norm": 0.292503297328949,
      "learning_rate": 0.00018066942258967864,
      "loss": 0.9833,
      "step": 3629
    },
    {
      "epoch": 0.2904,
      "grad_norm": 0.35752660036087036,
      "learning_rate": 0.00018066408854513938,
      "loss": 0.976,
      "step": 3630
    },
    {
      "epoch": 0.29048,
      "grad_norm": 0.3928700089454651,
      "learning_rate": 0.0001806587545006001,
      "loss": 0.7207,
      "step": 3631
    },
    {
      "epoch": 0.29056,
      "grad_norm": 0.36007583141326904,
      "learning_rate": 0.00018065342045606083,
      "loss": 0.8588,
      "step": 3632
    },
    {
      "epoch": 0.29064,
      "grad_norm": 0.3401169776916504,
      "learning_rate": 0.00018064808641152154,
      "loss": 0.6671,
      "step": 3633
    },
    {
      "epoch": 0.29072,
      "grad_norm": 0.30814215540885925,
      "learning_rate": 0.00018064275236698228,
      "loss": 0.6485,
      "step": 3634
    },
    {
      "epoch": 0.2908,
      "grad_norm": 0.4265107810497284,
      "learning_rate": 0.000180637418322443,
      "loss": 0.8623,
      "step": 3635
    },
    {
      "epoch": 0.29088,
      "grad_norm": 0.349822074174881,
      "learning_rate": 0.00018063208427790374,
      "loss": 0.5185,
      "step": 3636
    },
    {
      "epoch": 0.29096,
      "grad_norm": 0.3258581757545471,
      "learning_rate": 0.00018062675023336445,
      "loss": 0.863,
      "step": 3637
    },
    {
      "epoch": 0.29104,
      "grad_norm": 0.3408251106739044,
      "learning_rate": 0.0001806214161888252,
      "loss": 0.6511,
      "step": 3638
    },
    {
      "epoch": 0.29112,
      "grad_norm": 0.3665101230144501,
      "learning_rate": 0.00018061608214428593,
      "loss": 0.8911,
      "step": 3639
    },
    {
      "epoch": 0.2912,
      "grad_norm": 0.3888396620750427,
      "learning_rate": 0.00018061074809974664,
      "loss": 0.6995,
      "step": 3640
    },
    {
      "epoch": 0.29128,
      "grad_norm": 0.42349371314048767,
      "learning_rate": 0.00018060541405520738,
      "loss": 1.1608,
      "step": 3641
    },
    {
      "epoch": 0.29136,
      "grad_norm": 0.39169031381607056,
      "learning_rate": 0.0001806000800106681,
      "loss": 0.6342,
      "step": 3642
    },
    {
      "epoch": 0.29144,
      "grad_norm": 0.29919689893722534,
      "learning_rate": 0.00018059474596612883,
      "loss": 0.6688,
      "step": 3643
    },
    {
      "epoch": 0.29152,
      "grad_norm": 0.37057024240493774,
      "learning_rate": 0.00018058941192158955,
      "loss": 0.9265,
      "step": 3644
    },
    {
      "epoch": 0.2916,
      "grad_norm": 0.43443968892097473,
      "learning_rate": 0.00018058407787705029,
      "loss": 0.722,
      "step": 3645
    },
    {
      "epoch": 0.29168,
      "grad_norm": 0.43517714738845825,
      "learning_rate": 0.000180578743832511,
      "loss": 0.9215,
      "step": 3646
    },
    {
      "epoch": 0.29176,
      "grad_norm": 0.35500454902648926,
      "learning_rate": 0.00018057340978797174,
      "loss": 0.7563,
      "step": 3647
    },
    {
      "epoch": 0.29184,
      "grad_norm": 0.48710760474205017,
      "learning_rate": 0.00018056807574343245,
      "loss": 0.777,
      "step": 3648
    },
    {
      "epoch": 0.29192,
      "grad_norm": 0.44954031705856323,
      "learning_rate": 0.0001805627416988932,
      "loss": 1.0849,
      "step": 3649
    },
    {
      "epoch": 0.292,
      "grad_norm": 0.42526930570602417,
      "learning_rate": 0.0001805574076543539,
      "loss": 0.9097,
      "step": 3650
    },
    {
      "epoch": 0.29208,
      "grad_norm": 0.24318534135818481,
      "learning_rate": 0.00018055207360981464,
      "loss": 0.7994,
      "step": 3651
    },
    {
      "epoch": 0.29216,
      "grad_norm": 0.3591676652431488,
      "learning_rate": 0.00018054673956527536,
      "loss": 0.6771,
      "step": 3652
    },
    {
      "epoch": 0.29224,
      "grad_norm": 0.36741605401039124,
      "learning_rate": 0.0001805414055207361,
      "loss": 0.6569,
      "step": 3653
    },
    {
      "epoch": 0.29232,
      "grad_norm": 0.43698057532310486,
      "learning_rate": 0.00018053607147619684,
      "loss": 0.8191,
      "step": 3654
    },
    {
      "epoch": 0.2924,
      "grad_norm": 0.33318179845809937,
      "learning_rate": 0.00018053073743165755,
      "loss": 0.627,
      "step": 3655
    },
    {
      "epoch": 0.29248,
      "grad_norm": 0.3238365948200226,
      "learning_rate": 0.0001805254033871183,
      "loss": 1.0561,
      "step": 3656
    },
    {
      "epoch": 0.29256,
      "grad_norm": 0.381930410861969,
      "learning_rate": 0.000180520069342579,
      "loss": 0.6849,
      "step": 3657
    },
    {
      "epoch": 0.29264,
      "grad_norm": 0.3902406394481659,
      "learning_rate": 0.00018051473529803974,
      "loss": 0.8071,
      "step": 3658
    },
    {
      "epoch": 0.29272,
      "grad_norm": 0.3349408507347107,
      "learning_rate": 0.00018050940125350045,
      "loss": 0.7741,
      "step": 3659
    },
    {
      "epoch": 0.2928,
      "grad_norm": 0.4190385043621063,
      "learning_rate": 0.0001805040672089612,
      "loss": 1.1677,
      "step": 3660
    },
    {
      "epoch": 0.29288,
      "grad_norm": 0.46550253033638,
      "learning_rate": 0.00018049873316442193,
      "loss": 1.1685,
      "step": 3661
    },
    {
      "epoch": 0.29296,
      "grad_norm": 0.3349822759628296,
      "learning_rate": 0.00018049339911988265,
      "loss": 1.0046,
      "step": 3662
    },
    {
      "epoch": 0.29304,
      "grad_norm": 0.42894452810287476,
      "learning_rate": 0.00018048806507534339,
      "loss": 0.9188,
      "step": 3663
    },
    {
      "epoch": 0.29312,
      "grad_norm": 0.345018595457077,
      "learning_rate": 0.0001804827310308041,
      "loss": 0.7789,
      "step": 3664
    },
    {
      "epoch": 0.2932,
      "grad_norm": 0.49263420701026917,
      "learning_rate": 0.00018047739698626484,
      "loss": 0.6578,
      "step": 3665
    },
    {
      "epoch": 0.29328,
      "grad_norm": 0.363952100276947,
      "learning_rate": 0.00018047206294172555,
      "loss": 0.7316,
      "step": 3666
    },
    {
      "epoch": 0.29336,
      "grad_norm": 0.36041441559791565,
      "learning_rate": 0.0001804667288971863,
      "loss": 1.01,
      "step": 3667
    },
    {
      "epoch": 0.29344,
      "grad_norm": 0.46978169679641724,
      "learning_rate": 0.00018046139485264703,
      "loss": 1.0846,
      "step": 3668
    },
    {
      "epoch": 0.29352,
      "grad_norm": 0.42103588581085205,
      "learning_rate": 0.00018045606080810774,
      "loss": 0.9272,
      "step": 3669
    },
    {
      "epoch": 0.2936,
      "grad_norm": 0.34874528646469116,
      "learning_rate": 0.00018045072676356848,
      "loss": 0.6443,
      "step": 3670
    },
    {
      "epoch": 0.29368,
      "grad_norm": 0.3887890577316284,
      "learning_rate": 0.0001804453927190292,
      "loss": 0.8371,
      "step": 3671
    },
    {
      "epoch": 0.29376,
      "grad_norm": 0.4122401475906372,
      "learning_rate": 0.00018044005867448994,
      "loss": 0.7593,
      "step": 3672
    },
    {
      "epoch": 0.29384,
      "grad_norm": 0.4785245954990387,
      "learning_rate": 0.00018043472462995065,
      "loss": 1.0095,
      "step": 3673
    },
    {
      "epoch": 0.29392,
      "grad_norm": 0.2818450629711151,
      "learning_rate": 0.0001804293905854114,
      "loss": 0.5755,
      "step": 3674
    },
    {
      "epoch": 0.294,
      "grad_norm": 0.629889965057373,
      "learning_rate": 0.00018042405654087213,
      "loss": 0.9834,
      "step": 3675
    },
    {
      "epoch": 0.29408,
      "grad_norm": 0.3864484429359436,
      "learning_rate": 0.00018041872249633284,
      "loss": 0.7234,
      "step": 3676
    },
    {
      "epoch": 0.29416,
      "grad_norm": 0.3950803279876709,
      "learning_rate": 0.00018041338845179358,
      "loss": 0.8133,
      "step": 3677
    },
    {
      "epoch": 0.29424,
      "grad_norm": 0.48068922758102417,
      "learning_rate": 0.0001804080544072543,
      "loss": 0.9221,
      "step": 3678
    },
    {
      "epoch": 0.29432,
      "grad_norm": 0.4176855683326721,
      "learning_rate": 0.00018040272036271503,
      "loss": 0.7891,
      "step": 3679
    },
    {
      "epoch": 0.2944,
      "grad_norm": 0.3955657482147217,
      "learning_rate": 0.00018039738631817577,
      "loss": 0.7705,
      "step": 3680
    },
    {
      "epoch": 0.29448,
      "grad_norm": 0.3478740453720093,
      "learning_rate": 0.00018039205227363649,
      "loss": 0.5083,
      "step": 3681
    },
    {
      "epoch": 0.29456,
      "grad_norm": 0.703918993473053,
      "learning_rate": 0.00018038671822909723,
      "loss": 0.9356,
      "step": 3682
    },
    {
      "epoch": 0.29464,
      "grad_norm": 0.31981325149536133,
      "learning_rate": 0.00018038138418455794,
      "loss": 0.6085,
      "step": 3683
    },
    {
      "epoch": 0.29472,
      "grad_norm": 0.2832711637020111,
      "learning_rate": 0.00018037605014001868,
      "loss": 0.7066,
      "step": 3684
    },
    {
      "epoch": 0.2948,
      "grad_norm": 0.47484090924263,
      "learning_rate": 0.0001803707160954794,
      "loss": 0.6892,
      "step": 3685
    },
    {
      "epoch": 0.29488,
      "grad_norm": 0.36365070939064026,
      "learning_rate": 0.00018036538205094013,
      "loss": 0.9452,
      "step": 3686
    },
    {
      "epoch": 0.29496,
      "grad_norm": 0.4485444128513336,
      "learning_rate": 0.00018036004800640087,
      "loss": 0.8727,
      "step": 3687
    },
    {
      "epoch": 0.29504,
      "grad_norm": 0.28421658277511597,
      "learning_rate": 0.00018035471396186158,
      "loss": 0.7862,
      "step": 3688
    },
    {
      "epoch": 0.29512,
      "grad_norm": 0.4194876551628113,
      "learning_rate": 0.00018034937991732232,
      "loss": 0.8366,
      "step": 3689
    },
    {
      "epoch": 0.2952,
      "grad_norm": 0.359697163105011,
      "learning_rate": 0.00018034404587278304,
      "loss": 0.7849,
      "step": 3690
    },
    {
      "epoch": 0.29528,
      "grad_norm": 0.3310885429382324,
      "learning_rate": 0.00018033871182824378,
      "loss": 0.9879,
      "step": 3691
    },
    {
      "epoch": 0.29536,
      "grad_norm": 0.32182788848876953,
      "learning_rate": 0.0001803333777837045,
      "loss": 0.7207,
      "step": 3692
    },
    {
      "epoch": 0.29544,
      "grad_norm": 0.38503479957580566,
      "learning_rate": 0.00018032804373916523,
      "loss": 0.9823,
      "step": 3693
    },
    {
      "epoch": 0.29552,
      "grad_norm": 0.3627740740776062,
      "learning_rate": 0.00018032270969462597,
      "loss": 0.6452,
      "step": 3694
    },
    {
      "epoch": 0.2956,
      "grad_norm": 0.3847056031227112,
      "learning_rate": 0.00018031737565008668,
      "loss": 1.0799,
      "step": 3695
    },
    {
      "epoch": 0.29568,
      "grad_norm": 0.3633497357368469,
      "learning_rate": 0.00018031204160554742,
      "loss": 0.9215,
      "step": 3696
    },
    {
      "epoch": 0.29576,
      "grad_norm": 0.43047454953193665,
      "learning_rate": 0.00018030670756100813,
      "loss": 0.9744,
      "step": 3697
    },
    {
      "epoch": 0.29584,
      "grad_norm": 0.4156504273414612,
      "learning_rate": 0.00018030137351646887,
      "loss": 0.7708,
      "step": 3698
    },
    {
      "epoch": 0.29592,
      "grad_norm": 0.34482747316360474,
      "learning_rate": 0.0001802960394719296,
      "loss": 0.9033,
      "step": 3699
    },
    {
      "epoch": 0.296,
      "grad_norm": 0.3398917615413666,
      "learning_rate": 0.00018029070542739033,
      "loss": 0.5904,
      "step": 3700
    },
    {
      "epoch": 0.29608,
      "grad_norm": 0.35359132289886475,
      "learning_rate": 0.00018028537138285107,
      "loss": 0.4958,
      "step": 3701
    },
    {
      "epoch": 0.29616,
      "grad_norm": 0.35518938302993774,
      "learning_rate": 0.00018028003733831178,
      "loss": 0.6814,
      "step": 3702
    },
    {
      "epoch": 0.29624,
      "grad_norm": 0.3839164972305298,
      "learning_rate": 0.00018027470329377252,
      "loss": 0.8885,
      "step": 3703
    },
    {
      "epoch": 0.29632,
      "grad_norm": 0.3042689859867096,
      "learning_rate": 0.00018026936924923323,
      "loss": 0.8525,
      "step": 3704
    },
    {
      "epoch": 0.2964,
      "grad_norm": 0.3894856870174408,
      "learning_rate": 0.00018026403520469397,
      "loss": 0.8543,
      "step": 3705
    },
    {
      "epoch": 0.29648,
      "grad_norm": 0.2913530170917511,
      "learning_rate": 0.00018025870116015468,
      "loss": 0.7919,
      "step": 3706
    },
    {
      "epoch": 0.29656,
      "grad_norm": 0.5612819790840149,
      "learning_rate": 0.00018025336711561542,
      "loss": 1.114,
      "step": 3707
    },
    {
      "epoch": 0.29664,
      "grad_norm": 0.35118675231933594,
      "learning_rate": 0.00018024803307107616,
      "loss": 1.1392,
      "step": 3708
    },
    {
      "epoch": 0.29672,
      "grad_norm": 0.32691553235054016,
      "learning_rate": 0.00018024269902653688,
      "loss": 0.9677,
      "step": 3709
    },
    {
      "epoch": 0.2968,
      "grad_norm": 0.4122696816921234,
      "learning_rate": 0.00018023736498199762,
      "loss": 0.8321,
      "step": 3710
    },
    {
      "epoch": 0.29688,
      "grad_norm": 0.30624157190322876,
      "learning_rate": 0.00018023203093745833,
      "loss": 0.5765,
      "step": 3711
    },
    {
      "epoch": 0.29696,
      "grad_norm": 0.4991063177585602,
      "learning_rate": 0.00018022669689291907,
      "loss": 0.7141,
      "step": 3712
    },
    {
      "epoch": 0.29704,
      "grad_norm": 0.39177000522613525,
      "learning_rate": 0.00018022136284837978,
      "loss": 1.1002,
      "step": 3713
    },
    {
      "epoch": 0.29712,
      "grad_norm": 0.36163052916526794,
      "learning_rate": 0.00018021602880384052,
      "loss": 0.7804,
      "step": 3714
    },
    {
      "epoch": 0.2972,
      "grad_norm": 0.39205506443977356,
      "learning_rate": 0.00018021069475930126,
      "loss": 0.7011,
      "step": 3715
    },
    {
      "epoch": 0.29728,
      "grad_norm": 0.5164151191711426,
      "learning_rate": 0.00018020536071476197,
      "loss": 0.987,
      "step": 3716
    },
    {
      "epoch": 0.29736,
      "grad_norm": 0.36909162998199463,
      "learning_rate": 0.00018020002667022271,
      "loss": 0.6885,
      "step": 3717
    },
    {
      "epoch": 0.29744,
      "grad_norm": 0.3608921468257904,
      "learning_rate": 0.00018019469262568343,
      "loss": 0.6456,
      "step": 3718
    },
    {
      "epoch": 0.29752,
      "grad_norm": 0.36491113901138306,
      "learning_rate": 0.00018018935858114417,
      "loss": 0.6957,
      "step": 3719
    },
    {
      "epoch": 0.2976,
      "grad_norm": 0.4927941560745239,
      "learning_rate": 0.00018018402453660488,
      "loss": 1.0862,
      "step": 3720
    },
    {
      "epoch": 0.29768,
      "grad_norm": 0.3540436029434204,
      "learning_rate": 0.00018017869049206562,
      "loss": 0.7742,
      "step": 3721
    },
    {
      "epoch": 0.29776,
      "grad_norm": 0.3370109498500824,
      "learning_rate": 0.00018017335644752636,
      "loss": 0.5809,
      "step": 3722
    },
    {
      "epoch": 0.29784,
      "grad_norm": 0.4369634687900543,
      "learning_rate": 0.00018016802240298707,
      "loss": 0.8271,
      "step": 3723
    },
    {
      "epoch": 0.29792,
      "grad_norm": 0.37378567457199097,
      "learning_rate": 0.0001801626883584478,
      "loss": 1.0762,
      "step": 3724
    },
    {
      "epoch": 0.298,
      "grad_norm": 0.46514588594436646,
      "learning_rate": 0.00018015735431390852,
      "loss": 1.1059,
      "step": 3725
    },
    {
      "epoch": 0.29808,
      "grad_norm": 0.468894898891449,
      "learning_rate": 0.00018015202026936926,
      "loss": 0.8938,
      "step": 3726
    },
    {
      "epoch": 0.29816,
      "grad_norm": 0.376128613948822,
      "learning_rate": 0.00018014668622483,
      "loss": 0.7686,
      "step": 3727
    },
    {
      "epoch": 0.29824,
      "grad_norm": 0.38804757595062256,
      "learning_rate": 0.00018014135218029072,
      "loss": 0.798,
      "step": 3728
    },
    {
      "epoch": 0.29832,
      "grad_norm": 0.36272791028022766,
      "learning_rate": 0.00018013601813575146,
      "loss": 0.6716,
      "step": 3729
    },
    {
      "epoch": 0.2984,
      "grad_norm": 0.4395151436328888,
      "learning_rate": 0.00018013068409121217,
      "loss": 0.7437,
      "step": 3730
    },
    {
      "epoch": 0.29848,
      "grad_norm": 0.35355088114738464,
      "learning_rate": 0.0001801253500466729,
      "loss": 1.0498,
      "step": 3731
    },
    {
      "epoch": 0.29856,
      "grad_norm": 0.34677988290786743,
      "learning_rate": 0.00018012001600213362,
      "loss": 0.8598,
      "step": 3732
    },
    {
      "epoch": 0.29864,
      "grad_norm": 0.33074551820755005,
      "learning_rate": 0.00018011468195759436,
      "loss": 0.9414,
      "step": 3733
    },
    {
      "epoch": 0.29872,
      "grad_norm": 0.41886061429977417,
      "learning_rate": 0.0001801093479130551,
      "loss": 0.6067,
      "step": 3734
    },
    {
      "epoch": 0.2988,
      "grad_norm": 0.3015618622303009,
      "learning_rate": 0.00018010401386851581,
      "loss": 0.5868,
      "step": 3735
    },
    {
      "epoch": 0.29888,
      "grad_norm": 0.4938082695007324,
      "learning_rate": 0.00018009867982397655,
      "loss": 0.7697,
      "step": 3736
    },
    {
      "epoch": 0.29896,
      "grad_norm": 0.4737706482410431,
      "learning_rate": 0.00018009334577943727,
      "loss": 0.7261,
      "step": 3737
    },
    {
      "epoch": 0.29904,
      "grad_norm": 0.2966579794883728,
      "learning_rate": 0.000180088011734898,
      "loss": 0.6953,
      "step": 3738
    },
    {
      "epoch": 0.29912,
      "grad_norm": 0.3745627701282501,
      "learning_rate": 0.00018008267769035872,
      "loss": 0.6169,
      "step": 3739
    },
    {
      "epoch": 0.2992,
      "grad_norm": 0.43727535009384155,
      "learning_rate": 0.00018007734364581946,
      "loss": 1.1931,
      "step": 3740
    },
    {
      "epoch": 0.29928,
      "grad_norm": 0.43252602219581604,
      "learning_rate": 0.0001800720096012802,
      "loss": 0.6585,
      "step": 3741
    },
    {
      "epoch": 0.29936,
      "grad_norm": 0.3067927956581116,
      "learning_rate": 0.0001800666755567409,
      "loss": 1.0071,
      "step": 3742
    },
    {
      "epoch": 0.29944,
      "grad_norm": 0.3986855149269104,
      "learning_rate": 0.00018006134151220165,
      "loss": 1.0516,
      "step": 3743
    },
    {
      "epoch": 0.29952,
      "grad_norm": 0.3816511332988739,
      "learning_rate": 0.00018005600746766236,
      "loss": 0.9627,
      "step": 3744
    },
    {
      "epoch": 0.2996,
      "grad_norm": 0.32380741834640503,
      "learning_rate": 0.0001800506734231231,
      "loss": 0.8785,
      "step": 3745
    },
    {
      "epoch": 0.29968,
      "grad_norm": 0.3781172037124634,
      "learning_rate": 0.00018004533937858382,
      "loss": 0.8693,
      "step": 3746
    },
    {
      "epoch": 0.29976,
      "grad_norm": 0.3681291937828064,
      "learning_rate": 0.00018004000533404456,
      "loss": 0.802,
      "step": 3747
    },
    {
      "epoch": 0.29984,
      "grad_norm": 0.2925749719142914,
      "learning_rate": 0.0001800346712895053,
      "loss": 0.8416,
      "step": 3748
    },
    {
      "epoch": 0.29992,
      "grad_norm": 0.41742148995399475,
      "learning_rate": 0.000180029337244966,
      "loss": 0.6982,
      "step": 3749
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.3920883238315582,
      "learning_rate": 0.00018002400320042675,
      "loss": 1.0313,
      "step": 3750
    },
    {
      "epoch": 0.30008,
      "grad_norm": 0.39664870500564575,
      "learning_rate": 0.00018001866915588746,
      "loss": 0.8308,
      "step": 3751
    },
    {
      "epoch": 0.30016,
      "grad_norm": 0.40582627058029175,
      "learning_rate": 0.0001800133351113482,
      "loss": 0.6223,
      "step": 3752
    },
    {
      "epoch": 0.30024,
      "grad_norm": 0.4101187288761139,
      "learning_rate": 0.00018000800106680892,
      "loss": 0.7042,
      "step": 3753
    },
    {
      "epoch": 0.30032,
      "grad_norm": 0.3642193675041199,
      "learning_rate": 0.00018000266702226965,
      "loss": 0.7636,
      "step": 3754
    },
    {
      "epoch": 0.3004,
      "grad_norm": 0.43531742691993713,
      "learning_rate": 0.0001799973329777304,
      "loss": 0.9169,
      "step": 3755
    },
    {
      "epoch": 0.30048,
      "grad_norm": 0.3882179856300354,
      "learning_rate": 0.0001799919989331911,
      "loss": 0.79,
      "step": 3756
    },
    {
      "epoch": 0.30056,
      "grad_norm": 0.4679514467716217,
      "learning_rate": 0.00017998666488865185,
      "loss": 0.9917,
      "step": 3757
    },
    {
      "epoch": 0.30064,
      "grad_norm": 0.3326565623283386,
      "learning_rate": 0.00017998133084411256,
      "loss": 0.7291,
      "step": 3758
    },
    {
      "epoch": 0.30072,
      "grad_norm": 0.3200722336769104,
      "learning_rate": 0.0001799759967995733,
      "loss": 0.7664,
      "step": 3759
    },
    {
      "epoch": 0.3008,
      "grad_norm": 0.3261730670928955,
      "learning_rate": 0.000179970662755034,
      "loss": 0.6557,
      "step": 3760
    },
    {
      "epoch": 0.30088,
      "grad_norm": 0.4576931893825531,
      "learning_rate": 0.00017996532871049475,
      "loss": 0.6787,
      "step": 3761
    },
    {
      "epoch": 0.30096,
      "grad_norm": 0.4634295701980591,
      "learning_rate": 0.00017995999466595547,
      "loss": 1.0273,
      "step": 3762
    },
    {
      "epoch": 0.30104,
      "grad_norm": 0.3306910991668701,
      "learning_rate": 0.0001799546606214162,
      "loss": 0.5939,
      "step": 3763
    },
    {
      "epoch": 0.30112,
      "grad_norm": 0.3477284014225006,
      "learning_rate": 0.00017994932657687692,
      "loss": 0.7693,
      "step": 3764
    },
    {
      "epoch": 0.3012,
      "grad_norm": 0.4625900685787201,
      "learning_rate": 0.00017994399253233766,
      "loss": 0.8774,
      "step": 3765
    },
    {
      "epoch": 0.30128,
      "grad_norm": 0.3182341456413269,
      "learning_rate": 0.00017993865848779837,
      "loss": 1.0233,
      "step": 3766
    },
    {
      "epoch": 0.30136,
      "grad_norm": 0.37889373302459717,
      "learning_rate": 0.0001799333244432591,
      "loss": 0.7061,
      "step": 3767
    },
    {
      "epoch": 0.30144,
      "grad_norm": 0.26734769344329834,
      "learning_rate": 0.00017992799039871985,
      "loss": 0.6604,
      "step": 3768
    },
    {
      "epoch": 0.30152,
      "grad_norm": 0.5075023174285889,
      "learning_rate": 0.00017992265635418056,
      "loss": 0.939,
      "step": 3769
    },
    {
      "epoch": 0.3016,
      "grad_norm": 0.6044561266899109,
      "learning_rate": 0.0001799173223096413,
      "loss": 1.0163,
      "step": 3770
    },
    {
      "epoch": 0.30168,
      "grad_norm": 0.3111051321029663,
      "learning_rate": 0.00017991198826510202,
      "loss": 0.6393,
      "step": 3771
    },
    {
      "epoch": 0.30176,
      "grad_norm": 0.35740819573402405,
      "learning_rate": 0.00017990665422056276,
      "loss": 0.6808,
      "step": 3772
    },
    {
      "epoch": 0.30184,
      "grad_norm": 0.4139276444911957,
      "learning_rate": 0.00017990132017602347,
      "loss": 0.9581,
      "step": 3773
    },
    {
      "epoch": 0.30192,
      "grad_norm": 0.34873443841934204,
      "learning_rate": 0.0001798959861314842,
      "loss": 0.9621,
      "step": 3774
    },
    {
      "epoch": 0.302,
      "grad_norm": 0.45422855019569397,
      "learning_rate": 0.00017989065208694492,
      "loss": 0.8644,
      "step": 3775
    },
    {
      "epoch": 0.30208,
      "grad_norm": 0.42195427417755127,
      "learning_rate": 0.00017988531804240566,
      "loss": 0.7345,
      "step": 3776
    },
    {
      "epoch": 0.30216,
      "grad_norm": 0.44097474217414856,
      "learning_rate": 0.00017987998399786637,
      "loss": 1.0986,
      "step": 3777
    },
    {
      "epoch": 0.30224,
      "grad_norm": 0.41170236468315125,
      "learning_rate": 0.0001798746499533271,
      "loss": 0.851,
      "step": 3778
    },
    {
      "epoch": 0.30232,
      "grad_norm": 0.36255407333374023,
      "learning_rate": 0.00017986931590878783,
      "loss": 0.6739,
      "step": 3779
    },
    {
      "epoch": 0.3024,
      "grad_norm": 0.3270629942417145,
      "learning_rate": 0.00017986398186424857,
      "loss": 0.7945,
      "step": 3780
    },
    {
      "epoch": 0.30248,
      "grad_norm": 0.3700542151927948,
      "learning_rate": 0.0001798586478197093,
      "loss": 0.5358,
      "step": 3781
    },
    {
      "epoch": 0.30256,
      "grad_norm": 0.2963257133960724,
      "learning_rate": 0.00017985331377517002,
      "loss": 0.6837,
      "step": 3782
    },
    {
      "epoch": 0.30264,
      "grad_norm": 0.39469826221466064,
      "learning_rate": 0.00017984797973063076,
      "loss": 0.7256,
      "step": 3783
    },
    {
      "epoch": 0.30272,
      "grad_norm": 0.3833441734313965,
      "learning_rate": 0.00017984264568609147,
      "loss": 0.9245,
      "step": 3784
    },
    {
      "epoch": 0.3028,
      "grad_norm": 0.395488977432251,
      "learning_rate": 0.0001798373116415522,
      "loss": 0.6158,
      "step": 3785
    },
    {
      "epoch": 0.30288,
      "grad_norm": 0.28887277841567993,
      "learning_rate": 0.00017983197759701292,
      "loss": 0.7868,
      "step": 3786
    },
    {
      "epoch": 0.30296,
      "grad_norm": 0.5096101760864258,
      "learning_rate": 0.00017982664355247366,
      "loss": 1.0806,
      "step": 3787
    },
    {
      "epoch": 0.30304,
      "grad_norm": 0.38473016023635864,
      "learning_rate": 0.0001798213095079344,
      "loss": 0.6685,
      "step": 3788
    },
    {
      "epoch": 0.30312,
      "grad_norm": 0.41179159283638,
      "learning_rate": 0.00017981597546339512,
      "loss": 0.9759,
      "step": 3789
    },
    {
      "epoch": 0.3032,
      "grad_norm": 0.3884789049625397,
      "learning_rate": 0.00017981064141885586,
      "loss": 0.8348,
      "step": 3790
    },
    {
      "epoch": 0.30328,
      "grad_norm": 0.35200047492980957,
      "learning_rate": 0.00017980530737431657,
      "loss": 0.898,
      "step": 3791
    },
    {
      "epoch": 0.30336,
      "grad_norm": 0.36674338579177856,
      "learning_rate": 0.0001797999733297773,
      "loss": 0.9298,
      "step": 3792
    },
    {
      "epoch": 0.30344,
      "grad_norm": 0.3405247926712036,
      "learning_rate": 0.00017979463928523802,
      "loss": 0.6277,
      "step": 3793
    },
    {
      "epoch": 0.30352,
      "grad_norm": 0.4292054772377014,
      "learning_rate": 0.00017978930524069876,
      "loss": 0.8236,
      "step": 3794
    },
    {
      "epoch": 0.3036,
      "grad_norm": 0.3688279986381531,
      "learning_rate": 0.0001797839711961595,
      "loss": 0.5573,
      "step": 3795
    },
    {
      "epoch": 0.30368,
      "grad_norm": 0.3902488946914673,
      "learning_rate": 0.00017977863715162021,
      "loss": 0.9792,
      "step": 3796
    },
    {
      "epoch": 0.30376,
      "grad_norm": 0.3844808340072632,
      "learning_rate": 0.00017977330310708095,
      "loss": 0.9931,
      "step": 3797
    },
    {
      "epoch": 0.30384,
      "grad_norm": 0.35864612460136414,
      "learning_rate": 0.00017976796906254167,
      "loss": 0.8147,
      "step": 3798
    },
    {
      "epoch": 0.30392,
      "grad_norm": 0.29619723558425903,
      "learning_rate": 0.0001797626350180024,
      "loss": 0.9234,
      "step": 3799
    },
    {
      "epoch": 0.304,
      "grad_norm": 0.3961988687515259,
      "learning_rate": 0.00017975730097346312,
      "loss": 0.7428,
      "step": 3800
    },
    {
      "epoch": 0.30408,
      "grad_norm": 0.4372445046901703,
      "learning_rate": 0.00017975196692892386,
      "loss": 0.863,
      "step": 3801
    },
    {
      "epoch": 0.30416,
      "grad_norm": 0.4451107680797577,
      "learning_rate": 0.0001797466328843846,
      "loss": 0.9517,
      "step": 3802
    },
    {
      "epoch": 0.30424,
      "grad_norm": 0.3674789071083069,
      "learning_rate": 0.0001797412988398453,
      "loss": 0.7298,
      "step": 3803
    },
    {
      "epoch": 0.30432,
      "grad_norm": 0.4845358431339264,
      "learning_rate": 0.00017973596479530605,
      "loss": 1.0424,
      "step": 3804
    },
    {
      "epoch": 0.3044,
      "grad_norm": 0.3850647509098053,
      "learning_rate": 0.00017973063075076676,
      "loss": 0.7551,
      "step": 3805
    },
    {
      "epoch": 0.30448,
      "grad_norm": 0.42124485969543457,
      "learning_rate": 0.0001797252967062275,
      "loss": 0.704,
      "step": 3806
    },
    {
      "epoch": 0.30456,
      "grad_norm": 0.40962642431259155,
      "learning_rate": 0.00017971996266168822,
      "loss": 0.9679,
      "step": 3807
    },
    {
      "epoch": 0.30464,
      "grad_norm": 0.3410643935203552,
      "learning_rate": 0.00017971462861714896,
      "loss": 0.9943,
      "step": 3808
    },
    {
      "epoch": 0.30472,
      "grad_norm": 0.35703712701797485,
      "learning_rate": 0.0001797092945726097,
      "loss": 0.7986,
      "step": 3809
    },
    {
      "epoch": 0.3048,
      "grad_norm": 0.38339224457740784,
      "learning_rate": 0.0001797039605280704,
      "loss": 0.6023,
      "step": 3810
    },
    {
      "epoch": 0.30488,
      "grad_norm": 0.33406126499176025,
      "learning_rate": 0.00017969862648353115,
      "loss": 1.0644,
      "step": 3811
    },
    {
      "epoch": 0.30496,
      "grad_norm": 0.43943649530410767,
      "learning_rate": 0.00017969329243899186,
      "loss": 0.6937,
      "step": 3812
    },
    {
      "epoch": 0.30504,
      "grad_norm": 0.3938084542751312,
      "learning_rate": 0.0001796879583944526,
      "loss": 0.9731,
      "step": 3813
    },
    {
      "epoch": 0.30512,
      "grad_norm": 0.46740689873695374,
      "learning_rate": 0.00017968262434991331,
      "loss": 1.0821,
      "step": 3814
    },
    {
      "epoch": 0.3052,
      "grad_norm": 0.45072585344314575,
      "learning_rate": 0.00017967729030537405,
      "loss": 0.7458,
      "step": 3815
    },
    {
      "epoch": 0.30528,
      "grad_norm": 0.39907100796699524,
      "learning_rate": 0.0001796719562608348,
      "loss": 0.8641,
      "step": 3816
    },
    {
      "epoch": 0.30536,
      "grad_norm": 0.2982921004295349,
      "learning_rate": 0.0001796666222162955,
      "loss": 0.5589,
      "step": 3817
    },
    {
      "epoch": 0.30544,
      "grad_norm": 0.3723672032356262,
      "learning_rate": 0.00017966128817175625,
      "loss": 1.0068,
      "step": 3818
    },
    {
      "epoch": 0.30552,
      "grad_norm": 0.35299190878868103,
      "learning_rate": 0.00017965595412721696,
      "loss": 0.6519,
      "step": 3819
    },
    {
      "epoch": 0.3056,
      "grad_norm": 0.38680586218833923,
      "learning_rate": 0.0001796506200826777,
      "loss": 0.7183,
      "step": 3820
    },
    {
      "epoch": 0.30568,
      "grad_norm": 0.38559168577194214,
      "learning_rate": 0.0001796452860381384,
      "loss": 0.8268,
      "step": 3821
    },
    {
      "epoch": 0.30576,
      "grad_norm": 0.43884703516960144,
      "learning_rate": 0.00017963995199359915,
      "loss": 0.6765,
      "step": 3822
    },
    {
      "epoch": 0.30584,
      "grad_norm": 0.3758871257305145,
      "learning_rate": 0.0001796346179490599,
      "loss": 0.9265,
      "step": 3823
    },
    {
      "epoch": 0.30592,
      "grad_norm": 0.40709227323532104,
      "learning_rate": 0.0001796292839045206,
      "loss": 0.6938,
      "step": 3824
    },
    {
      "epoch": 0.306,
      "grad_norm": 0.384879469871521,
      "learning_rate": 0.00017962394985998134,
      "loss": 0.8364,
      "step": 3825
    },
    {
      "epoch": 0.30608,
      "grad_norm": 0.31338635087013245,
      "learning_rate": 0.00017961861581544206,
      "loss": 0.8422,
      "step": 3826
    },
    {
      "epoch": 0.30616,
      "grad_norm": 0.39978110790252686,
      "learning_rate": 0.0001796132817709028,
      "loss": 0.8309,
      "step": 3827
    },
    {
      "epoch": 0.30624,
      "grad_norm": 0.4459803104400635,
      "learning_rate": 0.00017960794772636354,
      "loss": 0.7539,
      "step": 3828
    },
    {
      "epoch": 0.30632,
      "grad_norm": 0.38468536734580994,
      "learning_rate": 0.00017960261368182425,
      "loss": 0.7437,
      "step": 3829
    },
    {
      "epoch": 0.3064,
      "grad_norm": 0.46358758211135864,
      "learning_rate": 0.000179597279637285,
      "loss": 0.9987,
      "step": 3830
    },
    {
      "epoch": 0.30648,
      "grad_norm": 0.31905168294906616,
      "learning_rate": 0.0001795919455927457,
      "loss": 0.9926,
      "step": 3831
    },
    {
      "epoch": 0.30656,
      "grad_norm": 0.2915076017379761,
      "learning_rate": 0.00017958661154820644,
      "loss": 0.6198,
      "step": 3832
    },
    {
      "epoch": 0.30664,
      "grad_norm": 0.37742388248443604,
      "learning_rate": 0.00017958127750366715,
      "loss": 0.7726,
      "step": 3833
    },
    {
      "epoch": 0.30672,
      "grad_norm": 0.3906323313713074,
      "learning_rate": 0.0001795759434591279,
      "loss": 0.8703,
      "step": 3834
    },
    {
      "epoch": 0.3068,
      "grad_norm": 0.3196161091327667,
      "learning_rate": 0.00017957060941458863,
      "loss": 0.7541,
      "step": 3835
    },
    {
      "epoch": 0.30688,
      "grad_norm": 0.31392228603363037,
      "learning_rate": 0.00017956527537004935,
      "loss": 0.8805,
      "step": 3836
    },
    {
      "epoch": 0.30696,
      "grad_norm": 0.26562681794166565,
      "learning_rate": 0.0001795599413255101,
      "loss": 0.7302,
      "step": 3837
    },
    {
      "epoch": 0.30704,
      "grad_norm": 0.402568519115448,
      "learning_rate": 0.0001795546072809708,
      "loss": 0.6641,
      "step": 3838
    },
    {
      "epoch": 0.30712,
      "grad_norm": 0.39354947209358215,
      "learning_rate": 0.00017954927323643154,
      "loss": 0.8046,
      "step": 3839
    },
    {
      "epoch": 0.3072,
      "grad_norm": 0.3542526364326477,
      "learning_rate": 0.00017954393919189225,
      "loss": 0.4999,
      "step": 3840
    },
    {
      "epoch": 0.30728,
      "grad_norm": 0.31459519267082214,
      "learning_rate": 0.000179538605147353,
      "loss": 1.1166,
      "step": 3841
    },
    {
      "epoch": 0.30736,
      "grad_norm": 0.42365533113479614,
      "learning_rate": 0.00017953327110281373,
      "loss": 0.9839,
      "step": 3842
    },
    {
      "epoch": 0.30744,
      "grad_norm": 0.4624696969985962,
      "learning_rate": 0.00017952793705827444,
      "loss": 0.8543,
      "step": 3843
    },
    {
      "epoch": 0.30752,
      "grad_norm": 0.4650023877620697,
      "learning_rate": 0.00017952260301373518,
      "loss": 1.0342,
      "step": 3844
    },
    {
      "epoch": 0.3076,
      "grad_norm": 0.3974297344684601,
      "learning_rate": 0.0001795172689691959,
      "loss": 0.8894,
      "step": 3845
    },
    {
      "epoch": 0.30768,
      "grad_norm": 0.3445703089237213,
      "learning_rate": 0.00017951193492465664,
      "loss": 0.9892,
      "step": 3846
    },
    {
      "epoch": 0.30776,
      "grad_norm": 0.38067498803138733,
      "learning_rate": 0.00017950660088011735,
      "loss": 0.6391,
      "step": 3847
    },
    {
      "epoch": 0.30784,
      "grad_norm": 0.3544333279132843,
      "learning_rate": 0.0001795012668355781,
      "loss": 0.8989,
      "step": 3848
    },
    {
      "epoch": 0.30792,
      "grad_norm": 0.42804908752441406,
      "learning_rate": 0.00017949593279103883,
      "loss": 0.9392,
      "step": 3849
    },
    {
      "epoch": 0.308,
      "grad_norm": 0.3396024703979492,
      "learning_rate": 0.00017949059874649954,
      "loss": 0.717,
      "step": 3850
    },
    {
      "epoch": 0.30808,
      "grad_norm": 0.45999589562416077,
      "learning_rate": 0.00017948526470196028,
      "loss": 0.6011,
      "step": 3851
    },
    {
      "epoch": 0.30816,
      "grad_norm": 0.3563213050365448,
      "learning_rate": 0.000179479930657421,
      "loss": 0.6468,
      "step": 3852
    },
    {
      "epoch": 0.30824,
      "grad_norm": 0.427573025226593,
      "learning_rate": 0.00017947459661288173,
      "loss": 1.0028,
      "step": 3853
    },
    {
      "epoch": 0.30832,
      "grad_norm": 0.3675219714641571,
      "learning_rate": 0.00017946926256834245,
      "loss": 0.6561,
      "step": 3854
    },
    {
      "epoch": 0.3084,
      "grad_norm": 0.49348580837249756,
      "learning_rate": 0.0001794639285238032,
      "loss": 1.0958,
      "step": 3855
    },
    {
      "epoch": 0.30848,
      "grad_norm": 0.38280537724494934,
      "learning_rate": 0.00017945859447926393,
      "loss": 0.9363,
      "step": 3856
    },
    {
      "epoch": 0.30856,
      "grad_norm": 0.29356998205184937,
      "learning_rate": 0.00017945326043472464,
      "loss": 0.5946,
      "step": 3857
    },
    {
      "epoch": 0.30864,
      "grad_norm": 0.38220033049583435,
      "learning_rate": 0.00017944792639018538,
      "loss": 0.6326,
      "step": 3858
    },
    {
      "epoch": 0.30872,
      "grad_norm": 0.4200810194015503,
      "learning_rate": 0.0001794425923456461,
      "loss": 0.5863,
      "step": 3859
    },
    {
      "epoch": 0.3088,
      "grad_norm": 0.31866398453712463,
      "learning_rate": 0.00017943725830110683,
      "loss": 1.1235,
      "step": 3860
    },
    {
      "epoch": 0.30888,
      "grad_norm": 0.394573837518692,
      "learning_rate": 0.00017943192425656754,
      "loss": 0.7299,
      "step": 3861
    },
    {
      "epoch": 0.30896,
      "grad_norm": 0.4068346917629242,
      "learning_rate": 0.00017942659021202828,
      "loss": 0.8178,
      "step": 3862
    },
    {
      "epoch": 0.30904,
      "grad_norm": 0.3662802577018738,
      "learning_rate": 0.00017942125616748902,
      "loss": 0.723,
      "step": 3863
    },
    {
      "epoch": 0.30912,
      "grad_norm": 0.39171308279037476,
      "learning_rate": 0.00017941592212294974,
      "loss": 0.8249,
      "step": 3864
    },
    {
      "epoch": 0.3092,
      "grad_norm": 0.42723748087882996,
      "learning_rate": 0.00017941058807841048,
      "loss": 0.7864,
      "step": 3865
    },
    {
      "epoch": 0.30928,
      "grad_norm": 0.47181689739227295,
      "learning_rate": 0.0001794052540338712,
      "loss": 1.1668,
      "step": 3866
    },
    {
      "epoch": 0.30936,
      "grad_norm": 0.3542664349079132,
      "learning_rate": 0.00017939991998933193,
      "loss": 0.7559,
      "step": 3867
    },
    {
      "epoch": 0.30944,
      "grad_norm": 0.3839470446109772,
      "learning_rate": 0.00017939458594479264,
      "loss": 0.7117,
      "step": 3868
    },
    {
      "epoch": 0.30952,
      "grad_norm": 0.41675591468811035,
      "learning_rate": 0.00017938925190025338,
      "loss": 0.8492,
      "step": 3869
    },
    {
      "epoch": 0.3096,
      "grad_norm": 0.2973752021789551,
      "learning_rate": 0.00017938391785571412,
      "loss": 0.6494,
      "step": 3870
    },
    {
      "epoch": 0.30968,
      "grad_norm": 0.5146158933639526,
      "learning_rate": 0.00017937858381117483,
      "loss": 0.8758,
      "step": 3871
    },
    {
      "epoch": 0.30976,
      "grad_norm": 0.5379782915115356,
      "learning_rate": 0.00017937324976663557,
      "loss": 1.2484,
      "step": 3872
    },
    {
      "epoch": 0.30984,
      "grad_norm": 0.4397953450679779,
      "learning_rate": 0.0001793679157220963,
      "loss": 1.031,
      "step": 3873
    },
    {
      "epoch": 0.30992,
      "grad_norm": 0.2915576994419098,
      "learning_rate": 0.00017936258167755703,
      "loss": 0.8448,
      "step": 3874
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.38457220792770386,
      "learning_rate": 0.00017935724763301777,
      "loss": 0.5492,
      "step": 3875
    },
    {
      "epoch": 0.31008,
      "grad_norm": 0.3778167963027954,
      "learning_rate": 0.00017935191358847848,
      "loss": 0.989,
      "step": 3876
    },
    {
      "epoch": 0.31016,
      "grad_norm": 0.41228318214416504,
      "learning_rate": 0.00017934657954393922,
      "loss": 0.6701,
      "step": 3877
    },
    {
      "epoch": 0.31024,
      "grad_norm": 0.3498058021068573,
      "learning_rate": 0.00017934124549939993,
      "loss": 0.6461,
      "step": 3878
    },
    {
      "epoch": 0.31032,
      "grad_norm": 0.3628478944301605,
      "learning_rate": 0.00017933591145486067,
      "loss": 0.6544,
      "step": 3879
    },
    {
      "epoch": 0.3104,
      "grad_norm": 0.36147215962409973,
      "learning_rate": 0.00017933057741032139,
      "loss": 0.7991,
      "step": 3880
    },
    {
      "epoch": 0.31048,
      "grad_norm": 0.364671528339386,
      "learning_rate": 0.00017932524336578212,
      "loss": 0.8668,
      "step": 3881
    },
    {
      "epoch": 0.31056,
      "grad_norm": 0.34969934821128845,
      "learning_rate": 0.00017931990932124286,
      "loss": 0.5463,
      "step": 3882
    },
    {
      "epoch": 0.31064,
      "grad_norm": 0.4916601777076721,
      "learning_rate": 0.00017931457527670358,
      "loss": 0.8857,
      "step": 3883
    },
    {
      "epoch": 0.31072,
      "grad_norm": 0.34682929515838623,
      "learning_rate": 0.00017930924123216432,
      "loss": 0.964,
      "step": 3884
    },
    {
      "epoch": 0.3108,
      "grad_norm": 0.36424311995506287,
      "learning_rate": 0.00017930390718762503,
      "loss": 0.6261,
      "step": 3885
    },
    {
      "epoch": 0.31088,
      "grad_norm": 0.3529607355594635,
      "learning_rate": 0.00017929857314308577,
      "loss": 0.8961,
      "step": 3886
    },
    {
      "epoch": 0.31096,
      "grad_norm": 0.45268887281417847,
      "learning_rate": 0.00017929323909854648,
      "loss": 0.5119,
      "step": 3887
    },
    {
      "epoch": 0.31104,
      "grad_norm": 0.40346482396125793,
      "learning_rate": 0.00017928790505400722,
      "loss": 0.8903,
      "step": 3888
    },
    {
      "epoch": 0.31112,
      "grad_norm": 0.3731189966201782,
      "learning_rate": 0.00017928257100946794,
      "loss": 0.7271,
      "step": 3889
    },
    {
      "epoch": 0.3112,
      "grad_norm": 0.33017808198928833,
      "learning_rate": 0.00017927723696492868,
      "loss": 0.6678,
      "step": 3890
    },
    {
      "epoch": 0.31128,
      "grad_norm": 0.40974387526512146,
      "learning_rate": 0.0001792719029203894,
      "loss": 0.9477,
      "step": 3891
    },
    {
      "epoch": 0.31136,
      "grad_norm": 0.34587904810905457,
      "learning_rate": 0.00017926656887585013,
      "loss": 1.0874,
      "step": 3892
    },
    {
      "epoch": 0.31144,
      "grad_norm": 0.521950900554657,
      "learning_rate": 0.00017926123483131084,
      "loss": 0.9019,
      "step": 3893
    },
    {
      "epoch": 0.31152,
      "grad_norm": 0.367194265127182,
      "learning_rate": 0.00017925590078677158,
      "loss": 0.7415,
      "step": 3894
    },
    {
      "epoch": 0.3116,
      "grad_norm": 0.3457355499267578,
      "learning_rate": 0.0001792505667422323,
      "loss": 0.689,
      "step": 3895
    },
    {
      "epoch": 0.31168,
      "grad_norm": 0.3415844142436981,
      "learning_rate": 0.00017924523269769303,
      "loss": 0.8022,
      "step": 3896
    },
    {
      "epoch": 0.31176,
      "grad_norm": 0.38297465443611145,
      "learning_rate": 0.00017923989865315377,
      "loss": 0.8212,
      "step": 3897
    },
    {
      "epoch": 0.31184,
      "grad_norm": 0.37121424078941345,
      "learning_rate": 0.00017923456460861449,
      "loss": 0.9278,
      "step": 3898
    },
    {
      "epoch": 0.31192,
      "grad_norm": 0.3736172020435333,
      "learning_rate": 0.00017922923056407523,
      "loss": 0.9647,
      "step": 3899
    },
    {
      "epoch": 0.312,
      "grad_norm": 0.3076673746109009,
      "learning_rate": 0.00017922389651953594,
      "loss": 0.788,
      "step": 3900
    },
    {
      "epoch": 0.31208,
      "grad_norm": 0.3927079439163208,
      "learning_rate": 0.00017921856247499668,
      "loss": 0.8474,
      "step": 3901
    },
    {
      "epoch": 0.31216,
      "grad_norm": 0.3479446470737457,
      "learning_rate": 0.0001792132284304574,
      "loss": 0.8412,
      "step": 3902
    },
    {
      "epoch": 0.31224,
      "grad_norm": 0.34261244535446167,
      "learning_rate": 0.00017920789438591813,
      "loss": 1.1623,
      "step": 3903
    },
    {
      "epoch": 0.31232,
      "grad_norm": 0.3108665645122528,
      "learning_rate": 0.00017920256034137884,
      "loss": 0.7621,
      "step": 3904
    },
    {
      "epoch": 0.3124,
      "grad_norm": 0.30622634291648865,
      "learning_rate": 0.00017919722629683958,
      "loss": 0.8405,
      "step": 3905
    },
    {
      "epoch": 0.31248,
      "grad_norm": 0.37532415986061096,
      "learning_rate": 0.0001791918922523003,
      "loss": 1.3111,
      "step": 3906
    },
    {
      "epoch": 0.31256,
      "grad_norm": 0.41650116443634033,
      "learning_rate": 0.00017918655820776104,
      "loss": 1.1651,
      "step": 3907
    },
    {
      "epoch": 0.31264,
      "grad_norm": 0.3633931875228882,
      "learning_rate": 0.00017918122416322175,
      "loss": 0.7337,
      "step": 3908
    },
    {
      "epoch": 0.31272,
      "grad_norm": 0.36833691596984863,
      "learning_rate": 0.0001791758901186825,
      "loss": 0.7495,
      "step": 3909
    },
    {
      "epoch": 0.3128,
      "grad_norm": 0.30838125944137573,
      "learning_rate": 0.00017917055607414323,
      "loss": 0.9104,
      "step": 3910
    },
    {
      "epoch": 0.31288,
      "grad_norm": 0.35710105299949646,
      "learning_rate": 0.00017916522202960394,
      "loss": 0.9542,
      "step": 3911
    },
    {
      "epoch": 0.31296,
      "grad_norm": 0.29817071557044983,
      "learning_rate": 0.00017915988798506468,
      "loss": 0.7367,
      "step": 3912
    },
    {
      "epoch": 0.31304,
      "grad_norm": 0.3800963759422302,
      "learning_rate": 0.0001791545539405254,
      "loss": 0.531,
      "step": 3913
    },
    {
      "epoch": 0.31312,
      "grad_norm": 0.3366307318210602,
      "learning_rate": 0.00017914921989598613,
      "loss": 0.572,
      "step": 3914
    },
    {
      "epoch": 0.3132,
      "grad_norm": 0.41499558091163635,
      "learning_rate": 0.00017914388585144685,
      "loss": 0.5757,
      "step": 3915
    },
    {
      "epoch": 0.31328,
      "grad_norm": 0.3807865083217621,
      "learning_rate": 0.00017913855180690759,
      "loss": 1.1157,
      "step": 3916
    },
    {
      "epoch": 0.31336,
      "grad_norm": 0.34884315729141235,
      "learning_rate": 0.00017913321776236833,
      "loss": 0.6267,
      "step": 3917
    },
    {
      "epoch": 0.31344,
      "grad_norm": 0.28942301869392395,
      "learning_rate": 0.00017912788371782904,
      "loss": 0.5672,
      "step": 3918
    },
    {
      "epoch": 0.31352,
      "grad_norm": 0.3177877962589264,
      "learning_rate": 0.00017912254967328978,
      "loss": 0.6245,
      "step": 3919
    },
    {
      "epoch": 0.3136,
      "grad_norm": 0.434490442276001,
      "learning_rate": 0.0001791172156287505,
      "loss": 0.9034,
      "step": 3920
    },
    {
      "epoch": 0.31368,
      "grad_norm": 0.38951680064201355,
      "learning_rate": 0.00017911188158421123,
      "loss": 0.7181,
      "step": 3921
    },
    {
      "epoch": 0.31376,
      "grad_norm": 0.35545238852500916,
      "learning_rate": 0.00017910654753967194,
      "loss": 0.5926,
      "step": 3922
    },
    {
      "epoch": 0.31384,
      "grad_norm": 0.35173460841178894,
      "learning_rate": 0.00017910121349513268,
      "loss": 0.81,
      "step": 3923
    },
    {
      "epoch": 0.31392,
      "grad_norm": 0.4164726436138153,
      "learning_rate": 0.00017909587945059342,
      "loss": 1.1023,
      "step": 3924
    },
    {
      "epoch": 0.314,
      "grad_norm": 0.374658465385437,
      "learning_rate": 0.00017909054540605414,
      "loss": 0.8471,
      "step": 3925
    },
    {
      "epoch": 0.31408,
      "grad_norm": 0.39387014508247375,
      "learning_rate": 0.00017908521136151488,
      "loss": 0.6454,
      "step": 3926
    },
    {
      "epoch": 0.31416,
      "grad_norm": 0.4079238772392273,
      "learning_rate": 0.0001790798773169756,
      "loss": 0.653,
      "step": 3927
    },
    {
      "epoch": 0.31424,
      "grad_norm": 0.3353803753852844,
      "learning_rate": 0.00017907454327243633,
      "loss": 0.9949,
      "step": 3928
    },
    {
      "epoch": 0.31432,
      "grad_norm": 0.3000492453575134,
      "learning_rate": 0.00017906920922789707,
      "loss": 0.807,
      "step": 3929
    },
    {
      "epoch": 0.3144,
      "grad_norm": 0.3782431483268738,
      "learning_rate": 0.00017906387518335778,
      "loss": 1.1733,
      "step": 3930
    },
    {
      "epoch": 0.31448,
      "grad_norm": 0.3527286648750305,
      "learning_rate": 0.00017905854113881852,
      "loss": 1.0364,
      "step": 3931
    },
    {
      "epoch": 0.31456,
      "grad_norm": 0.36813658475875854,
      "learning_rate": 0.00017905320709427923,
      "loss": 0.8094,
      "step": 3932
    },
    {
      "epoch": 0.31464,
      "grad_norm": 0.4629760682582855,
      "learning_rate": 0.00017904787304973997,
      "loss": 0.9092,
      "step": 3933
    },
    {
      "epoch": 0.31472,
      "grad_norm": 0.4223119020462036,
      "learning_rate": 0.00017904253900520069,
      "loss": 0.9282,
      "step": 3934
    },
    {
      "epoch": 0.3148,
      "grad_norm": 0.3563321828842163,
      "learning_rate": 0.00017903720496066143,
      "loss": 0.9605,
      "step": 3935
    },
    {
      "epoch": 0.31488,
      "grad_norm": 0.3859553635120392,
      "learning_rate": 0.00017903187091612217,
      "loss": 0.8219,
      "step": 3936
    },
    {
      "epoch": 0.31496,
      "grad_norm": 0.37713614106178284,
      "learning_rate": 0.00017902653687158288,
      "loss": 0.7883,
      "step": 3937
    },
    {
      "epoch": 0.31504,
      "grad_norm": 0.3708152174949646,
      "learning_rate": 0.00017902120282704362,
      "loss": 0.7568,
      "step": 3938
    },
    {
      "epoch": 0.31512,
      "grad_norm": 0.3258802890777588,
      "learning_rate": 0.00017901586878250433,
      "loss": 1.0943,
      "step": 3939
    },
    {
      "epoch": 0.3152,
      "grad_norm": 0.5042648315429688,
      "learning_rate": 0.00017901053473796507,
      "loss": 0.8854,
      "step": 3940
    },
    {
      "epoch": 0.31528,
      "grad_norm": 0.3709414601325989,
      "learning_rate": 0.00017900520069342578,
      "loss": 0.907,
      "step": 3941
    },
    {
      "epoch": 0.31536,
      "grad_norm": 0.40331825613975525,
      "learning_rate": 0.00017899986664888652,
      "loss": 0.8904,
      "step": 3942
    },
    {
      "epoch": 0.31544,
      "grad_norm": 0.44094642996788025,
      "learning_rate": 0.00017899453260434726,
      "loss": 0.9307,
      "step": 3943
    },
    {
      "epoch": 0.31552,
      "grad_norm": 0.3266099989414215,
      "learning_rate": 0.00017898919855980798,
      "loss": 1.1186,
      "step": 3944
    },
    {
      "epoch": 0.3156,
      "grad_norm": 0.4011683464050293,
      "learning_rate": 0.00017898386451526872,
      "loss": 1.0865,
      "step": 3945
    },
    {
      "epoch": 0.31568,
      "grad_norm": 0.36824169754981995,
      "learning_rate": 0.00017897853047072943,
      "loss": 0.7041,
      "step": 3946
    },
    {
      "epoch": 0.31576,
      "grad_norm": 0.5391929149627686,
      "learning_rate": 0.00017897319642619017,
      "loss": 0.8192,
      "step": 3947
    },
    {
      "epoch": 0.31584,
      "grad_norm": 0.3833993673324585,
      "learning_rate": 0.00017896786238165088,
      "loss": 0.9021,
      "step": 3948
    },
    {
      "epoch": 0.31592,
      "grad_norm": 0.2989320158958435,
      "learning_rate": 0.00017896252833711162,
      "loss": 0.6328,
      "step": 3949
    },
    {
      "epoch": 0.316,
      "grad_norm": 0.42999592423439026,
      "learning_rate": 0.00017895719429257236,
      "loss": 0.9633,
      "step": 3950
    },
    {
      "epoch": 0.31608,
      "grad_norm": 0.40089932084083557,
      "learning_rate": 0.00017895186024803307,
      "loss": 1.0567,
      "step": 3951
    },
    {
      "epoch": 0.31616,
      "grad_norm": 0.3649844825267792,
      "learning_rate": 0.00017894652620349381,
      "loss": 0.6365,
      "step": 3952
    },
    {
      "epoch": 0.31624,
      "grad_norm": 0.398269385099411,
      "learning_rate": 0.00017894119215895453,
      "loss": 1.2795,
      "step": 3953
    },
    {
      "epoch": 0.31632,
      "grad_norm": 0.4384427070617676,
      "learning_rate": 0.00017893585811441527,
      "loss": 0.8841,
      "step": 3954
    },
    {
      "epoch": 0.3164,
      "grad_norm": 0.38315075635910034,
      "learning_rate": 0.00017893052406987598,
      "loss": 0.9607,
      "step": 3955
    },
    {
      "epoch": 0.31648,
      "grad_norm": 0.5053973197937012,
      "learning_rate": 0.00017892519002533672,
      "loss": 0.8355,
      "step": 3956
    },
    {
      "epoch": 0.31656,
      "grad_norm": 0.32105693221092224,
      "learning_rate": 0.00017891985598079746,
      "loss": 0.8426,
      "step": 3957
    },
    {
      "epoch": 0.31664,
      "grad_norm": 0.3746001720428467,
      "learning_rate": 0.00017891452193625817,
      "loss": 0.5877,
      "step": 3958
    },
    {
      "epoch": 0.31672,
      "grad_norm": 0.3274102210998535,
      "learning_rate": 0.0001789091878917189,
      "loss": 0.9255,
      "step": 3959
    },
    {
      "epoch": 0.3168,
      "grad_norm": 0.3759654760360718,
      "learning_rate": 0.00017890385384717962,
      "loss": 0.4924,
      "step": 3960
    },
    {
      "epoch": 0.31688,
      "grad_norm": 0.40031445026397705,
      "learning_rate": 0.00017889851980264036,
      "loss": 0.8235,
      "step": 3961
    },
    {
      "epoch": 0.31696,
      "grad_norm": 0.3761211037635803,
      "learning_rate": 0.00017889318575810108,
      "loss": 1.1672,
      "step": 3962
    },
    {
      "epoch": 0.31704,
      "grad_norm": 0.4573202133178711,
      "learning_rate": 0.00017888785171356182,
      "loss": 0.6283,
      "step": 3963
    },
    {
      "epoch": 0.31712,
      "grad_norm": 0.42719516158103943,
      "learning_rate": 0.00017888251766902256,
      "loss": 0.7254,
      "step": 3964
    },
    {
      "epoch": 0.3172,
      "grad_norm": 0.3670531213283539,
      "learning_rate": 0.00017887718362448327,
      "loss": 0.9156,
      "step": 3965
    },
    {
      "epoch": 0.31728,
      "grad_norm": 0.35036829113960266,
      "learning_rate": 0.000178871849579944,
      "loss": 0.9314,
      "step": 3966
    },
    {
      "epoch": 0.31736,
      "grad_norm": 0.29254719614982605,
      "learning_rate": 0.00017886651553540472,
      "loss": 0.8373,
      "step": 3967
    },
    {
      "epoch": 0.31744,
      "grad_norm": 0.44492384791374207,
      "learning_rate": 0.00017886118149086546,
      "loss": 0.8272,
      "step": 3968
    },
    {
      "epoch": 0.31752,
      "grad_norm": 0.38784071803092957,
      "learning_rate": 0.00017885584744632617,
      "loss": 1.0281,
      "step": 3969
    },
    {
      "epoch": 0.3176,
      "grad_norm": 0.38366037607192993,
      "learning_rate": 0.00017885051340178691,
      "loss": 0.9135,
      "step": 3970
    },
    {
      "epoch": 0.31768,
      "grad_norm": 0.376432865858078,
      "learning_rate": 0.00017884517935724765,
      "loss": 0.757,
      "step": 3971
    },
    {
      "epoch": 0.31776,
      "grad_norm": 0.49169862270355225,
      "learning_rate": 0.00017883984531270837,
      "loss": 0.7682,
      "step": 3972
    },
    {
      "epoch": 0.31784,
      "grad_norm": 0.39898762106895447,
      "learning_rate": 0.0001788345112681691,
      "loss": 0.9038,
      "step": 3973
    },
    {
      "epoch": 0.31792,
      "grad_norm": 0.3504106402397156,
      "learning_rate": 0.00017882917722362982,
      "loss": 1.037,
      "step": 3974
    },
    {
      "epoch": 0.318,
      "grad_norm": 0.36949077248573303,
      "learning_rate": 0.00017882384317909056,
      "loss": 0.9771,
      "step": 3975
    },
    {
      "epoch": 0.31808,
      "grad_norm": 0.3507448136806488,
      "learning_rate": 0.0001788185091345513,
      "loss": 0.5914,
      "step": 3976
    },
    {
      "epoch": 0.31816,
      "grad_norm": 0.4400343596935272,
      "learning_rate": 0.000178813175090012,
      "loss": 0.9034,
      "step": 3977
    },
    {
      "epoch": 0.31824,
      "grad_norm": 0.3562520146369934,
      "learning_rate": 0.00017880784104547275,
      "loss": 0.9005,
      "step": 3978
    },
    {
      "epoch": 0.31832,
      "grad_norm": 0.4269499182701111,
      "learning_rate": 0.00017880250700093346,
      "loss": 0.7041,
      "step": 3979
    },
    {
      "epoch": 0.3184,
      "grad_norm": 0.32676956057548523,
      "learning_rate": 0.0001787971729563942,
      "loss": 0.8842,
      "step": 3980
    },
    {
      "epoch": 0.31848,
      "grad_norm": 0.25767064094543457,
      "learning_rate": 0.00017879183891185492,
      "loss": 0.5523,
      "step": 3981
    },
    {
      "epoch": 0.31856,
      "grad_norm": 0.5191094875335693,
      "learning_rate": 0.00017878650486731566,
      "loss": 0.8107,
      "step": 3982
    },
    {
      "epoch": 0.31864,
      "grad_norm": 0.36153092980384827,
      "learning_rate": 0.0001787811708227764,
      "loss": 0.9341,
      "step": 3983
    },
    {
      "epoch": 0.31872,
      "grad_norm": 0.46672579646110535,
      "learning_rate": 0.0001787758367782371,
      "loss": 0.7776,
      "step": 3984
    },
    {
      "epoch": 0.3188,
      "grad_norm": 0.4638959467411041,
      "learning_rate": 0.00017877050273369785,
      "loss": 0.9817,
      "step": 3985
    },
    {
      "epoch": 0.31888,
      "grad_norm": 0.33607718348503113,
      "learning_rate": 0.00017876516868915856,
      "loss": 0.9574,
      "step": 3986
    },
    {
      "epoch": 0.31896,
      "grad_norm": 0.29063060879707336,
      "learning_rate": 0.0001787598346446193,
      "loss": 0.8514,
      "step": 3987
    },
    {
      "epoch": 0.31904,
      "grad_norm": 0.2745089530944824,
      "learning_rate": 0.00017875450060008001,
      "loss": 0.5508,
      "step": 3988
    },
    {
      "epoch": 0.31912,
      "grad_norm": 0.32492178678512573,
      "learning_rate": 0.00017874916655554075,
      "loss": 1.0364,
      "step": 3989
    },
    {
      "epoch": 0.3192,
      "grad_norm": 0.35806646943092346,
      "learning_rate": 0.0001787438325110015,
      "loss": 0.624,
      "step": 3990
    },
    {
      "epoch": 0.31928,
      "grad_norm": 0.3834846615791321,
      "learning_rate": 0.0001787384984664622,
      "loss": 0.8281,
      "step": 3991
    },
    {
      "epoch": 0.31936,
      "grad_norm": 0.3944576382637024,
      "learning_rate": 0.00017873316442192295,
      "loss": 0.7793,
      "step": 3992
    },
    {
      "epoch": 0.31944,
      "grad_norm": 0.3302689492702484,
      "learning_rate": 0.00017872783037738366,
      "loss": 0.7899,
      "step": 3993
    },
    {
      "epoch": 0.31952,
      "grad_norm": 0.42049384117126465,
      "learning_rate": 0.0001787224963328444,
      "loss": 1.0491,
      "step": 3994
    },
    {
      "epoch": 0.3196,
      "grad_norm": 0.4997255504131317,
      "learning_rate": 0.0001787171622883051,
      "loss": 0.867,
      "step": 3995
    },
    {
      "epoch": 0.31968,
      "grad_norm": 0.2742707133293152,
      "learning_rate": 0.00017871182824376585,
      "loss": 0.9071,
      "step": 3996
    },
    {
      "epoch": 0.31976,
      "grad_norm": 0.3628964126110077,
      "learning_rate": 0.0001787064941992266,
      "loss": 1.0322,
      "step": 3997
    },
    {
      "epoch": 0.31984,
      "grad_norm": 0.33297809958457947,
      "learning_rate": 0.0001787011601546873,
      "loss": 1.1706,
      "step": 3998
    },
    {
      "epoch": 0.31992,
      "grad_norm": 0.38277122378349304,
      "learning_rate": 0.00017869582611014804,
      "loss": 0.9496,
      "step": 3999
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.40781834721565247,
      "learning_rate": 0.00017869049206560876,
      "loss": 1.2114,
      "step": 4000
    },
    {
      "epoch": 0.32008,
      "grad_norm": 0.4024088382720947,
      "learning_rate": 0.0001786851580210695,
      "loss": 0.7999,
      "step": 4001
    },
    {
      "epoch": 0.32016,
      "grad_norm": 0.3254545331001282,
      "learning_rate": 0.0001786798239765302,
      "loss": 0.9926,
      "step": 4002
    },
    {
      "epoch": 0.32024,
      "grad_norm": 0.3539740741252899,
      "learning_rate": 0.00017867448993199095,
      "loss": 0.6429,
      "step": 4003
    },
    {
      "epoch": 0.32032,
      "grad_norm": 0.48498672246932983,
      "learning_rate": 0.0001786691558874517,
      "loss": 0.6184,
      "step": 4004
    },
    {
      "epoch": 0.3204,
      "grad_norm": 0.3013443946838379,
      "learning_rate": 0.0001786638218429124,
      "loss": 0.9032,
      "step": 4005
    },
    {
      "epoch": 0.32048,
      "grad_norm": 0.5227071046829224,
      "learning_rate": 0.00017865848779837314,
      "loss": 0.9284,
      "step": 4006
    },
    {
      "epoch": 0.32056,
      "grad_norm": 0.4957019090652466,
      "learning_rate": 0.00017865315375383386,
      "loss": 0.9355,
      "step": 4007
    },
    {
      "epoch": 0.32064,
      "grad_norm": 0.33342820405960083,
      "learning_rate": 0.0001786478197092946,
      "loss": 0.6758,
      "step": 4008
    },
    {
      "epoch": 0.32072,
      "grad_norm": 0.3975563645362854,
      "learning_rate": 0.0001786424856647553,
      "loss": 0.9573,
      "step": 4009
    },
    {
      "epoch": 0.3208,
      "grad_norm": 0.4032089114189148,
      "learning_rate": 0.00017863715162021605,
      "loss": 0.8621,
      "step": 4010
    },
    {
      "epoch": 0.32088,
      "grad_norm": 0.3212025463581085,
      "learning_rate": 0.0001786318175756768,
      "loss": 0.8932,
      "step": 4011
    },
    {
      "epoch": 0.32096,
      "grad_norm": 0.36337968707084656,
      "learning_rate": 0.0001786264835311375,
      "loss": 0.7183,
      "step": 4012
    },
    {
      "epoch": 0.32104,
      "grad_norm": 0.38708606362342834,
      "learning_rate": 0.00017862114948659824,
      "loss": 0.6221,
      "step": 4013
    },
    {
      "epoch": 0.32112,
      "grad_norm": 0.3371298015117645,
      "learning_rate": 0.00017861581544205895,
      "loss": 0.7062,
      "step": 4014
    },
    {
      "epoch": 0.3212,
      "grad_norm": 0.340718150138855,
      "learning_rate": 0.0001786104813975197,
      "loss": 0.7706,
      "step": 4015
    },
    {
      "epoch": 0.32128,
      "grad_norm": 0.3816606104373932,
      "learning_rate": 0.0001786051473529804,
      "loss": 1.0146,
      "step": 4016
    },
    {
      "epoch": 0.32136,
      "grad_norm": 0.32223498821258545,
      "learning_rate": 0.00017859981330844115,
      "loss": 1.031,
      "step": 4017
    },
    {
      "epoch": 0.32144,
      "grad_norm": 0.40561702847480774,
      "learning_rate": 0.00017859447926390186,
      "loss": 0.6812,
      "step": 4018
    },
    {
      "epoch": 0.32152,
      "grad_norm": 0.421818345785141,
      "learning_rate": 0.0001785891452193626,
      "loss": 0.7081,
      "step": 4019
    },
    {
      "epoch": 0.3216,
      "grad_norm": 0.419892281293869,
      "learning_rate": 0.0001785838111748233,
      "loss": 0.9002,
      "step": 4020
    },
    {
      "epoch": 0.32168,
      "grad_norm": 0.33723223209381104,
      "learning_rate": 0.00017857847713028405,
      "loss": 0.8646,
      "step": 4021
    },
    {
      "epoch": 0.32176,
      "grad_norm": 0.3765513300895691,
      "learning_rate": 0.00017857314308574476,
      "loss": 0.6687,
      "step": 4022
    },
    {
      "epoch": 0.32184,
      "grad_norm": 0.41667136549949646,
      "learning_rate": 0.0001785678090412055,
      "loss": 0.6842,
      "step": 4023
    },
    {
      "epoch": 0.32192,
      "grad_norm": 0.39319026470184326,
      "learning_rate": 0.00017856247499666622,
      "loss": 0.6956,
      "step": 4024
    },
    {
      "epoch": 0.322,
      "grad_norm": 0.42870819568634033,
      "learning_rate": 0.00017855714095212696,
      "loss": 0.6769,
      "step": 4025
    },
    {
      "epoch": 0.32208,
      "grad_norm": 0.3337508738040924,
      "learning_rate": 0.0001785518069075877,
      "loss": 0.5995,
      "step": 4026
    },
    {
      "epoch": 0.32216,
      "grad_norm": 0.3645893335342407,
      "learning_rate": 0.0001785464728630484,
      "loss": 0.7869,
      "step": 4027
    },
    {
      "epoch": 0.32224,
      "grad_norm": 0.3773653507232666,
      "learning_rate": 0.00017854113881850915,
      "loss": 1.139,
      "step": 4028
    },
    {
      "epoch": 0.32232,
      "grad_norm": 0.347289115190506,
      "learning_rate": 0.00017853580477396986,
      "loss": 0.8423,
      "step": 4029
    },
    {
      "epoch": 0.3224,
      "grad_norm": 0.5618217587471008,
      "learning_rate": 0.0001785304707294306,
      "loss": 0.8486,
      "step": 4030
    },
    {
      "epoch": 0.32248,
      "grad_norm": 0.3986320495605469,
      "learning_rate": 0.0001785251366848913,
      "loss": 0.7139,
      "step": 4031
    },
    {
      "epoch": 0.32256,
      "grad_norm": 0.33966773748397827,
      "learning_rate": 0.00017851980264035205,
      "loss": 0.5722,
      "step": 4032
    },
    {
      "epoch": 0.32264,
      "grad_norm": 0.4337129294872284,
      "learning_rate": 0.00017851446859581277,
      "loss": 0.8385,
      "step": 4033
    },
    {
      "epoch": 0.32272,
      "grad_norm": 0.34891924262046814,
      "learning_rate": 0.0001785091345512735,
      "loss": 0.8767,
      "step": 4034
    },
    {
      "epoch": 0.3228,
      "grad_norm": 0.5138195157051086,
      "learning_rate": 0.00017850380050673422,
      "loss": 1.1296,
      "step": 4035
    },
    {
      "epoch": 0.32288,
      "grad_norm": 0.35503944754600525,
      "learning_rate": 0.00017849846646219496,
      "loss": 0.7288,
      "step": 4036
    },
    {
      "epoch": 0.32296,
      "grad_norm": 0.3395674526691437,
      "learning_rate": 0.0001784931324176557,
      "loss": 0.9938,
      "step": 4037
    },
    {
      "epoch": 0.32304,
      "grad_norm": 0.34368255734443665,
      "learning_rate": 0.0001784877983731164,
      "loss": 1.1959,
      "step": 4038
    },
    {
      "epoch": 0.32312,
      "grad_norm": 0.35695114731788635,
      "learning_rate": 0.00017848246432857715,
      "loss": 1.154,
      "step": 4039
    },
    {
      "epoch": 0.3232,
      "grad_norm": 0.4605084955692291,
      "learning_rate": 0.00017847713028403786,
      "loss": 0.9717,
      "step": 4040
    },
    {
      "epoch": 0.32328,
      "grad_norm": 0.29146337509155273,
      "learning_rate": 0.0001784717962394986,
      "loss": 0.5202,
      "step": 4041
    },
    {
      "epoch": 0.32336,
      "grad_norm": 0.36533892154693604,
      "learning_rate": 0.00017846646219495932,
      "loss": 0.8744,
      "step": 4042
    },
    {
      "epoch": 0.32344,
      "grad_norm": 0.443672776222229,
      "learning_rate": 0.00017846112815042006,
      "loss": 0.9933,
      "step": 4043
    },
    {
      "epoch": 0.32352,
      "grad_norm": 0.5572291016578674,
      "learning_rate": 0.0001784557941058808,
      "loss": 1.1699,
      "step": 4044
    },
    {
      "epoch": 0.3236,
      "grad_norm": 0.28904712200164795,
      "learning_rate": 0.0001784504600613415,
      "loss": 0.6505,
      "step": 4045
    },
    {
      "epoch": 0.32368,
      "grad_norm": 0.49474984407424927,
      "learning_rate": 0.00017844512601680225,
      "loss": 0.7737,
      "step": 4046
    },
    {
      "epoch": 0.32376,
      "grad_norm": 0.37619200348854065,
      "learning_rate": 0.00017843979197226296,
      "loss": 1.0361,
      "step": 4047
    },
    {
      "epoch": 0.32384,
      "grad_norm": 0.36928167939186096,
      "learning_rate": 0.0001784344579277237,
      "loss": 0.6918,
      "step": 4048
    },
    {
      "epoch": 0.32392,
      "grad_norm": 0.5141335129737854,
      "learning_rate": 0.00017842912388318441,
      "loss": 0.8121,
      "step": 4049
    },
    {
      "epoch": 0.324,
      "grad_norm": 0.39363572001457214,
      "learning_rate": 0.00017842378983864515,
      "loss": 0.6614,
      "step": 4050
    },
    {
      "epoch": 0.32408,
      "grad_norm": 0.38063836097717285,
      "learning_rate": 0.0001784184557941059,
      "loss": 0.7689,
      "step": 4051
    },
    {
      "epoch": 0.32416,
      "grad_norm": 0.38192033767700195,
      "learning_rate": 0.0001784131217495666,
      "loss": 0.9063,
      "step": 4052
    },
    {
      "epoch": 0.32424,
      "grad_norm": 0.44809573888778687,
      "learning_rate": 0.00017840778770502735,
      "loss": 0.9015,
      "step": 4053
    },
    {
      "epoch": 0.32432,
      "grad_norm": 0.35604387521743774,
      "learning_rate": 0.00017840245366048806,
      "loss": 0.8925,
      "step": 4054
    },
    {
      "epoch": 0.3244,
      "grad_norm": 0.3688453435897827,
      "learning_rate": 0.0001783971196159488,
      "loss": 0.8117,
      "step": 4055
    },
    {
      "epoch": 0.32448,
      "grad_norm": 0.28559231758117676,
      "learning_rate": 0.0001783917855714095,
      "loss": 0.6707,
      "step": 4056
    },
    {
      "epoch": 0.32456,
      "grad_norm": 0.3806304931640625,
      "learning_rate": 0.00017838645152687025,
      "loss": 0.793,
      "step": 4057
    },
    {
      "epoch": 0.32464,
      "grad_norm": 0.35796645283699036,
      "learning_rate": 0.000178381117482331,
      "loss": 0.9462,
      "step": 4058
    },
    {
      "epoch": 0.32472,
      "grad_norm": 0.38405168056488037,
      "learning_rate": 0.0001783757834377917,
      "loss": 0.8698,
      "step": 4059
    },
    {
      "epoch": 0.3248,
      "grad_norm": 0.41425809264183044,
      "learning_rate": 0.00017837044939325244,
      "loss": 0.9407,
      "step": 4060
    },
    {
      "epoch": 0.32488,
      "grad_norm": 0.2710129916667938,
      "learning_rate": 0.00017836511534871316,
      "loss": 0.8413,
      "step": 4061
    },
    {
      "epoch": 0.32496,
      "grad_norm": 0.38788723945617676,
      "learning_rate": 0.0001783597813041739,
      "loss": 0.6103,
      "step": 4062
    },
    {
      "epoch": 0.32504,
      "grad_norm": 0.3418675363063812,
      "learning_rate": 0.0001783544472596346,
      "loss": 0.9708,
      "step": 4063
    },
    {
      "epoch": 0.32512,
      "grad_norm": 0.48378726840019226,
      "learning_rate": 0.00017834911321509535,
      "loss": 1.1809,
      "step": 4064
    },
    {
      "epoch": 0.3252,
      "grad_norm": 0.3799324035644531,
      "learning_rate": 0.0001783437791705561,
      "loss": 0.958,
      "step": 4065
    },
    {
      "epoch": 0.32528,
      "grad_norm": 0.3477305769920349,
      "learning_rate": 0.0001783384451260168,
      "loss": 0.7918,
      "step": 4066
    },
    {
      "epoch": 0.32536,
      "grad_norm": 0.390126496553421,
      "learning_rate": 0.00017833311108147754,
      "loss": 0.8667,
      "step": 4067
    },
    {
      "epoch": 0.32544,
      "grad_norm": 0.44228804111480713,
      "learning_rate": 0.00017832777703693825,
      "loss": 0.8011,
      "step": 4068
    },
    {
      "epoch": 0.32552,
      "grad_norm": 0.3547585606575012,
      "learning_rate": 0.000178322442992399,
      "loss": 0.9871,
      "step": 4069
    },
    {
      "epoch": 0.3256,
      "grad_norm": 0.4165344834327698,
      "learning_rate": 0.0001783171089478597,
      "loss": 0.7085,
      "step": 4070
    },
    {
      "epoch": 0.32568,
      "grad_norm": 0.34910574555397034,
      "learning_rate": 0.00017831177490332045,
      "loss": 0.6165,
      "step": 4071
    },
    {
      "epoch": 0.32576,
      "grad_norm": 0.5003367066383362,
      "learning_rate": 0.00017830644085878119,
      "loss": 0.8346,
      "step": 4072
    },
    {
      "epoch": 0.32584,
      "grad_norm": 0.3620705306529999,
      "learning_rate": 0.0001783011068142419,
      "loss": 0.9858,
      "step": 4073
    },
    {
      "epoch": 0.32592,
      "grad_norm": 0.3619180917739868,
      "learning_rate": 0.00017829577276970264,
      "loss": 0.8994,
      "step": 4074
    },
    {
      "epoch": 0.326,
      "grad_norm": 0.44752246141433716,
      "learning_rate": 0.00017829043872516335,
      "loss": 0.9214,
      "step": 4075
    },
    {
      "epoch": 0.32608,
      "grad_norm": 0.3377698063850403,
      "learning_rate": 0.0001782851046806241,
      "loss": 0.9257,
      "step": 4076
    },
    {
      "epoch": 0.32616,
      "grad_norm": 0.361332505941391,
      "learning_rate": 0.00017827977063608483,
      "loss": 1.0618,
      "step": 4077
    },
    {
      "epoch": 0.32624,
      "grad_norm": 0.39728423953056335,
      "learning_rate": 0.00017827443659154554,
      "loss": 1.0327,
      "step": 4078
    },
    {
      "epoch": 0.32632,
      "grad_norm": 0.43655699491500854,
      "learning_rate": 0.00017826910254700628,
      "loss": 0.869,
      "step": 4079
    },
    {
      "epoch": 0.3264,
      "grad_norm": 0.40928876399993896,
      "learning_rate": 0.000178263768502467,
      "loss": 0.7161,
      "step": 4080
    },
    {
      "epoch": 0.32648,
      "grad_norm": 0.37311336398124695,
      "learning_rate": 0.00017825843445792774,
      "loss": 0.9865,
      "step": 4081
    },
    {
      "epoch": 0.32656,
      "grad_norm": 0.3660587966442108,
      "learning_rate": 0.00017825310041338845,
      "loss": 0.6702,
      "step": 4082
    },
    {
      "epoch": 0.32664,
      "grad_norm": 0.35920828580856323,
      "learning_rate": 0.0001782477663688492,
      "loss": 0.815,
      "step": 4083
    },
    {
      "epoch": 0.32672,
      "grad_norm": 0.5612649917602539,
      "learning_rate": 0.00017824243232430993,
      "loss": 1.1503,
      "step": 4084
    },
    {
      "epoch": 0.3268,
      "grad_norm": 0.34004732966423035,
      "learning_rate": 0.00017823709827977064,
      "loss": 0.653,
      "step": 4085
    },
    {
      "epoch": 0.32688,
      "grad_norm": 0.44155406951904297,
      "learning_rate": 0.00017823176423523138,
      "loss": 0.9168,
      "step": 4086
    },
    {
      "epoch": 0.32696,
      "grad_norm": 0.33412447571754456,
      "learning_rate": 0.0001782264301906921,
      "loss": 0.7326,
      "step": 4087
    },
    {
      "epoch": 0.32704,
      "grad_norm": 0.3482505977153778,
      "learning_rate": 0.00017822109614615283,
      "loss": 0.8697,
      "step": 4088
    },
    {
      "epoch": 0.32712,
      "grad_norm": 0.40533319115638733,
      "learning_rate": 0.00017821576210161355,
      "loss": 0.843,
      "step": 4089
    },
    {
      "epoch": 0.3272,
      "grad_norm": 0.34562042355537415,
      "learning_rate": 0.0001782104280570743,
      "loss": 1.1056,
      "step": 4090
    },
    {
      "epoch": 0.32728,
      "grad_norm": 0.30078622698783875,
      "learning_rate": 0.00017820509401253503,
      "loss": 1.1384,
      "step": 4091
    },
    {
      "epoch": 0.32736,
      "grad_norm": 0.3439312279224396,
      "learning_rate": 0.00017819975996799574,
      "loss": 0.7077,
      "step": 4092
    },
    {
      "epoch": 0.32744,
      "grad_norm": 0.30654653906822205,
      "learning_rate": 0.00017819442592345648,
      "loss": 0.9511,
      "step": 4093
    },
    {
      "epoch": 0.32752,
      "grad_norm": 0.4532029628753662,
      "learning_rate": 0.0001781890918789172,
      "loss": 0.7444,
      "step": 4094
    },
    {
      "epoch": 0.3276,
      "grad_norm": 0.3609422743320465,
      "learning_rate": 0.00017818375783437793,
      "loss": 1.0978,
      "step": 4095
    },
    {
      "epoch": 0.32768,
      "grad_norm": 0.44482824206352234,
      "learning_rate": 0.00017817842378983864,
      "loss": 0.8526,
      "step": 4096
    },
    {
      "epoch": 0.32776,
      "grad_norm": 0.33487236499786377,
      "learning_rate": 0.00017817308974529938,
      "loss": 1.0067,
      "step": 4097
    },
    {
      "epoch": 0.32784,
      "grad_norm": 0.3616499900817871,
      "learning_rate": 0.00017816775570076012,
      "loss": 0.7266,
      "step": 4098
    },
    {
      "epoch": 0.32792,
      "grad_norm": 0.3176024854183197,
      "learning_rate": 0.00017816242165622084,
      "loss": 0.7323,
      "step": 4099
    },
    {
      "epoch": 0.328,
      "grad_norm": 0.30445629358291626,
      "learning_rate": 0.00017815708761168158,
      "loss": 1.181,
      "step": 4100
    },
    {
      "epoch": 0.32808,
      "grad_norm": 0.290615439414978,
      "learning_rate": 0.0001781517535671423,
      "loss": 0.9766,
      "step": 4101
    },
    {
      "epoch": 0.32816,
      "grad_norm": 0.3684491217136383,
      "learning_rate": 0.00017814641952260303,
      "loss": 0.8788,
      "step": 4102
    },
    {
      "epoch": 0.32824,
      "grad_norm": 0.423002153635025,
      "learning_rate": 0.00017814108547806374,
      "loss": 0.7602,
      "step": 4103
    },
    {
      "epoch": 0.32832,
      "grad_norm": 0.38189780712127686,
      "learning_rate": 0.00017813575143352448,
      "loss": 0.593,
      "step": 4104
    },
    {
      "epoch": 0.3284,
      "grad_norm": 0.37099534273147583,
      "learning_rate": 0.00017813041738898522,
      "loss": 0.6557,
      "step": 4105
    },
    {
      "epoch": 0.32848,
      "grad_norm": 0.35122764110565186,
      "learning_rate": 0.00017812508334444593,
      "loss": 0.7655,
      "step": 4106
    },
    {
      "epoch": 0.32856,
      "grad_norm": 0.3666975498199463,
      "learning_rate": 0.00017811974929990667,
      "loss": 0.9799,
      "step": 4107
    },
    {
      "epoch": 0.32864,
      "grad_norm": 0.29782670736312866,
      "learning_rate": 0.0001781144152553674,
      "loss": 0.6985,
      "step": 4108
    },
    {
      "epoch": 0.32872,
      "grad_norm": 0.36553987860679626,
      "learning_rate": 0.00017810908121082813,
      "loss": 0.6101,
      "step": 4109
    },
    {
      "epoch": 0.3288,
      "grad_norm": 0.3422071039676666,
      "learning_rate": 0.00017810374716628884,
      "loss": 0.6326,
      "step": 4110
    },
    {
      "epoch": 0.32888,
      "grad_norm": 0.3373556137084961,
      "learning_rate": 0.00017809841312174958,
      "loss": 0.5574,
      "step": 4111
    },
    {
      "epoch": 0.32896,
      "grad_norm": 0.3854144215583801,
      "learning_rate": 0.00017809307907721032,
      "loss": 1.0486,
      "step": 4112
    },
    {
      "epoch": 0.32904,
      "grad_norm": 0.34168195724487305,
      "learning_rate": 0.00017808774503267103,
      "loss": 1.106,
      "step": 4113
    },
    {
      "epoch": 0.32912,
      "grad_norm": 0.42589008808135986,
      "learning_rate": 0.00017808241098813177,
      "loss": 0.7785,
      "step": 4114
    },
    {
      "epoch": 0.3292,
      "grad_norm": 0.3529016673564911,
      "learning_rate": 0.00017807707694359248,
      "loss": 0.7666,
      "step": 4115
    },
    {
      "epoch": 0.32928,
      "grad_norm": 0.4861002564430237,
      "learning_rate": 0.00017807174289905322,
      "loss": 0.891,
      "step": 4116
    },
    {
      "epoch": 0.32936,
      "grad_norm": 0.36322689056396484,
      "learning_rate": 0.00017806640885451394,
      "loss": 0.7737,
      "step": 4117
    },
    {
      "epoch": 0.32944,
      "grad_norm": 0.4954933226108551,
      "learning_rate": 0.00017806107480997468,
      "loss": 1.14,
      "step": 4118
    },
    {
      "epoch": 0.32952,
      "grad_norm": 0.36318087577819824,
      "learning_rate": 0.00017805574076543542,
      "loss": 0.9044,
      "step": 4119
    },
    {
      "epoch": 0.3296,
      "grad_norm": 0.45323771238327026,
      "learning_rate": 0.00017805040672089613,
      "loss": 1.1044,
      "step": 4120
    },
    {
      "epoch": 0.32968,
      "grad_norm": 0.3644237816333771,
      "learning_rate": 0.00017804507267635687,
      "loss": 0.9571,
      "step": 4121
    },
    {
      "epoch": 0.32976,
      "grad_norm": 0.29929086565971375,
      "learning_rate": 0.00017803973863181758,
      "loss": 0.7366,
      "step": 4122
    },
    {
      "epoch": 0.32984,
      "grad_norm": 0.27573612332344055,
      "learning_rate": 0.00017803440458727832,
      "loss": 0.4784,
      "step": 4123
    },
    {
      "epoch": 0.32992,
      "grad_norm": 0.32271549105644226,
      "learning_rate": 0.00017802907054273906,
      "loss": 0.5599,
      "step": 4124
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.4256030321121216,
      "learning_rate": 0.00017802373649819977,
      "loss": 0.8501,
      "step": 4125
    },
    {
      "epoch": 0.33008,
      "grad_norm": 0.4271220564842224,
      "learning_rate": 0.00017801840245366051,
      "loss": 0.8199,
      "step": 4126
    },
    {
      "epoch": 0.33016,
      "grad_norm": 0.5597627758979797,
      "learning_rate": 0.00017801306840912123,
      "loss": 1.0528,
      "step": 4127
    },
    {
      "epoch": 0.33024,
      "grad_norm": 0.31670162081718445,
      "learning_rate": 0.00017800773436458197,
      "loss": 0.5928,
      "step": 4128
    },
    {
      "epoch": 0.33032,
      "grad_norm": 0.28921064734458923,
      "learning_rate": 0.00017800240032004268,
      "loss": 1.015,
      "step": 4129
    },
    {
      "epoch": 0.3304,
      "grad_norm": 0.4518836736679077,
      "learning_rate": 0.00017799706627550342,
      "loss": 0.7199,
      "step": 4130
    },
    {
      "epoch": 0.33048,
      "grad_norm": 0.36507782340049744,
      "learning_rate": 0.00017799173223096416,
      "loss": 1.1074,
      "step": 4131
    },
    {
      "epoch": 0.33056,
      "grad_norm": 0.4175003468990326,
      "learning_rate": 0.00017798639818642487,
      "loss": 1.2758,
      "step": 4132
    },
    {
      "epoch": 0.33064,
      "grad_norm": 0.4101105332374573,
      "learning_rate": 0.0001779810641418856,
      "loss": 0.7018,
      "step": 4133
    },
    {
      "epoch": 0.33072,
      "grad_norm": 0.4223499596118927,
      "learning_rate": 0.00017797573009734632,
      "loss": 0.9174,
      "step": 4134
    },
    {
      "epoch": 0.3308,
      "grad_norm": 0.40753766894340515,
      "learning_rate": 0.00017797039605280706,
      "loss": 0.6201,
      "step": 4135
    },
    {
      "epoch": 0.33088,
      "grad_norm": 0.38968542218208313,
      "learning_rate": 0.00017796506200826778,
      "loss": 1.1408,
      "step": 4136
    },
    {
      "epoch": 0.33096,
      "grad_norm": 0.3218420743942261,
      "learning_rate": 0.00017795972796372852,
      "loss": 0.8733,
      "step": 4137
    },
    {
      "epoch": 0.33104,
      "grad_norm": 0.3200685679912567,
      "learning_rate": 0.00017795439391918926,
      "loss": 0.6048,
      "step": 4138
    },
    {
      "epoch": 0.33112,
      "grad_norm": 0.33037319779396057,
      "learning_rate": 0.00017794905987464997,
      "loss": 0.559,
      "step": 4139
    },
    {
      "epoch": 0.3312,
      "grad_norm": 0.36762386560440063,
      "learning_rate": 0.0001779437258301107,
      "loss": 0.6364,
      "step": 4140
    },
    {
      "epoch": 0.33128,
      "grad_norm": 0.2565496563911438,
      "learning_rate": 0.00017793839178557142,
      "loss": 0.6656,
      "step": 4141
    },
    {
      "epoch": 0.33136,
      "grad_norm": 0.3765428066253662,
      "learning_rate": 0.00017793305774103216,
      "loss": 0.7851,
      "step": 4142
    },
    {
      "epoch": 0.33144,
      "grad_norm": 0.3519103527069092,
      "learning_rate": 0.00017792772369649288,
      "loss": 0.9868,
      "step": 4143
    },
    {
      "epoch": 0.33152,
      "grad_norm": 0.4088604152202606,
      "learning_rate": 0.00017792238965195362,
      "loss": 0.6953,
      "step": 4144
    },
    {
      "epoch": 0.3316,
      "grad_norm": 0.341229110956192,
      "learning_rate": 0.00017791705560741433,
      "loss": 0.8884,
      "step": 4145
    },
    {
      "epoch": 0.33168,
      "grad_norm": 0.4381580650806427,
      "learning_rate": 0.00017791172156287507,
      "loss": 0.696,
      "step": 4146
    },
    {
      "epoch": 0.33176,
      "grad_norm": 0.38927578926086426,
      "learning_rate": 0.00017790638751833578,
      "loss": 0.8388,
      "step": 4147
    },
    {
      "epoch": 0.33184,
      "grad_norm": 0.37492895126342773,
      "learning_rate": 0.00017790105347379652,
      "loss": 1.1542,
      "step": 4148
    },
    {
      "epoch": 0.33192,
      "grad_norm": 0.31487834453582764,
      "learning_rate": 0.00017789571942925723,
      "loss": 0.6078,
      "step": 4149
    },
    {
      "epoch": 0.332,
      "grad_norm": 0.3179258108139038,
      "learning_rate": 0.00017789038538471797,
      "loss": 0.9895,
      "step": 4150
    },
    {
      "epoch": 0.33208,
      "grad_norm": 0.3996388018131256,
      "learning_rate": 0.00017788505134017869,
      "loss": 0.7891,
      "step": 4151
    },
    {
      "epoch": 0.33216,
      "grad_norm": 0.5914068818092346,
      "learning_rate": 0.00017787971729563943,
      "loss": 0.6708,
      "step": 4152
    },
    {
      "epoch": 0.33224,
      "grad_norm": 0.42496457695961,
      "learning_rate": 0.00017787438325110017,
      "loss": 0.9065,
      "step": 4153
    },
    {
      "epoch": 0.33232,
      "grad_norm": 0.46298640966415405,
      "learning_rate": 0.00017786904920656088,
      "loss": 1.5287,
      "step": 4154
    },
    {
      "epoch": 0.3324,
      "grad_norm": 0.43777790665626526,
      "learning_rate": 0.00017786371516202162,
      "loss": 1.0967,
      "step": 4155
    },
    {
      "epoch": 0.33248,
      "grad_norm": 0.43455109000205994,
      "learning_rate": 0.00017785838111748233,
      "loss": 0.9133,
      "step": 4156
    },
    {
      "epoch": 0.33256,
      "grad_norm": 0.37066730856895447,
      "learning_rate": 0.00017785304707294307,
      "loss": 0.7803,
      "step": 4157
    },
    {
      "epoch": 0.33264,
      "grad_norm": 0.4132497012615204,
      "learning_rate": 0.00017784771302840378,
      "loss": 0.7573,
      "step": 4158
    },
    {
      "epoch": 0.33272,
      "grad_norm": 0.46888267993927,
      "learning_rate": 0.00017784237898386452,
      "loss": 0.9743,
      "step": 4159
    },
    {
      "epoch": 0.3328,
      "grad_norm": 0.5419130921363831,
      "learning_rate": 0.00017783704493932524,
      "loss": 0.7448,
      "step": 4160
    },
    {
      "epoch": 0.33288,
      "grad_norm": 0.3328070044517517,
      "learning_rate": 0.00017783171089478598,
      "loss": 1.1802,
      "step": 4161
    },
    {
      "epoch": 0.33296,
      "grad_norm": 0.4090666174888611,
      "learning_rate": 0.0001778263768502467,
      "loss": 0.7429,
      "step": 4162
    },
    {
      "epoch": 0.33304,
      "grad_norm": 0.39961767196655273,
      "learning_rate": 0.00017782104280570743,
      "loss": 0.7121,
      "step": 4163
    },
    {
      "epoch": 0.33312,
      "grad_norm": 0.421675443649292,
      "learning_rate": 0.00017781570876116814,
      "loss": 0.7674,
      "step": 4164
    },
    {
      "epoch": 0.3332,
      "grad_norm": 0.3842213749885559,
      "learning_rate": 0.00017781037471662888,
      "loss": 0.8385,
      "step": 4165
    },
    {
      "epoch": 0.33328,
      "grad_norm": 0.33642420172691345,
      "learning_rate": 0.00017780504067208962,
      "loss": 0.7941,
      "step": 4166
    },
    {
      "epoch": 0.33336,
      "grad_norm": 0.34342315793037415,
      "learning_rate": 0.00017779970662755033,
      "loss": 1.0272,
      "step": 4167
    },
    {
      "epoch": 0.33344,
      "grad_norm": 0.2721102237701416,
      "learning_rate": 0.00017779437258301107,
      "loss": 0.8555,
      "step": 4168
    },
    {
      "epoch": 0.33352,
      "grad_norm": 0.3775078058242798,
      "learning_rate": 0.00017778903853847179,
      "loss": 0.7848,
      "step": 4169
    },
    {
      "epoch": 0.3336,
      "grad_norm": 0.3807523846626282,
      "learning_rate": 0.00017778370449393253,
      "loss": 0.6267,
      "step": 4170
    },
    {
      "epoch": 0.33368,
      "grad_norm": 0.38409700989723206,
      "learning_rate": 0.00017777837044939324,
      "loss": 1.0889,
      "step": 4171
    },
    {
      "epoch": 0.33376,
      "grad_norm": 0.38239315152168274,
      "learning_rate": 0.00017777303640485398,
      "loss": 0.6835,
      "step": 4172
    },
    {
      "epoch": 0.33384,
      "grad_norm": 0.3433641195297241,
      "learning_rate": 0.00017776770236031472,
      "loss": 1.1525,
      "step": 4173
    },
    {
      "epoch": 0.33392,
      "grad_norm": 0.29853948950767517,
      "learning_rate": 0.00017776236831577543,
      "loss": 0.5235,
      "step": 4174
    },
    {
      "epoch": 0.334,
      "grad_norm": 0.42729949951171875,
      "learning_rate": 0.00017775703427123617,
      "loss": 0.8533,
      "step": 4175
    },
    {
      "epoch": 0.33408,
      "grad_norm": 0.3880999684333801,
      "learning_rate": 0.00017775170022669688,
      "loss": 0.927,
      "step": 4176
    },
    {
      "epoch": 0.33416,
      "grad_norm": 0.43288010358810425,
      "learning_rate": 0.00017774636618215762,
      "loss": 0.8615,
      "step": 4177
    },
    {
      "epoch": 0.33424,
      "grad_norm": 0.31711727380752563,
      "learning_rate": 0.00017774103213761836,
      "loss": 0.5413,
      "step": 4178
    },
    {
      "epoch": 0.33432,
      "grad_norm": 0.4481821656227112,
      "learning_rate": 0.00017773569809307908,
      "loss": 0.9882,
      "step": 4179
    },
    {
      "epoch": 0.3344,
      "grad_norm": 0.5225058197975159,
      "learning_rate": 0.00017773036404853982,
      "loss": 0.984,
      "step": 4180
    },
    {
      "epoch": 0.33448,
      "grad_norm": 0.34256482124328613,
      "learning_rate": 0.00017772503000400053,
      "loss": 0.7643,
      "step": 4181
    },
    {
      "epoch": 0.33456,
      "grad_norm": 0.39389756321907043,
      "learning_rate": 0.00017771969595946127,
      "loss": 0.8063,
      "step": 4182
    },
    {
      "epoch": 0.33464,
      "grad_norm": 0.3839201033115387,
      "learning_rate": 0.00017771436191492198,
      "loss": 0.612,
      "step": 4183
    },
    {
      "epoch": 0.33472,
      "grad_norm": 0.4017809331417084,
      "learning_rate": 0.00017770902787038272,
      "loss": 0.6557,
      "step": 4184
    },
    {
      "epoch": 0.3348,
      "grad_norm": 0.3851527273654938,
      "learning_rate": 0.00017770369382584346,
      "loss": 0.9246,
      "step": 4185
    },
    {
      "epoch": 0.33488,
      "grad_norm": 0.38160422444343567,
      "learning_rate": 0.00017769835978130417,
      "loss": 0.7619,
      "step": 4186
    },
    {
      "epoch": 0.33496,
      "grad_norm": 0.4226432144641876,
      "learning_rate": 0.0001776930257367649,
      "loss": 0.8911,
      "step": 4187
    },
    {
      "epoch": 0.33504,
      "grad_norm": 0.39372509717941284,
      "learning_rate": 0.00017768769169222563,
      "loss": 0.6802,
      "step": 4188
    },
    {
      "epoch": 0.33512,
      "grad_norm": 0.44310760498046875,
      "learning_rate": 0.00017768235764768637,
      "loss": 1.1324,
      "step": 4189
    },
    {
      "epoch": 0.3352,
      "grad_norm": 0.3592202663421631,
      "learning_rate": 0.00017767702360314708,
      "loss": 0.879,
      "step": 4190
    },
    {
      "epoch": 0.33528,
      "grad_norm": 0.5079615116119385,
      "learning_rate": 0.00017767168955860782,
      "loss": 0.8477,
      "step": 4191
    },
    {
      "epoch": 0.33536,
      "grad_norm": 0.39064764976501465,
      "learning_rate": 0.00017766635551406856,
      "loss": 1.1185,
      "step": 4192
    },
    {
      "epoch": 0.33544,
      "grad_norm": 0.39088159799575806,
      "learning_rate": 0.00017766102146952927,
      "loss": 0.7528,
      "step": 4193
    },
    {
      "epoch": 0.33552,
      "grad_norm": 0.39836230874061584,
      "learning_rate": 0.00017765568742499,
      "loss": 1.0196,
      "step": 4194
    },
    {
      "epoch": 0.3356,
      "grad_norm": 0.36638885736465454,
      "learning_rate": 0.00017765035338045072,
      "loss": 0.7594,
      "step": 4195
    },
    {
      "epoch": 0.33568,
      "grad_norm": 0.426011323928833,
      "learning_rate": 0.00017764501933591146,
      "loss": 0.7205,
      "step": 4196
    },
    {
      "epoch": 0.33576,
      "grad_norm": 0.4099293351173401,
      "learning_rate": 0.00017763968529137218,
      "loss": 1.0227,
      "step": 4197
    },
    {
      "epoch": 0.33584,
      "grad_norm": 0.468472957611084,
      "learning_rate": 0.00017763435124683292,
      "loss": 0.9679,
      "step": 4198
    },
    {
      "epoch": 0.33592,
      "grad_norm": 0.4158327579498291,
      "learning_rate": 0.00017762901720229366,
      "loss": 0.6653,
      "step": 4199
    },
    {
      "epoch": 0.336,
      "grad_norm": 0.4159713387489319,
      "learning_rate": 0.00017762368315775437,
      "loss": 0.9844,
      "step": 4200
    },
    {
      "epoch": 0.33608,
      "grad_norm": 0.4413051903247833,
      "learning_rate": 0.0001776183491132151,
      "loss": 0.6547,
      "step": 4201
    },
    {
      "epoch": 0.33616,
      "grad_norm": 0.33129435777664185,
      "learning_rate": 0.00017761301506867582,
      "loss": 0.8296,
      "step": 4202
    },
    {
      "epoch": 0.33624,
      "grad_norm": 0.4722890257835388,
      "learning_rate": 0.00017760768102413656,
      "loss": 0.9047,
      "step": 4203
    },
    {
      "epoch": 0.33632,
      "grad_norm": 0.5054415464401245,
      "learning_rate": 0.00017760234697959727,
      "loss": 0.9232,
      "step": 4204
    },
    {
      "epoch": 0.3364,
      "grad_norm": 0.3756479322910309,
      "learning_rate": 0.00017759701293505801,
      "loss": 0.8664,
      "step": 4205
    },
    {
      "epoch": 0.33648,
      "grad_norm": 0.3342423141002655,
      "learning_rate": 0.00017759167889051875,
      "loss": 0.6394,
      "step": 4206
    },
    {
      "epoch": 0.33656,
      "grad_norm": 0.335803359746933,
      "learning_rate": 0.00017758634484597947,
      "loss": 1.2398,
      "step": 4207
    },
    {
      "epoch": 0.33664,
      "grad_norm": 0.33630841970443726,
      "learning_rate": 0.0001775810108014402,
      "loss": 0.9297,
      "step": 4208
    },
    {
      "epoch": 0.33672,
      "grad_norm": 0.44312572479248047,
      "learning_rate": 0.00017757567675690092,
      "loss": 0.801,
      "step": 4209
    },
    {
      "epoch": 0.3368,
      "grad_norm": 0.34385648369789124,
      "learning_rate": 0.00017757034271236166,
      "loss": 0.992,
      "step": 4210
    },
    {
      "epoch": 0.33688,
      "grad_norm": 0.4094001352787018,
      "learning_rate": 0.00017756500866782237,
      "loss": 0.7789,
      "step": 4211
    },
    {
      "epoch": 0.33696,
      "grad_norm": 0.35779309272766113,
      "learning_rate": 0.0001775596746232831,
      "loss": 0.7546,
      "step": 4212
    },
    {
      "epoch": 0.33704,
      "grad_norm": 0.3817597031593323,
      "learning_rate": 0.00017755434057874385,
      "loss": 0.7877,
      "step": 4213
    },
    {
      "epoch": 0.33712,
      "grad_norm": 0.37848713994026184,
      "learning_rate": 0.00017754900653420456,
      "loss": 0.7143,
      "step": 4214
    },
    {
      "epoch": 0.3372,
      "grad_norm": 0.36384478211402893,
      "learning_rate": 0.0001775436724896653,
      "loss": 1.0089,
      "step": 4215
    },
    {
      "epoch": 0.33728,
      "grad_norm": 0.4378838539123535,
      "learning_rate": 0.00017753833844512602,
      "loss": 0.8863,
      "step": 4216
    },
    {
      "epoch": 0.33736,
      "grad_norm": 0.39240044355392456,
      "learning_rate": 0.00017753300440058676,
      "loss": 0.8844,
      "step": 4217
    },
    {
      "epoch": 0.33744,
      "grad_norm": 0.3746517598628998,
      "learning_rate": 0.00017752767035604747,
      "loss": 0.9869,
      "step": 4218
    },
    {
      "epoch": 0.33752,
      "grad_norm": 0.3586902320384979,
      "learning_rate": 0.0001775223363115082,
      "loss": 1.1404,
      "step": 4219
    },
    {
      "epoch": 0.3376,
      "grad_norm": 0.4028104543685913,
      "learning_rate": 0.00017751700226696895,
      "loss": 0.6278,
      "step": 4220
    },
    {
      "epoch": 0.33768,
      "grad_norm": 0.4812697172164917,
      "learning_rate": 0.00017751166822242966,
      "loss": 0.9814,
      "step": 4221
    },
    {
      "epoch": 0.33776,
      "grad_norm": 0.4366820156574249,
      "learning_rate": 0.0001775063341778904,
      "loss": 0.9526,
      "step": 4222
    },
    {
      "epoch": 0.33784,
      "grad_norm": 0.33644798398017883,
      "learning_rate": 0.00017750100013335111,
      "loss": 0.9618,
      "step": 4223
    },
    {
      "epoch": 0.33792,
      "grad_norm": 0.3011030852794647,
      "learning_rate": 0.00017749566608881185,
      "loss": 0.8921,
      "step": 4224
    },
    {
      "epoch": 0.338,
      "grad_norm": 0.5116255879402161,
      "learning_rate": 0.0001774903320442726,
      "loss": 0.953,
      "step": 4225
    },
    {
      "epoch": 0.33808,
      "grad_norm": 0.53929203748703,
      "learning_rate": 0.0001774849979997333,
      "loss": 0.6152,
      "step": 4226
    },
    {
      "epoch": 0.33816,
      "grad_norm": 0.3979659080505371,
      "learning_rate": 0.00017747966395519405,
      "loss": 0.739,
      "step": 4227
    },
    {
      "epoch": 0.33824,
      "grad_norm": 0.2539553642272949,
      "learning_rate": 0.00017747432991065476,
      "loss": 0.4752,
      "step": 4228
    },
    {
      "epoch": 0.33832,
      "grad_norm": 0.3475002646446228,
      "learning_rate": 0.0001774689958661155,
      "loss": 0.6509,
      "step": 4229
    },
    {
      "epoch": 0.3384,
      "grad_norm": 0.33037859201431274,
      "learning_rate": 0.0001774636618215762,
      "loss": 0.8244,
      "step": 4230
    },
    {
      "epoch": 0.33848,
      "grad_norm": 0.42147037386894226,
      "learning_rate": 0.00017745832777703695,
      "loss": 0.6766,
      "step": 4231
    },
    {
      "epoch": 0.33856,
      "grad_norm": 0.3605092465877533,
      "learning_rate": 0.0001774529937324977,
      "loss": 0.7221,
      "step": 4232
    },
    {
      "epoch": 0.33864,
      "grad_norm": 0.47716134786605835,
      "learning_rate": 0.0001774476596879584,
      "loss": 0.974,
      "step": 4233
    },
    {
      "epoch": 0.33872,
      "grad_norm": 0.31075674295425415,
      "learning_rate": 0.00017744232564341914,
      "loss": 0.6708,
      "step": 4234
    },
    {
      "epoch": 0.3388,
      "grad_norm": 0.5832539200782776,
      "learning_rate": 0.00017743699159887986,
      "loss": 1.0526,
      "step": 4235
    },
    {
      "epoch": 0.33888,
      "grad_norm": 0.3705729842185974,
      "learning_rate": 0.0001774316575543406,
      "loss": 0.5793,
      "step": 4236
    },
    {
      "epoch": 0.33896,
      "grad_norm": 0.31857070326805115,
      "learning_rate": 0.0001774263235098013,
      "loss": 1.0201,
      "step": 4237
    },
    {
      "epoch": 0.33904,
      "grad_norm": 0.4664507806301117,
      "learning_rate": 0.00017742098946526205,
      "loss": 0.815,
      "step": 4238
    },
    {
      "epoch": 0.33912,
      "grad_norm": 0.43070220947265625,
      "learning_rate": 0.0001774156554207228,
      "loss": 1.0194,
      "step": 4239
    },
    {
      "epoch": 0.3392,
      "grad_norm": 0.46681976318359375,
      "learning_rate": 0.0001774103213761835,
      "loss": 0.7857,
      "step": 4240
    },
    {
      "epoch": 0.33928,
      "grad_norm": 0.44038552045822144,
      "learning_rate": 0.00017740498733164424,
      "loss": 0.7753,
      "step": 4241
    },
    {
      "epoch": 0.33936,
      "grad_norm": 0.3850717544555664,
      "learning_rate": 0.00017739965328710495,
      "loss": 0.7571,
      "step": 4242
    },
    {
      "epoch": 0.33944,
      "grad_norm": 0.33773428201675415,
      "learning_rate": 0.0001773943192425657,
      "loss": 0.8784,
      "step": 4243
    },
    {
      "epoch": 0.33952,
      "grad_norm": 0.37762778997421265,
      "learning_rate": 0.0001773889851980264,
      "loss": 0.6825,
      "step": 4244
    },
    {
      "epoch": 0.3396,
      "grad_norm": 0.3856884837150574,
      "learning_rate": 0.00017738365115348715,
      "loss": 0.834,
      "step": 4245
    },
    {
      "epoch": 0.33968,
      "grad_norm": 0.45170506834983826,
      "learning_rate": 0.0001773783171089479,
      "loss": 1.2132,
      "step": 4246
    },
    {
      "epoch": 0.33976,
      "grad_norm": 0.4324685037136078,
      "learning_rate": 0.0001773729830644086,
      "loss": 0.7675,
      "step": 4247
    },
    {
      "epoch": 0.33984,
      "grad_norm": 0.3859665095806122,
      "learning_rate": 0.00017736764901986934,
      "loss": 0.8188,
      "step": 4248
    },
    {
      "epoch": 0.33992,
      "grad_norm": 0.30573341250419617,
      "learning_rate": 0.00017736231497533005,
      "loss": 0.6223,
      "step": 4249
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.48803409934043884,
      "learning_rate": 0.0001773569809307908,
      "loss": 0.955,
      "step": 4250
    },
    {
      "epoch": 0.34008,
      "grad_norm": 0.4086083471775055,
      "learning_rate": 0.0001773516468862515,
      "loss": 0.5813,
      "step": 4251
    },
    {
      "epoch": 0.34016,
      "grad_norm": 0.3572084307670593,
      "learning_rate": 0.00017734631284171224,
      "loss": 1.2122,
      "step": 4252
    },
    {
      "epoch": 0.34024,
      "grad_norm": 0.40580976009368896,
      "learning_rate": 0.00017734097879717298,
      "loss": 0.9461,
      "step": 4253
    },
    {
      "epoch": 0.34032,
      "grad_norm": 0.4000674784183502,
      "learning_rate": 0.0001773356447526337,
      "loss": 1.0473,
      "step": 4254
    },
    {
      "epoch": 0.3404,
      "grad_norm": 0.3686314821243286,
      "learning_rate": 0.00017733031070809444,
      "loss": 0.7287,
      "step": 4255
    },
    {
      "epoch": 0.34048,
      "grad_norm": 0.4274081289768219,
      "learning_rate": 0.00017732497666355515,
      "loss": 0.8301,
      "step": 4256
    },
    {
      "epoch": 0.34056,
      "grad_norm": 0.35492223501205444,
      "learning_rate": 0.0001773196426190159,
      "loss": 0.7352,
      "step": 4257
    },
    {
      "epoch": 0.34064,
      "grad_norm": 0.3399125039577484,
      "learning_rate": 0.0001773143085744766,
      "loss": 0.5878,
      "step": 4258
    },
    {
      "epoch": 0.34072,
      "grad_norm": 0.4307815134525299,
      "learning_rate": 0.00017730897452993734,
      "loss": 0.678,
      "step": 4259
    },
    {
      "epoch": 0.3408,
      "grad_norm": 0.33068740367889404,
      "learning_rate": 0.00017730364048539808,
      "loss": 0.6709,
      "step": 4260
    },
    {
      "epoch": 0.34088,
      "grad_norm": 0.3737551271915436,
      "learning_rate": 0.0001772983064408588,
      "loss": 0.6074,
      "step": 4261
    },
    {
      "epoch": 0.34096,
      "grad_norm": 0.3309435248374939,
      "learning_rate": 0.00017729297239631953,
      "loss": 0.6249,
      "step": 4262
    },
    {
      "epoch": 0.34104,
      "grad_norm": 0.3601144850254059,
      "learning_rate": 0.00017728763835178025,
      "loss": 0.651,
      "step": 4263
    },
    {
      "epoch": 0.34112,
      "grad_norm": 0.4177323281764984,
      "learning_rate": 0.000177282304307241,
      "loss": 0.6961,
      "step": 4264
    },
    {
      "epoch": 0.3412,
      "grad_norm": 0.38663604855537415,
      "learning_rate": 0.0001772769702627017,
      "loss": 1.1341,
      "step": 4265
    },
    {
      "epoch": 0.34128,
      "grad_norm": 0.5017352104187012,
      "learning_rate": 0.00017727163621816244,
      "loss": 0.7123,
      "step": 4266
    },
    {
      "epoch": 0.34136,
      "grad_norm": 0.3521566390991211,
      "learning_rate": 0.00017726630217362318,
      "loss": 0.7378,
      "step": 4267
    },
    {
      "epoch": 0.34144,
      "grad_norm": 0.3952394127845764,
      "learning_rate": 0.0001772609681290839,
      "loss": 0.6015,
      "step": 4268
    },
    {
      "epoch": 0.34152,
      "grad_norm": 0.36439937353134155,
      "learning_rate": 0.00017725563408454463,
      "loss": 0.9121,
      "step": 4269
    },
    {
      "epoch": 0.3416,
      "grad_norm": 0.4840688705444336,
      "learning_rate": 0.00017725030004000535,
      "loss": 1.1483,
      "step": 4270
    },
    {
      "epoch": 0.34168,
      "grad_norm": 0.2657332122325897,
      "learning_rate": 0.00017724496599546608,
      "loss": 1.0341,
      "step": 4271
    },
    {
      "epoch": 0.34176,
      "grad_norm": 0.36523717641830444,
      "learning_rate": 0.0001772396319509268,
      "loss": 0.7573,
      "step": 4272
    },
    {
      "epoch": 0.34184,
      "grad_norm": 0.34460151195526123,
      "learning_rate": 0.00017723429790638754,
      "loss": 0.6812,
      "step": 4273
    },
    {
      "epoch": 0.34192,
      "grad_norm": 0.43651843070983887,
      "learning_rate": 0.00017722896386184825,
      "loss": 0.9002,
      "step": 4274
    },
    {
      "epoch": 0.342,
      "grad_norm": 0.32300952076911926,
      "learning_rate": 0.000177223629817309,
      "loss": 0.6416,
      "step": 4275
    },
    {
      "epoch": 0.34208,
      "grad_norm": 0.40544846653938293,
      "learning_rate": 0.0001772182957727697,
      "loss": 1.1543,
      "step": 4276
    },
    {
      "epoch": 0.34216,
      "grad_norm": 0.35089564323425293,
      "learning_rate": 0.00017721296172823044,
      "loss": 0.8762,
      "step": 4277
    },
    {
      "epoch": 0.34224,
      "grad_norm": 0.33734720945358276,
      "learning_rate": 0.00017720762768369116,
      "loss": 0.6416,
      "step": 4278
    },
    {
      "epoch": 0.34232,
      "grad_norm": 0.3194301426410675,
      "learning_rate": 0.0001772022936391519,
      "loss": 0.8216,
      "step": 4279
    },
    {
      "epoch": 0.3424,
      "grad_norm": 0.40764015913009644,
      "learning_rate": 0.0001771969595946126,
      "loss": 0.8229,
      "step": 4280
    },
    {
      "epoch": 0.34248,
      "grad_norm": 0.35631367564201355,
      "learning_rate": 0.00017719162555007335,
      "loss": 0.9785,
      "step": 4281
    },
    {
      "epoch": 0.34256,
      "grad_norm": 0.43358632922172546,
      "learning_rate": 0.0001771862915055341,
      "loss": 0.8177,
      "step": 4282
    },
    {
      "epoch": 0.34264,
      "grad_norm": 0.38356897234916687,
      "learning_rate": 0.0001771809574609948,
      "loss": 0.9286,
      "step": 4283
    },
    {
      "epoch": 0.34272,
      "grad_norm": 0.3545122444629669,
      "learning_rate": 0.00017717562341645554,
      "loss": 1.134,
      "step": 4284
    },
    {
      "epoch": 0.3428,
      "grad_norm": 0.39924705028533936,
      "learning_rate": 0.00017717028937191625,
      "loss": 0.6581,
      "step": 4285
    },
    {
      "epoch": 0.34288,
      "grad_norm": 0.356466144323349,
      "learning_rate": 0.000177164955327377,
      "loss": 0.5872,
      "step": 4286
    },
    {
      "epoch": 0.34296,
      "grad_norm": 0.36707109212875366,
      "learning_rate": 0.0001771596212828377,
      "loss": 0.7697,
      "step": 4287
    },
    {
      "epoch": 0.34304,
      "grad_norm": 0.32668161392211914,
      "learning_rate": 0.00017715428723829845,
      "loss": 0.6778,
      "step": 4288
    },
    {
      "epoch": 0.34312,
      "grad_norm": 0.34538891911506653,
      "learning_rate": 0.00017714895319375916,
      "loss": 0.8394,
      "step": 4289
    },
    {
      "epoch": 0.3432,
      "grad_norm": 0.35209736227989197,
      "learning_rate": 0.0001771436191492199,
      "loss": 0.7081,
      "step": 4290
    },
    {
      "epoch": 0.34328,
      "grad_norm": 0.36294034123420715,
      "learning_rate": 0.0001771382851046806,
      "loss": 1.054,
      "step": 4291
    },
    {
      "epoch": 0.34336,
      "grad_norm": 0.3702804744243622,
      "learning_rate": 0.00017713295106014135,
      "loss": 0.9818,
      "step": 4292
    },
    {
      "epoch": 0.34344,
      "grad_norm": 0.26692476868629456,
      "learning_rate": 0.0001771276170156021,
      "loss": 0.485,
      "step": 4293
    },
    {
      "epoch": 0.34352,
      "grad_norm": 0.43547818064689636,
      "learning_rate": 0.0001771222829710628,
      "loss": 0.6627,
      "step": 4294
    },
    {
      "epoch": 0.3436,
      "grad_norm": 0.44090500473976135,
      "learning_rate": 0.00017711694892652354,
      "loss": 0.6685,
      "step": 4295
    },
    {
      "epoch": 0.34368,
      "grad_norm": 0.41998887062072754,
      "learning_rate": 0.00017711161488198426,
      "loss": 0.7724,
      "step": 4296
    },
    {
      "epoch": 0.34376,
      "grad_norm": 0.30143821239471436,
      "learning_rate": 0.000177106280837445,
      "loss": 0.738,
      "step": 4297
    },
    {
      "epoch": 0.34384,
      "grad_norm": 0.4241781532764435,
      "learning_rate": 0.0001771009467929057,
      "loss": 0.6429,
      "step": 4298
    },
    {
      "epoch": 0.34392,
      "grad_norm": 0.33858028054237366,
      "learning_rate": 0.00017709561274836645,
      "loss": 0.9401,
      "step": 4299
    },
    {
      "epoch": 0.344,
      "grad_norm": 0.25853726267814636,
      "learning_rate": 0.0001770902787038272,
      "loss": 0.529,
      "step": 4300
    },
    {
      "epoch": 0.34408,
      "grad_norm": 0.3868972361087799,
      "learning_rate": 0.0001770849446592879,
      "loss": 0.7895,
      "step": 4301
    },
    {
      "epoch": 0.34416,
      "grad_norm": 0.495292603969574,
      "learning_rate": 0.00017707961061474864,
      "loss": 0.9556,
      "step": 4302
    },
    {
      "epoch": 0.34424,
      "grad_norm": 0.3834618031978607,
      "learning_rate": 0.00017707427657020935,
      "loss": 1.0578,
      "step": 4303
    },
    {
      "epoch": 0.34432,
      "grad_norm": 0.43355798721313477,
      "learning_rate": 0.0001770689425256701,
      "loss": 0.8016,
      "step": 4304
    },
    {
      "epoch": 0.3444,
      "grad_norm": 0.4077915549278259,
      "learning_rate": 0.0001770636084811308,
      "loss": 0.638,
      "step": 4305
    },
    {
      "epoch": 0.34448,
      "grad_norm": 0.5114704370498657,
      "learning_rate": 0.00017705827443659155,
      "loss": 1.0289,
      "step": 4306
    },
    {
      "epoch": 0.34456,
      "grad_norm": 0.4375457465648651,
      "learning_rate": 0.00017705294039205229,
      "loss": 0.9895,
      "step": 4307
    },
    {
      "epoch": 0.34464,
      "grad_norm": 0.4560468792915344,
      "learning_rate": 0.000177047606347513,
      "loss": 1.0998,
      "step": 4308
    },
    {
      "epoch": 0.34472,
      "grad_norm": 0.5326722264289856,
      "learning_rate": 0.00017704227230297374,
      "loss": 0.8222,
      "step": 4309
    },
    {
      "epoch": 0.3448,
      "grad_norm": 0.5274457335472107,
      "learning_rate": 0.00017703693825843445,
      "loss": 0.9276,
      "step": 4310
    },
    {
      "epoch": 0.34488,
      "grad_norm": 0.34186673164367676,
      "learning_rate": 0.0001770316042138952,
      "loss": 1.1104,
      "step": 4311
    },
    {
      "epoch": 0.34496,
      "grad_norm": 0.3510762155056,
      "learning_rate": 0.0001770262701693559,
      "loss": 0.5084,
      "step": 4312
    },
    {
      "epoch": 0.34504,
      "grad_norm": 0.45965686440467834,
      "learning_rate": 0.00017702093612481664,
      "loss": 0.708,
      "step": 4313
    },
    {
      "epoch": 0.34512,
      "grad_norm": 0.2785089910030365,
      "learning_rate": 0.00017701560208027738,
      "loss": 0.5646,
      "step": 4314
    },
    {
      "epoch": 0.3452,
      "grad_norm": 0.34261763095855713,
      "learning_rate": 0.0001770102680357381,
      "loss": 1.0725,
      "step": 4315
    },
    {
      "epoch": 0.34528,
      "grad_norm": 0.3422306776046753,
      "learning_rate": 0.00017700493399119884,
      "loss": 0.8418,
      "step": 4316
    },
    {
      "epoch": 0.34536,
      "grad_norm": 0.589076042175293,
      "learning_rate": 0.00017699959994665955,
      "loss": 0.6807,
      "step": 4317
    },
    {
      "epoch": 0.34544,
      "grad_norm": 0.4101366698741913,
      "learning_rate": 0.0001769942659021203,
      "loss": 0.8569,
      "step": 4318
    },
    {
      "epoch": 0.34552,
      "grad_norm": 0.3442586660385132,
      "learning_rate": 0.000176988931857581,
      "loss": 1.0082,
      "step": 4319
    },
    {
      "epoch": 0.3456,
      "grad_norm": 0.3907824456691742,
      "learning_rate": 0.00017698359781304174,
      "loss": 1.2211,
      "step": 4320
    },
    {
      "epoch": 0.34568,
      "grad_norm": 0.35462692379951477,
      "learning_rate": 0.00017697826376850248,
      "loss": 0.67,
      "step": 4321
    },
    {
      "epoch": 0.34576,
      "grad_norm": 0.4484778046607971,
      "learning_rate": 0.0001769729297239632,
      "loss": 1.0937,
      "step": 4322
    },
    {
      "epoch": 0.34584,
      "grad_norm": 0.39552274346351624,
      "learning_rate": 0.00017696759567942393,
      "loss": 1.0071,
      "step": 4323
    },
    {
      "epoch": 0.34592,
      "grad_norm": 0.41473472118377686,
      "learning_rate": 0.00017696226163488465,
      "loss": 0.8538,
      "step": 4324
    },
    {
      "epoch": 0.346,
      "grad_norm": 0.29520776867866516,
      "learning_rate": 0.00017695692759034539,
      "loss": 1.0428,
      "step": 4325
    },
    {
      "epoch": 0.34608,
      "grad_norm": 0.44062772393226624,
      "learning_rate": 0.00017695159354580613,
      "loss": 0.8129,
      "step": 4326
    },
    {
      "epoch": 0.34616,
      "grad_norm": 0.3702448010444641,
      "learning_rate": 0.00017694625950126684,
      "loss": 0.7801,
      "step": 4327
    },
    {
      "epoch": 0.34624,
      "grad_norm": 0.289223313331604,
      "learning_rate": 0.00017694092545672758,
      "loss": 0.3819,
      "step": 4328
    },
    {
      "epoch": 0.34632,
      "grad_norm": 0.527857780456543,
      "learning_rate": 0.0001769355914121883,
      "loss": 0.8326,
      "step": 4329
    },
    {
      "epoch": 0.3464,
      "grad_norm": 0.27087992429733276,
      "learning_rate": 0.00017693025736764903,
      "loss": 0.9261,
      "step": 4330
    },
    {
      "epoch": 0.34648,
      "grad_norm": 0.36185526847839355,
      "learning_rate": 0.00017692492332310974,
      "loss": 0.665,
      "step": 4331
    },
    {
      "epoch": 0.34656,
      "grad_norm": 0.32580769062042236,
      "learning_rate": 0.00017691958927857048,
      "loss": 0.4989,
      "step": 4332
    },
    {
      "epoch": 0.34664,
      "grad_norm": 0.2857198119163513,
      "learning_rate": 0.00017691425523403122,
      "loss": 1.0505,
      "step": 4333
    },
    {
      "epoch": 0.34672,
      "grad_norm": 0.35238024592399597,
      "learning_rate": 0.00017690892118949194,
      "loss": 0.5569,
      "step": 4334
    },
    {
      "epoch": 0.3468,
      "grad_norm": 0.5242781043052673,
      "learning_rate": 0.00017690358714495268,
      "loss": 0.7715,
      "step": 4335
    },
    {
      "epoch": 0.34688,
      "grad_norm": 0.2909647226333618,
      "learning_rate": 0.0001768982531004134,
      "loss": 0.6651,
      "step": 4336
    },
    {
      "epoch": 0.34696,
      "grad_norm": 0.3653591573238373,
      "learning_rate": 0.00017689291905587413,
      "loss": 0.7996,
      "step": 4337
    },
    {
      "epoch": 0.34704,
      "grad_norm": 0.38953283429145813,
      "learning_rate": 0.00017688758501133484,
      "loss": 0.7994,
      "step": 4338
    },
    {
      "epoch": 0.34712,
      "grad_norm": 0.3068741261959076,
      "learning_rate": 0.00017688225096679558,
      "loss": 0.553,
      "step": 4339
    },
    {
      "epoch": 0.3472,
      "grad_norm": 0.44244441390037537,
      "learning_rate": 0.00017687691692225632,
      "loss": 0.7528,
      "step": 4340
    },
    {
      "epoch": 0.34728,
      "grad_norm": 0.3708057999610901,
      "learning_rate": 0.00017687158287771703,
      "loss": 0.6738,
      "step": 4341
    },
    {
      "epoch": 0.34736,
      "grad_norm": 0.4149874150753021,
      "learning_rate": 0.00017686624883317777,
      "loss": 1.014,
      "step": 4342
    },
    {
      "epoch": 0.34744,
      "grad_norm": 0.3046411871910095,
      "learning_rate": 0.0001768609147886385,
      "loss": 1.037,
      "step": 4343
    },
    {
      "epoch": 0.34752,
      "grad_norm": 0.33797043561935425,
      "learning_rate": 0.00017685558074409923,
      "loss": 0.6481,
      "step": 4344
    },
    {
      "epoch": 0.3476,
      "grad_norm": 0.36314523220062256,
      "learning_rate": 0.00017685024669955994,
      "loss": 0.89,
      "step": 4345
    },
    {
      "epoch": 0.34768,
      "grad_norm": 0.3373867869377136,
      "learning_rate": 0.00017684491265502068,
      "loss": 0.9153,
      "step": 4346
    },
    {
      "epoch": 0.34776,
      "grad_norm": 0.44523224234580994,
      "learning_rate": 0.00017683957861048142,
      "loss": 0.9759,
      "step": 4347
    },
    {
      "epoch": 0.34784,
      "grad_norm": 0.3997300863265991,
      "learning_rate": 0.00017683424456594213,
      "loss": 0.774,
      "step": 4348
    },
    {
      "epoch": 0.34792,
      "grad_norm": 0.3357974588871002,
      "learning_rate": 0.00017682891052140287,
      "loss": 0.6014,
      "step": 4349
    },
    {
      "epoch": 0.348,
      "grad_norm": 0.3459237515926361,
      "learning_rate": 0.00017682357647686358,
      "loss": 0.966,
      "step": 4350
    },
    {
      "epoch": 0.34808,
      "grad_norm": 0.36653265357017517,
      "learning_rate": 0.00017681824243232432,
      "loss": 0.632,
      "step": 4351
    },
    {
      "epoch": 0.34816,
      "grad_norm": 0.3709828555583954,
      "learning_rate": 0.00017681290838778504,
      "loss": 0.912,
      "step": 4352
    },
    {
      "epoch": 0.34824,
      "grad_norm": 0.4068685472011566,
      "learning_rate": 0.00017680757434324578,
      "loss": 0.8144,
      "step": 4353
    },
    {
      "epoch": 0.34832,
      "grad_norm": 0.36570656299591064,
      "learning_rate": 0.00017680224029870652,
      "loss": 0.8284,
      "step": 4354
    },
    {
      "epoch": 0.3484,
      "grad_norm": 0.36930030584335327,
      "learning_rate": 0.00017679690625416723,
      "loss": 0.7701,
      "step": 4355
    },
    {
      "epoch": 0.34848,
      "grad_norm": 0.39746055006980896,
      "learning_rate": 0.00017679157220962797,
      "loss": 0.7731,
      "step": 4356
    },
    {
      "epoch": 0.34856,
      "grad_norm": 0.3848990797996521,
      "learning_rate": 0.00017678623816508868,
      "loss": 1.1056,
      "step": 4357
    },
    {
      "epoch": 0.34864,
      "grad_norm": 0.4110391139984131,
      "learning_rate": 0.00017678090412054942,
      "loss": 1.0027,
      "step": 4358
    },
    {
      "epoch": 0.34872,
      "grad_norm": 0.41717901825904846,
      "learning_rate": 0.00017677557007601013,
      "loss": 0.8778,
      "step": 4359
    },
    {
      "epoch": 0.3488,
      "grad_norm": 0.4100070297718048,
      "learning_rate": 0.00017677023603147087,
      "loss": 0.8011,
      "step": 4360
    },
    {
      "epoch": 0.34888,
      "grad_norm": 0.39260390400886536,
      "learning_rate": 0.00017676490198693161,
      "loss": 0.9237,
      "step": 4361
    },
    {
      "epoch": 0.34896,
      "grad_norm": 0.3747625946998596,
      "learning_rate": 0.00017675956794239233,
      "loss": 0.5641,
      "step": 4362
    },
    {
      "epoch": 0.34904,
      "grad_norm": 0.36630964279174805,
      "learning_rate": 0.00017675423389785307,
      "loss": 1.1407,
      "step": 4363
    },
    {
      "epoch": 0.34912,
      "grad_norm": 0.4429217278957367,
      "learning_rate": 0.00017674889985331378,
      "loss": 0.9802,
      "step": 4364
    },
    {
      "epoch": 0.3492,
      "grad_norm": 0.4969477355480194,
      "learning_rate": 0.00017674356580877452,
      "loss": 0.9983,
      "step": 4365
    },
    {
      "epoch": 0.34928,
      "grad_norm": 0.3716858923435211,
      "learning_rate": 0.00017673823176423523,
      "loss": 0.69,
      "step": 4366
    },
    {
      "epoch": 0.34936,
      "grad_norm": 0.3090839982032776,
      "learning_rate": 0.00017673289771969597,
      "loss": 1.1059,
      "step": 4367
    },
    {
      "epoch": 0.34944,
      "grad_norm": 0.3351787328720093,
      "learning_rate": 0.0001767275636751567,
      "loss": 0.6268,
      "step": 4368
    },
    {
      "epoch": 0.34952,
      "grad_norm": 0.3017209470272064,
      "learning_rate": 0.00017672222963061742,
      "loss": 0.8111,
      "step": 4369
    },
    {
      "epoch": 0.3496,
      "grad_norm": 0.3151201605796814,
      "learning_rate": 0.00017671689558607816,
      "loss": 0.7244,
      "step": 4370
    },
    {
      "epoch": 0.34968,
      "grad_norm": 0.2982381284236908,
      "learning_rate": 0.00017671156154153888,
      "loss": 0.4287,
      "step": 4371
    },
    {
      "epoch": 0.34976,
      "grad_norm": 0.48019155859947205,
      "learning_rate": 0.00017670622749699962,
      "loss": 1.0763,
      "step": 4372
    },
    {
      "epoch": 0.34984,
      "grad_norm": 0.4505928158760071,
      "learning_rate": 0.00017670089345246033,
      "loss": 1.0001,
      "step": 4373
    },
    {
      "epoch": 0.34992,
      "grad_norm": 0.4297032952308655,
      "learning_rate": 0.00017669555940792107,
      "loss": 0.71,
      "step": 4374
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.4849953055381775,
      "learning_rate": 0.0001766902253633818,
      "loss": 0.7157,
      "step": 4375
    },
    {
      "epoch": 0.35008,
      "grad_norm": 0.3710271418094635,
      "learning_rate": 0.00017668489131884252,
      "loss": 0.7523,
      "step": 4376
    },
    {
      "epoch": 0.35016,
      "grad_norm": 0.4482927620410919,
      "learning_rate": 0.00017667955727430326,
      "loss": 1.1676,
      "step": 4377
    },
    {
      "epoch": 0.35024,
      "grad_norm": 0.42612239718437195,
      "learning_rate": 0.00017667422322976397,
      "loss": 0.7317,
      "step": 4378
    },
    {
      "epoch": 0.35032,
      "grad_norm": 0.33618664741516113,
      "learning_rate": 0.00017666888918522471,
      "loss": 1.1302,
      "step": 4379
    },
    {
      "epoch": 0.3504,
      "grad_norm": 0.4325759708881378,
      "learning_rate": 0.00017666355514068545,
      "loss": 0.7337,
      "step": 4380
    },
    {
      "epoch": 0.35048,
      "grad_norm": 0.4323466420173645,
      "learning_rate": 0.00017665822109614617,
      "loss": 0.8098,
      "step": 4381
    },
    {
      "epoch": 0.35056,
      "grad_norm": 0.41045334935188293,
      "learning_rate": 0.0001766528870516069,
      "loss": 0.8949,
      "step": 4382
    },
    {
      "epoch": 0.35064,
      "grad_norm": 0.4545460343360901,
      "learning_rate": 0.00017664755300706762,
      "loss": 0.8856,
      "step": 4383
    },
    {
      "epoch": 0.35072,
      "grad_norm": 0.4188205301761627,
      "learning_rate": 0.00017664221896252836,
      "loss": 0.5349,
      "step": 4384
    },
    {
      "epoch": 0.3508,
      "grad_norm": 0.347398966550827,
      "learning_rate": 0.00017663688491798907,
      "loss": 0.5703,
      "step": 4385
    },
    {
      "epoch": 0.35088,
      "grad_norm": 0.45080289244651794,
      "learning_rate": 0.0001766315508734498,
      "loss": 0.8771,
      "step": 4386
    },
    {
      "epoch": 0.35096,
      "grad_norm": 0.3325425684452057,
      "learning_rate": 0.00017662621682891055,
      "loss": 0.9628,
      "step": 4387
    },
    {
      "epoch": 0.35104,
      "grad_norm": 0.33072054386138916,
      "learning_rate": 0.00017662088278437126,
      "loss": 0.9062,
      "step": 4388
    },
    {
      "epoch": 0.35112,
      "grad_norm": 0.3162713646888733,
      "learning_rate": 0.000176615548739832,
      "loss": 0.9365,
      "step": 4389
    },
    {
      "epoch": 0.3512,
      "grad_norm": 0.3636181950569153,
      "learning_rate": 0.00017661021469529272,
      "loss": 0.8407,
      "step": 4390
    },
    {
      "epoch": 0.35128,
      "grad_norm": 0.40483561158180237,
      "learning_rate": 0.00017660488065075346,
      "loss": 0.6981,
      "step": 4391
    },
    {
      "epoch": 0.35136,
      "grad_norm": 0.3923795521259308,
      "learning_rate": 0.00017659954660621417,
      "loss": 0.886,
      "step": 4392
    },
    {
      "epoch": 0.35144,
      "grad_norm": 0.3272838592529297,
      "learning_rate": 0.0001765942125616749,
      "loss": 0.6758,
      "step": 4393
    },
    {
      "epoch": 0.35152,
      "grad_norm": 0.3928528428077698,
      "learning_rate": 0.00017658887851713565,
      "loss": 0.7646,
      "step": 4394
    },
    {
      "epoch": 0.3516,
      "grad_norm": 0.5142637491226196,
      "learning_rate": 0.00017658354447259636,
      "loss": 0.9733,
      "step": 4395
    },
    {
      "epoch": 0.35168,
      "grad_norm": 0.5166188478469849,
      "learning_rate": 0.0001765782104280571,
      "loss": 1.1873,
      "step": 4396
    },
    {
      "epoch": 0.35176,
      "grad_norm": 0.4130379557609558,
      "learning_rate": 0.00017657287638351782,
      "loss": 0.4536,
      "step": 4397
    },
    {
      "epoch": 0.35184,
      "grad_norm": 0.47361260652542114,
      "learning_rate": 0.00017656754233897855,
      "loss": 1.0281,
      "step": 4398
    },
    {
      "epoch": 0.35192,
      "grad_norm": 0.3355066776275635,
      "learning_rate": 0.00017656220829443927,
      "loss": 0.9538,
      "step": 4399
    },
    {
      "epoch": 0.352,
      "grad_norm": 0.38799363374710083,
      "learning_rate": 0.0001765568742499,
      "loss": 0.5685,
      "step": 4400
    },
    {
      "epoch": 0.35208,
      "grad_norm": 0.4465647339820862,
      "learning_rate": 0.00017655154020536072,
      "loss": 0.9735,
      "step": 4401
    },
    {
      "epoch": 0.35216,
      "grad_norm": 0.28464147448539734,
      "learning_rate": 0.00017654620616082146,
      "loss": 1.0333,
      "step": 4402
    },
    {
      "epoch": 0.35224,
      "grad_norm": 0.2765219211578369,
      "learning_rate": 0.00017654087211628217,
      "loss": 0.5176,
      "step": 4403
    },
    {
      "epoch": 0.35232,
      "grad_norm": 0.51637864112854,
      "learning_rate": 0.0001765355380717429,
      "loss": 1.0217,
      "step": 4404
    },
    {
      "epoch": 0.3524,
      "grad_norm": 0.48997706174850464,
      "learning_rate": 0.00017653020402720363,
      "loss": 0.9075,
      "step": 4405
    },
    {
      "epoch": 0.35248,
      "grad_norm": 0.3949063718318939,
      "learning_rate": 0.00017652486998266437,
      "loss": 0.7342,
      "step": 4406
    },
    {
      "epoch": 0.35256,
      "grad_norm": 0.4360136091709137,
      "learning_rate": 0.00017651953593812508,
      "loss": 0.6842,
      "step": 4407
    },
    {
      "epoch": 0.35264,
      "grad_norm": 0.37777894735336304,
      "learning_rate": 0.00017651420189358582,
      "loss": 0.9097,
      "step": 4408
    },
    {
      "epoch": 0.35272,
      "grad_norm": 0.38304755091667175,
      "learning_rate": 0.00017650886784904653,
      "loss": 0.7086,
      "step": 4409
    },
    {
      "epoch": 0.3528,
      "grad_norm": 0.3736095428466797,
      "learning_rate": 0.00017650353380450727,
      "loss": 0.7316,
      "step": 4410
    },
    {
      "epoch": 0.35288,
      "grad_norm": 0.37179747223854065,
      "learning_rate": 0.000176498199759968,
      "loss": 1.0,
      "step": 4411
    },
    {
      "epoch": 0.35296,
      "grad_norm": 0.35499653220176697,
      "learning_rate": 0.00017649286571542872,
      "loss": 0.9809,
      "step": 4412
    },
    {
      "epoch": 0.35304,
      "grad_norm": 0.4646630585193634,
      "learning_rate": 0.00017648753167088946,
      "loss": 1.0176,
      "step": 4413
    },
    {
      "epoch": 0.35312,
      "grad_norm": 0.40809282660484314,
      "learning_rate": 0.00017648219762635018,
      "loss": 0.6848,
      "step": 4414
    },
    {
      "epoch": 0.3532,
      "grad_norm": 0.5706667900085449,
      "learning_rate": 0.00017647686358181092,
      "loss": 0.9183,
      "step": 4415
    },
    {
      "epoch": 0.35328,
      "grad_norm": 0.46353891491889954,
      "learning_rate": 0.00017647152953727163,
      "loss": 1.1293,
      "step": 4416
    },
    {
      "epoch": 0.35336,
      "grad_norm": 0.48159754276275635,
      "learning_rate": 0.00017646619549273237,
      "loss": 0.812,
      "step": 4417
    },
    {
      "epoch": 0.35344,
      "grad_norm": 0.38142043352127075,
      "learning_rate": 0.00017646086144819308,
      "loss": 0.6564,
      "step": 4418
    },
    {
      "epoch": 0.35352,
      "grad_norm": 0.4171387553215027,
      "learning_rate": 0.00017645552740365382,
      "loss": 1.2546,
      "step": 4419
    },
    {
      "epoch": 0.3536,
      "grad_norm": 0.30655166506767273,
      "learning_rate": 0.00017645019335911453,
      "loss": 0.6317,
      "step": 4420
    },
    {
      "epoch": 0.35368,
      "grad_norm": 0.415068119764328,
      "learning_rate": 0.00017644485931457527,
      "loss": 0.8815,
      "step": 4421
    },
    {
      "epoch": 0.35376,
      "grad_norm": 0.39403870701789856,
      "learning_rate": 0.000176439525270036,
      "loss": 0.6446,
      "step": 4422
    },
    {
      "epoch": 0.35384,
      "grad_norm": 0.3338124752044678,
      "learning_rate": 0.00017643419122549673,
      "loss": 0.5203,
      "step": 4423
    },
    {
      "epoch": 0.35392,
      "grad_norm": 0.39011919498443604,
      "learning_rate": 0.00017642885718095747,
      "loss": 0.6892,
      "step": 4424
    },
    {
      "epoch": 0.354,
      "grad_norm": 0.43525072932243347,
      "learning_rate": 0.00017642352313641818,
      "loss": 0.8564,
      "step": 4425
    },
    {
      "epoch": 0.35408,
      "grad_norm": 0.3363255560398102,
      "learning_rate": 0.00017641818909187892,
      "loss": 0.6448,
      "step": 4426
    },
    {
      "epoch": 0.35416,
      "grad_norm": 0.28266406059265137,
      "learning_rate": 0.00017641285504733966,
      "loss": 0.4784,
      "step": 4427
    },
    {
      "epoch": 0.35424,
      "grad_norm": 0.35776495933532715,
      "learning_rate": 0.00017640752100280037,
      "loss": 0.9674,
      "step": 4428
    },
    {
      "epoch": 0.35432,
      "grad_norm": 0.3804342746734619,
      "learning_rate": 0.0001764021869582611,
      "loss": 0.8154,
      "step": 4429
    },
    {
      "epoch": 0.3544,
      "grad_norm": 0.3742803931236267,
      "learning_rate": 0.00017639685291372182,
      "loss": 0.5969,
      "step": 4430
    },
    {
      "epoch": 0.35448,
      "grad_norm": 0.4417801797389984,
      "learning_rate": 0.00017639151886918256,
      "loss": 0.7223,
      "step": 4431
    },
    {
      "epoch": 0.35456,
      "grad_norm": 0.556509256362915,
      "learning_rate": 0.00017638618482464328,
      "loss": 0.9709,
      "step": 4432
    },
    {
      "epoch": 0.35464,
      "grad_norm": 0.3021562099456787,
      "learning_rate": 0.00017638085078010402,
      "loss": 0.8204,
      "step": 4433
    },
    {
      "epoch": 0.35472,
      "grad_norm": 0.41103750467300415,
      "learning_rate": 0.00017637551673556476,
      "loss": 0.5543,
      "step": 4434
    },
    {
      "epoch": 0.3548,
      "grad_norm": 0.4516286551952362,
      "learning_rate": 0.00017637018269102547,
      "loss": 0.7718,
      "step": 4435
    },
    {
      "epoch": 0.35488,
      "grad_norm": 0.33487096428871155,
      "learning_rate": 0.0001763648486464862,
      "loss": 0.8863,
      "step": 4436
    },
    {
      "epoch": 0.35496,
      "grad_norm": 0.350117564201355,
      "learning_rate": 0.00017635951460194692,
      "loss": 0.8309,
      "step": 4437
    },
    {
      "epoch": 0.35504,
      "grad_norm": 0.30587559938430786,
      "learning_rate": 0.00017635418055740766,
      "loss": 0.5327,
      "step": 4438
    },
    {
      "epoch": 0.35512,
      "grad_norm": 0.40779340267181396,
      "learning_rate": 0.00017634884651286837,
      "loss": 0.947,
      "step": 4439
    },
    {
      "epoch": 0.3552,
      "grad_norm": 0.39577722549438477,
      "learning_rate": 0.00017634351246832911,
      "loss": 0.7982,
      "step": 4440
    },
    {
      "epoch": 0.35528,
      "grad_norm": 0.26345014572143555,
      "learning_rate": 0.00017633817842378985,
      "loss": 0.9976,
      "step": 4441
    },
    {
      "epoch": 0.35536,
      "grad_norm": 0.3566162884235382,
      "learning_rate": 0.00017633284437925057,
      "loss": 0.77,
      "step": 4442
    },
    {
      "epoch": 0.35544,
      "grad_norm": 0.36230212450027466,
      "learning_rate": 0.0001763275103347113,
      "loss": 0.7677,
      "step": 4443
    },
    {
      "epoch": 0.35552,
      "grad_norm": 0.3834230601787567,
      "learning_rate": 0.00017632217629017202,
      "loss": 1.1991,
      "step": 4444
    },
    {
      "epoch": 0.3556,
      "grad_norm": 0.3169156014919281,
      "learning_rate": 0.00017631684224563276,
      "loss": 0.9357,
      "step": 4445
    },
    {
      "epoch": 0.35568,
      "grad_norm": 0.42688098549842834,
      "learning_rate": 0.00017631150820109347,
      "loss": 0.9293,
      "step": 4446
    },
    {
      "epoch": 0.35576,
      "grad_norm": 0.38321948051452637,
      "learning_rate": 0.0001763061741565542,
      "loss": 0.6729,
      "step": 4447
    },
    {
      "epoch": 0.35584,
      "grad_norm": 0.40486443042755127,
      "learning_rate": 0.00017630084011201495,
      "loss": 1.1983,
      "step": 4448
    },
    {
      "epoch": 0.35592,
      "grad_norm": 0.33085280656814575,
      "learning_rate": 0.00017629550606747566,
      "loss": 0.799,
      "step": 4449
    },
    {
      "epoch": 0.356,
      "grad_norm": 0.2894402742385864,
      "learning_rate": 0.0001762901720229364,
      "loss": 0.8518,
      "step": 4450
    },
    {
      "epoch": 0.35608,
      "grad_norm": 0.34867075085639954,
      "learning_rate": 0.00017628483797839712,
      "loss": 0.6408,
      "step": 4451
    },
    {
      "epoch": 0.35616,
      "grad_norm": 0.4287853538990021,
      "learning_rate": 0.00017627950393385786,
      "loss": 0.7338,
      "step": 4452
    },
    {
      "epoch": 0.35624,
      "grad_norm": 0.4128950238227844,
      "learning_rate": 0.00017627416988931857,
      "loss": 0.714,
      "step": 4453
    },
    {
      "epoch": 0.35632,
      "grad_norm": 0.5157495737075806,
      "learning_rate": 0.0001762688358447793,
      "loss": 0.8972,
      "step": 4454
    },
    {
      "epoch": 0.3564,
      "grad_norm": 0.41080039739608765,
      "learning_rate": 0.00017626350180024005,
      "loss": 0.7231,
      "step": 4455
    },
    {
      "epoch": 0.35648,
      "grad_norm": 0.375932514667511,
      "learning_rate": 0.00017625816775570076,
      "loss": 0.6944,
      "step": 4456
    },
    {
      "epoch": 0.35656,
      "grad_norm": 0.41722550988197327,
      "learning_rate": 0.0001762528337111615,
      "loss": 0.9171,
      "step": 4457
    },
    {
      "epoch": 0.35664,
      "grad_norm": 0.441754013299942,
      "learning_rate": 0.00017624749966662221,
      "loss": 0.8848,
      "step": 4458
    },
    {
      "epoch": 0.35672,
      "grad_norm": 0.3615110218524933,
      "learning_rate": 0.00017624216562208295,
      "loss": 0.9498,
      "step": 4459
    },
    {
      "epoch": 0.3568,
      "grad_norm": 0.38566136360168457,
      "learning_rate": 0.00017623683157754367,
      "loss": 0.6134,
      "step": 4460
    },
    {
      "epoch": 0.35688,
      "grad_norm": 0.42782893776893616,
      "learning_rate": 0.0001762314975330044,
      "loss": 0.9323,
      "step": 4461
    },
    {
      "epoch": 0.35696,
      "grad_norm": 0.3027021586894989,
      "learning_rate": 0.00017622616348846515,
      "loss": 0.4068,
      "step": 4462
    },
    {
      "epoch": 0.35704,
      "grad_norm": 0.35751280188560486,
      "learning_rate": 0.00017622082944392586,
      "loss": 0.587,
      "step": 4463
    },
    {
      "epoch": 0.35712,
      "grad_norm": 0.32809045910835266,
      "learning_rate": 0.0001762154953993866,
      "loss": 0.8104,
      "step": 4464
    },
    {
      "epoch": 0.3572,
      "grad_norm": 0.382059246301651,
      "learning_rate": 0.0001762101613548473,
      "loss": 0.6313,
      "step": 4465
    },
    {
      "epoch": 0.35728,
      "grad_norm": 0.4220024347305298,
      "learning_rate": 0.00017620482731030805,
      "loss": 1.1809,
      "step": 4466
    },
    {
      "epoch": 0.35736,
      "grad_norm": 0.35312700271606445,
      "learning_rate": 0.00017619949326576876,
      "loss": 0.7938,
      "step": 4467
    },
    {
      "epoch": 0.35744,
      "grad_norm": 0.47387784719467163,
      "learning_rate": 0.0001761941592212295,
      "loss": 0.6888,
      "step": 4468
    },
    {
      "epoch": 0.35752,
      "grad_norm": 0.32534661889076233,
      "learning_rate": 0.00017618882517669024,
      "loss": 1.0228,
      "step": 4469
    },
    {
      "epoch": 0.3576,
      "grad_norm": 0.46667492389678955,
      "learning_rate": 0.00017618349113215096,
      "loss": 1.3994,
      "step": 4470
    },
    {
      "epoch": 0.35768,
      "grad_norm": 0.5365886688232422,
      "learning_rate": 0.0001761781570876117,
      "loss": 0.7381,
      "step": 4471
    },
    {
      "epoch": 0.35776,
      "grad_norm": 0.4242114722728729,
      "learning_rate": 0.0001761728230430724,
      "loss": 0.8265,
      "step": 4472
    },
    {
      "epoch": 0.35784,
      "grad_norm": 0.3888421058654785,
      "learning_rate": 0.00017616748899853315,
      "loss": 0.4638,
      "step": 4473
    },
    {
      "epoch": 0.35792,
      "grad_norm": 0.5187479257583618,
      "learning_rate": 0.0001761621549539939,
      "loss": 0.8986,
      "step": 4474
    },
    {
      "epoch": 0.358,
      "grad_norm": 0.3237837553024292,
      "learning_rate": 0.0001761568209094546,
      "loss": 0.9386,
      "step": 4475
    },
    {
      "epoch": 0.35808,
      "grad_norm": 0.4478347599506378,
      "learning_rate": 0.00017615148686491534,
      "loss": 0.9108,
      "step": 4476
    },
    {
      "epoch": 0.35816,
      "grad_norm": 0.35185354948043823,
      "learning_rate": 0.00017614615282037605,
      "loss": 0.6605,
      "step": 4477
    },
    {
      "epoch": 0.35824,
      "grad_norm": 0.4497559070587158,
      "learning_rate": 0.0001761408187758368,
      "loss": 0.8513,
      "step": 4478
    },
    {
      "epoch": 0.35832,
      "grad_norm": 0.38316550850868225,
      "learning_rate": 0.0001761354847312975,
      "loss": 0.8324,
      "step": 4479
    },
    {
      "epoch": 0.3584,
      "grad_norm": 0.4767117202281952,
      "learning_rate": 0.00017613015068675825,
      "loss": 0.8529,
      "step": 4480
    },
    {
      "epoch": 0.35848,
      "grad_norm": 0.34312599897384644,
      "learning_rate": 0.000176124816642219,
      "loss": 0.6214,
      "step": 4481
    },
    {
      "epoch": 0.35856,
      "grad_norm": 0.3791389465332031,
      "learning_rate": 0.0001761194825976797,
      "loss": 0.8281,
      "step": 4482
    },
    {
      "epoch": 0.35864,
      "grad_norm": 0.4009937345981598,
      "learning_rate": 0.00017611414855314044,
      "loss": 0.8294,
      "step": 4483
    },
    {
      "epoch": 0.35872,
      "grad_norm": 0.37699761986732483,
      "learning_rate": 0.00017610881450860115,
      "loss": 0.5676,
      "step": 4484
    },
    {
      "epoch": 0.3588,
      "grad_norm": 0.4133804440498352,
      "learning_rate": 0.0001761034804640619,
      "loss": 0.7857,
      "step": 4485
    },
    {
      "epoch": 0.35888,
      "grad_norm": 0.418834924697876,
      "learning_rate": 0.0001760981464195226,
      "loss": 0.6278,
      "step": 4486
    },
    {
      "epoch": 0.35896,
      "grad_norm": 0.4026128351688385,
      "learning_rate": 0.00017609281237498334,
      "loss": 1.0372,
      "step": 4487
    },
    {
      "epoch": 0.35904,
      "grad_norm": 0.40743428468704224,
      "learning_rate": 0.00017608747833044408,
      "loss": 0.8313,
      "step": 4488
    },
    {
      "epoch": 0.35912,
      "grad_norm": 0.4413393437862396,
      "learning_rate": 0.0001760821442859048,
      "loss": 0.7891,
      "step": 4489
    },
    {
      "epoch": 0.3592,
      "grad_norm": 0.4271935522556305,
      "learning_rate": 0.00017607681024136554,
      "loss": 0.8087,
      "step": 4490
    },
    {
      "epoch": 0.35928,
      "grad_norm": 0.34187179803848267,
      "learning_rate": 0.00017607147619682625,
      "loss": 0.7671,
      "step": 4491
    },
    {
      "epoch": 0.35936,
      "grad_norm": 0.35333311557769775,
      "learning_rate": 0.000176066142152287,
      "loss": 0.6372,
      "step": 4492
    },
    {
      "epoch": 0.35944,
      "grad_norm": 0.36894869804382324,
      "learning_rate": 0.0001760608081077477,
      "loss": 0.9204,
      "step": 4493
    },
    {
      "epoch": 0.35952,
      "grad_norm": 0.42192792892456055,
      "learning_rate": 0.00017605547406320844,
      "loss": 0.9347,
      "step": 4494
    },
    {
      "epoch": 0.3596,
      "grad_norm": 0.32683902978897095,
      "learning_rate": 0.00017605014001866918,
      "loss": 0.6673,
      "step": 4495
    },
    {
      "epoch": 0.35968,
      "grad_norm": 0.38342931866645813,
      "learning_rate": 0.0001760448059741299,
      "loss": 1.1463,
      "step": 4496
    },
    {
      "epoch": 0.35976,
      "grad_norm": 0.3316880166530609,
      "learning_rate": 0.00017603947192959063,
      "loss": 0.6391,
      "step": 4497
    },
    {
      "epoch": 0.35984,
      "grad_norm": 0.38204050064086914,
      "learning_rate": 0.00017603413788505135,
      "loss": 0.4611,
      "step": 4498
    },
    {
      "epoch": 0.35992,
      "grad_norm": 0.3380041718482971,
      "learning_rate": 0.0001760288038405121,
      "loss": 0.5138,
      "step": 4499
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.36498504877090454,
      "learning_rate": 0.0001760234697959728,
      "loss": 0.9709,
      "step": 4500
    },
    {
      "epoch": 0.36008,
      "grad_norm": 0.29614290595054626,
      "learning_rate": 0.00017601813575143354,
      "loss": 0.8221,
      "step": 4501
    },
    {
      "epoch": 0.36016,
      "grad_norm": 0.40684759616851807,
      "learning_rate": 0.00017601280170689428,
      "loss": 0.943,
      "step": 4502
    },
    {
      "epoch": 0.36024,
      "grad_norm": 0.3333081305027008,
      "learning_rate": 0.000176007467662355,
      "loss": 0.6281,
      "step": 4503
    },
    {
      "epoch": 0.36032,
      "grad_norm": 0.41420572996139526,
      "learning_rate": 0.00017600213361781573,
      "loss": 0.5686,
      "step": 4504
    },
    {
      "epoch": 0.3604,
      "grad_norm": 0.3669990301132202,
      "learning_rate": 0.00017599679957327644,
      "loss": 0.7361,
      "step": 4505
    },
    {
      "epoch": 0.36048,
      "grad_norm": 0.5794739723205566,
      "learning_rate": 0.00017599146552873718,
      "loss": 0.8407,
      "step": 4506
    },
    {
      "epoch": 0.36056,
      "grad_norm": 0.35471901297569275,
      "learning_rate": 0.0001759861314841979,
      "loss": 0.7148,
      "step": 4507
    },
    {
      "epoch": 0.36064,
      "grad_norm": 0.4610680937767029,
      "learning_rate": 0.00017598079743965864,
      "loss": 0.7223,
      "step": 4508
    },
    {
      "epoch": 0.36072,
      "grad_norm": 0.37352561950683594,
      "learning_rate": 0.00017597546339511938,
      "loss": 0.8223,
      "step": 4509
    },
    {
      "epoch": 0.3608,
      "grad_norm": 0.43294697999954224,
      "learning_rate": 0.0001759701293505801,
      "loss": 1.007,
      "step": 4510
    },
    {
      "epoch": 0.36088,
      "grad_norm": 0.48927366733551025,
      "learning_rate": 0.00017596479530604083,
      "loss": 0.8566,
      "step": 4511
    },
    {
      "epoch": 0.36096,
      "grad_norm": 0.42432206869125366,
      "learning_rate": 0.00017595946126150154,
      "loss": 0.7383,
      "step": 4512
    },
    {
      "epoch": 0.36104,
      "grad_norm": 0.4344654977321625,
      "learning_rate": 0.00017595412721696228,
      "loss": 1.384,
      "step": 4513
    },
    {
      "epoch": 0.36112,
      "grad_norm": 0.36375248432159424,
      "learning_rate": 0.000175948793172423,
      "loss": 0.7417,
      "step": 4514
    },
    {
      "epoch": 0.3612,
      "grad_norm": 0.4260232448577881,
      "learning_rate": 0.00017594345912788373,
      "loss": 0.697,
      "step": 4515
    },
    {
      "epoch": 0.36128,
      "grad_norm": 0.4154844284057617,
      "learning_rate": 0.00017593812508334447,
      "loss": 0.6983,
      "step": 4516
    },
    {
      "epoch": 0.36136,
      "grad_norm": 0.28771939873695374,
      "learning_rate": 0.0001759327910388052,
      "loss": 1.1176,
      "step": 4517
    },
    {
      "epoch": 0.36144,
      "grad_norm": 0.40018391609191895,
      "learning_rate": 0.00017592745699426593,
      "loss": 0.5707,
      "step": 4518
    },
    {
      "epoch": 0.36152,
      "grad_norm": 0.39506253600120544,
      "learning_rate": 0.00017592212294972664,
      "loss": 0.945,
      "step": 4519
    },
    {
      "epoch": 0.3616,
      "grad_norm": 0.4488567113876343,
      "learning_rate": 0.00017591678890518738,
      "loss": 0.7643,
      "step": 4520
    },
    {
      "epoch": 0.36168,
      "grad_norm": 0.37961164116859436,
      "learning_rate": 0.0001759114548606481,
      "loss": 0.8259,
      "step": 4521
    },
    {
      "epoch": 0.36176,
      "grad_norm": 0.39799556136131287,
      "learning_rate": 0.00017590612081610883,
      "loss": 0.8612,
      "step": 4522
    },
    {
      "epoch": 0.36184,
      "grad_norm": 0.30118006467819214,
      "learning_rate": 0.00017590078677156957,
      "loss": 0.5969,
      "step": 4523
    },
    {
      "epoch": 0.36192,
      "grad_norm": 0.3828861117362976,
      "learning_rate": 0.00017589545272703029,
      "loss": 0.9541,
      "step": 4524
    },
    {
      "epoch": 0.362,
      "grad_norm": 0.3502817153930664,
      "learning_rate": 0.00017589011868249102,
      "loss": 1.0114,
      "step": 4525
    },
    {
      "epoch": 0.36208,
      "grad_norm": 0.43438127636909485,
      "learning_rate": 0.00017588478463795174,
      "loss": 0.8814,
      "step": 4526
    },
    {
      "epoch": 0.36216,
      "grad_norm": 0.4407861828804016,
      "learning_rate": 0.00017587945059341248,
      "loss": 0.7958,
      "step": 4527
    },
    {
      "epoch": 0.36224,
      "grad_norm": 0.2890775203704834,
      "learning_rate": 0.0001758741165488732,
      "loss": 0.6176,
      "step": 4528
    },
    {
      "epoch": 0.36232,
      "grad_norm": 0.4632020890712738,
      "learning_rate": 0.00017586878250433393,
      "loss": 0.8505,
      "step": 4529
    },
    {
      "epoch": 0.3624,
      "grad_norm": 0.6017899513244629,
      "learning_rate": 0.00017586344845979464,
      "loss": 0.8601,
      "step": 4530
    },
    {
      "epoch": 0.36248,
      "grad_norm": 0.3768903315067291,
      "learning_rate": 0.00017585811441525538,
      "loss": 1.0423,
      "step": 4531
    },
    {
      "epoch": 0.36256,
      "grad_norm": 0.5381613969802856,
      "learning_rate": 0.0001758527803707161,
      "loss": 1.0408,
      "step": 4532
    },
    {
      "epoch": 0.36264,
      "grad_norm": 0.4053570032119751,
      "learning_rate": 0.00017584744632617684,
      "loss": 0.8073,
      "step": 4533
    },
    {
      "epoch": 0.36272,
      "grad_norm": 0.339832067489624,
      "learning_rate": 0.00017584211228163755,
      "loss": 0.6441,
      "step": 4534
    },
    {
      "epoch": 0.3628,
      "grad_norm": 0.3688490092754364,
      "learning_rate": 0.0001758367782370983,
      "loss": 0.6313,
      "step": 4535
    },
    {
      "epoch": 0.36288,
      "grad_norm": 0.5723078846931458,
      "learning_rate": 0.000175831444192559,
      "loss": 0.624,
      "step": 4536
    },
    {
      "epoch": 0.36296,
      "grad_norm": 0.4449809491634369,
      "learning_rate": 0.00017582611014801974,
      "loss": 0.7346,
      "step": 4537
    },
    {
      "epoch": 0.36304,
      "grad_norm": 0.2935035824775696,
      "learning_rate": 0.00017582077610348048,
      "loss": 1.077,
      "step": 4538
    },
    {
      "epoch": 0.36312,
      "grad_norm": 0.4407740533351898,
      "learning_rate": 0.0001758154420589412,
      "loss": 0.7834,
      "step": 4539
    },
    {
      "epoch": 0.3632,
      "grad_norm": 0.47619110345840454,
      "learning_rate": 0.00017581010801440193,
      "loss": 0.8614,
      "step": 4540
    },
    {
      "epoch": 0.36328,
      "grad_norm": 0.4651104509830475,
      "learning_rate": 0.00017580477396986265,
      "loss": 0.7449,
      "step": 4541
    },
    {
      "epoch": 0.36336,
      "grad_norm": 0.3147610127925873,
      "learning_rate": 0.00017579943992532339,
      "loss": 0.9282,
      "step": 4542
    },
    {
      "epoch": 0.36344,
      "grad_norm": 0.33107131719589233,
      "learning_rate": 0.0001757941058807841,
      "loss": 1.1616,
      "step": 4543
    },
    {
      "epoch": 0.36352,
      "grad_norm": 0.39212462306022644,
      "learning_rate": 0.00017578877183624484,
      "loss": 0.6443,
      "step": 4544
    },
    {
      "epoch": 0.3636,
      "grad_norm": 0.3689415156841278,
      "learning_rate": 0.00017578343779170555,
      "loss": 0.8839,
      "step": 4545
    },
    {
      "epoch": 0.36368,
      "grad_norm": 0.4663669764995575,
      "learning_rate": 0.0001757781037471663,
      "loss": 0.8228,
      "step": 4546
    },
    {
      "epoch": 0.36376,
      "grad_norm": 0.3731600344181061,
      "learning_rate": 0.000175772769702627,
      "loss": 0.9255,
      "step": 4547
    },
    {
      "epoch": 0.36384,
      "grad_norm": 0.41509902477264404,
      "learning_rate": 0.00017576743565808774,
      "loss": 0.8469,
      "step": 4548
    },
    {
      "epoch": 0.36392,
      "grad_norm": 0.3814743161201477,
      "learning_rate": 0.00017576210161354848,
      "loss": 1.2127,
      "step": 4549
    },
    {
      "epoch": 0.364,
      "grad_norm": 0.34667065739631653,
      "learning_rate": 0.0001757567675690092,
      "loss": 1.0186,
      "step": 4550
    },
    {
      "epoch": 0.36408,
      "grad_norm": 0.42379364371299744,
      "learning_rate": 0.00017575143352446994,
      "loss": 0.9181,
      "step": 4551
    },
    {
      "epoch": 0.36416,
      "grad_norm": 0.4099263548851013,
      "learning_rate": 0.00017574609947993065,
      "loss": 0.5544,
      "step": 4552
    },
    {
      "epoch": 0.36424,
      "grad_norm": 0.4173303544521332,
      "learning_rate": 0.0001757407654353914,
      "loss": 0.795,
      "step": 4553
    },
    {
      "epoch": 0.36432,
      "grad_norm": 0.3782128095626831,
      "learning_rate": 0.0001757354313908521,
      "loss": 0.9951,
      "step": 4554
    },
    {
      "epoch": 0.3644,
      "grad_norm": 0.33277982473373413,
      "learning_rate": 0.00017573009734631284,
      "loss": 0.7172,
      "step": 4555
    },
    {
      "epoch": 0.36448,
      "grad_norm": 0.4457934498786926,
      "learning_rate": 0.00017572476330177358,
      "loss": 0.8369,
      "step": 4556
    },
    {
      "epoch": 0.36456,
      "grad_norm": 0.34797024726867676,
      "learning_rate": 0.0001757194292572343,
      "loss": 0.7357,
      "step": 4557
    },
    {
      "epoch": 0.36464,
      "grad_norm": 0.3001500368118286,
      "learning_rate": 0.00017571409521269503,
      "loss": 1.0974,
      "step": 4558
    },
    {
      "epoch": 0.36472,
      "grad_norm": 0.46857723593711853,
      "learning_rate": 0.00017570876116815575,
      "loss": 1.0333,
      "step": 4559
    },
    {
      "epoch": 0.3648,
      "grad_norm": 0.3552940785884857,
      "learning_rate": 0.00017570342712361649,
      "loss": 0.8067,
      "step": 4560
    },
    {
      "epoch": 0.36488,
      "grad_norm": 0.3113314211368561,
      "learning_rate": 0.0001756980930790772,
      "loss": 0.6319,
      "step": 4561
    },
    {
      "epoch": 0.36496,
      "grad_norm": 0.3016396760940552,
      "learning_rate": 0.00017569275903453794,
      "loss": 0.7227,
      "step": 4562
    },
    {
      "epoch": 0.36504,
      "grad_norm": 0.4476875960826874,
      "learning_rate": 0.00017568742498999868,
      "loss": 0.5632,
      "step": 4563
    },
    {
      "epoch": 0.36512,
      "grad_norm": 0.4271044433116913,
      "learning_rate": 0.0001756820909454594,
      "loss": 0.7719,
      "step": 4564
    },
    {
      "epoch": 0.3652,
      "grad_norm": 0.4159587621688843,
      "learning_rate": 0.00017567675690092013,
      "loss": 0.6923,
      "step": 4565
    },
    {
      "epoch": 0.36528,
      "grad_norm": 0.24870522320270538,
      "learning_rate": 0.00017567142285638084,
      "loss": 0.4018,
      "step": 4566
    },
    {
      "epoch": 0.36536,
      "grad_norm": 0.4059218466281891,
      "learning_rate": 0.00017566608881184158,
      "loss": 0.915,
      "step": 4567
    },
    {
      "epoch": 0.36544,
      "grad_norm": 0.3328298032283783,
      "learning_rate": 0.0001756607547673023,
      "loss": 0.678,
      "step": 4568
    },
    {
      "epoch": 0.36552,
      "grad_norm": 0.423388808965683,
      "learning_rate": 0.00017565542072276304,
      "loss": 0.7793,
      "step": 4569
    },
    {
      "epoch": 0.3656,
      "grad_norm": 0.4549902081489563,
      "learning_rate": 0.00017565008667822378,
      "loss": 0.6255,
      "step": 4570
    },
    {
      "epoch": 0.36568,
      "grad_norm": 0.40780964493751526,
      "learning_rate": 0.0001756447526336845,
      "loss": 0.8154,
      "step": 4571
    },
    {
      "epoch": 0.36576,
      "grad_norm": 0.42338940501213074,
      "learning_rate": 0.00017563941858914523,
      "loss": 0.5196,
      "step": 4572
    },
    {
      "epoch": 0.36584,
      "grad_norm": 0.43648403882980347,
      "learning_rate": 0.00017563408454460594,
      "loss": 0.9045,
      "step": 4573
    },
    {
      "epoch": 0.36592,
      "grad_norm": 0.3871554732322693,
      "learning_rate": 0.00017562875050006668,
      "loss": 0.6046,
      "step": 4574
    },
    {
      "epoch": 0.366,
      "grad_norm": 0.4127397835254669,
      "learning_rate": 0.00017562341645552742,
      "loss": 0.6572,
      "step": 4575
    },
    {
      "epoch": 0.36608,
      "grad_norm": 0.40472841262817383,
      "learning_rate": 0.00017561808241098813,
      "loss": 0.76,
      "step": 4576
    },
    {
      "epoch": 0.36616,
      "grad_norm": 0.3284164369106293,
      "learning_rate": 0.00017561274836644887,
      "loss": 0.899,
      "step": 4577
    },
    {
      "epoch": 0.36624,
      "grad_norm": 0.3052682876586914,
      "learning_rate": 0.00017560741432190959,
      "loss": 0.8244,
      "step": 4578
    },
    {
      "epoch": 0.36632,
      "grad_norm": 0.41198447346687317,
      "learning_rate": 0.00017560208027737033,
      "loss": 0.7155,
      "step": 4579
    },
    {
      "epoch": 0.3664,
      "grad_norm": 0.44215795397758484,
      "learning_rate": 0.00017559674623283104,
      "loss": 0.9395,
      "step": 4580
    },
    {
      "epoch": 0.36648,
      "grad_norm": 0.3408844769001007,
      "learning_rate": 0.00017559141218829178,
      "loss": 0.8803,
      "step": 4581
    },
    {
      "epoch": 0.36656,
      "grad_norm": 0.44959375262260437,
      "learning_rate": 0.00017558607814375252,
      "loss": 1.034,
      "step": 4582
    },
    {
      "epoch": 0.36664,
      "grad_norm": 0.33626362681388855,
      "learning_rate": 0.00017558074409921323,
      "loss": 1.0917,
      "step": 4583
    },
    {
      "epoch": 0.36672,
      "grad_norm": 0.43330448865890503,
      "learning_rate": 0.00017557541005467397,
      "loss": 1.0233,
      "step": 4584
    },
    {
      "epoch": 0.3668,
      "grad_norm": 0.3510514199733734,
      "learning_rate": 0.00017557007601013468,
      "loss": 0.5676,
      "step": 4585
    },
    {
      "epoch": 0.36688,
      "grad_norm": 0.4263828992843628,
      "learning_rate": 0.00017556474196559542,
      "loss": 0.9022,
      "step": 4586
    },
    {
      "epoch": 0.36696,
      "grad_norm": 0.3764125406742096,
      "learning_rate": 0.00017555940792105614,
      "loss": 0.8137,
      "step": 4587
    },
    {
      "epoch": 0.36704,
      "grad_norm": 0.34654200077056885,
      "learning_rate": 0.00017555407387651688,
      "loss": 0.6016,
      "step": 4588
    },
    {
      "epoch": 0.36712,
      "grad_norm": 0.38529062271118164,
      "learning_rate": 0.00017554873983197762,
      "loss": 1.0304,
      "step": 4589
    },
    {
      "epoch": 0.3672,
      "grad_norm": 0.4332068860530853,
      "learning_rate": 0.00017554340578743833,
      "loss": 0.8155,
      "step": 4590
    },
    {
      "epoch": 0.36728,
      "grad_norm": 0.3382910192012787,
      "learning_rate": 0.00017553807174289907,
      "loss": 0.8359,
      "step": 4591
    },
    {
      "epoch": 0.36736,
      "grad_norm": 0.3346709609031677,
      "learning_rate": 0.00017553273769835978,
      "loss": 0.6067,
      "step": 4592
    },
    {
      "epoch": 0.36744,
      "grad_norm": 0.36251842975616455,
      "learning_rate": 0.00017552740365382052,
      "loss": 0.8558,
      "step": 4593
    },
    {
      "epoch": 0.36752,
      "grad_norm": 0.34060433506965637,
      "learning_rate": 0.00017552206960928123,
      "loss": 0.7932,
      "step": 4594
    },
    {
      "epoch": 0.3676,
      "grad_norm": 0.43389976024627686,
      "learning_rate": 0.00017551673556474197,
      "loss": 1.1996,
      "step": 4595
    },
    {
      "epoch": 0.36768,
      "grad_norm": 0.4994945526123047,
      "learning_rate": 0.00017551140152020271,
      "loss": 1.0333,
      "step": 4596
    },
    {
      "epoch": 0.36776,
      "grad_norm": 0.39881259202957153,
      "learning_rate": 0.00017550606747566343,
      "loss": 0.6139,
      "step": 4597
    },
    {
      "epoch": 0.36784,
      "grad_norm": 0.3916316330432892,
      "learning_rate": 0.00017550073343112417,
      "loss": 0.7231,
      "step": 4598
    },
    {
      "epoch": 0.36792,
      "grad_norm": 0.3050474524497986,
      "learning_rate": 0.00017549539938658488,
      "loss": 0.9408,
      "step": 4599
    },
    {
      "epoch": 0.368,
      "grad_norm": 0.3723296523094177,
      "learning_rate": 0.00017549006534204562,
      "loss": 1.0948,
      "step": 4600
    },
    {
      "epoch": 0.36808,
      "grad_norm": 0.4502347707748413,
      "learning_rate": 0.00017548473129750633,
      "loss": 0.8595,
      "step": 4601
    },
    {
      "epoch": 0.36816,
      "grad_norm": 0.36997348070144653,
      "learning_rate": 0.00017547939725296707,
      "loss": 0.6725,
      "step": 4602
    },
    {
      "epoch": 0.36824,
      "grad_norm": 0.43113064765930176,
      "learning_rate": 0.0001754740632084278,
      "loss": 0.8938,
      "step": 4603
    },
    {
      "epoch": 0.36832,
      "grad_norm": 0.5179538726806641,
      "learning_rate": 0.00017546872916388852,
      "loss": 0.7009,
      "step": 4604
    },
    {
      "epoch": 0.3684,
      "grad_norm": 0.3561841547489166,
      "learning_rate": 0.00017546339511934926,
      "loss": 0.4678,
      "step": 4605
    },
    {
      "epoch": 0.36848,
      "grad_norm": 0.4507729113101959,
      "learning_rate": 0.00017545806107480998,
      "loss": 0.9905,
      "step": 4606
    },
    {
      "epoch": 0.36856,
      "grad_norm": 0.3303476572036743,
      "learning_rate": 0.00017545272703027072,
      "loss": 0.9867,
      "step": 4607
    },
    {
      "epoch": 0.36864,
      "grad_norm": 0.3661591112613678,
      "learning_rate": 0.00017544739298573143,
      "loss": 0.6397,
      "step": 4608
    },
    {
      "epoch": 0.36872,
      "grad_norm": 0.34541603922843933,
      "learning_rate": 0.00017544205894119217,
      "loss": 0.6581,
      "step": 4609
    },
    {
      "epoch": 0.3688,
      "grad_norm": 0.36831897497177124,
      "learning_rate": 0.0001754367248966529,
      "loss": 1.056,
      "step": 4610
    },
    {
      "epoch": 0.36888,
      "grad_norm": 0.3686811625957489,
      "learning_rate": 0.00017543139085211362,
      "loss": 0.9978,
      "step": 4611
    },
    {
      "epoch": 0.36896,
      "grad_norm": 0.36807873845100403,
      "learning_rate": 0.00017542605680757436,
      "loss": 0.9179,
      "step": 4612
    },
    {
      "epoch": 0.36904,
      "grad_norm": 0.37129732966423035,
      "learning_rate": 0.00017542072276303507,
      "loss": 0.6248,
      "step": 4613
    },
    {
      "epoch": 0.36912,
      "grad_norm": 0.33079075813293457,
      "learning_rate": 0.00017541538871849581,
      "loss": 0.6372,
      "step": 4614
    },
    {
      "epoch": 0.3692,
      "grad_norm": 0.24358120560646057,
      "learning_rate": 0.00017541005467395653,
      "loss": 0.6064,
      "step": 4615
    },
    {
      "epoch": 0.36928,
      "grad_norm": 0.4519207775592804,
      "learning_rate": 0.00017540472062941727,
      "loss": 0.9004,
      "step": 4616
    },
    {
      "epoch": 0.36936,
      "grad_norm": 0.3389029800891876,
      "learning_rate": 0.000175399386584878,
      "loss": 0.5037,
      "step": 4617
    },
    {
      "epoch": 0.36944,
      "grad_norm": 0.4043501019477844,
      "learning_rate": 0.00017539405254033872,
      "loss": 0.8456,
      "step": 4618
    },
    {
      "epoch": 0.36952,
      "grad_norm": 0.4001086950302124,
      "learning_rate": 0.00017538871849579946,
      "loss": 0.6331,
      "step": 4619
    },
    {
      "epoch": 0.3696,
      "grad_norm": 0.37168315052986145,
      "learning_rate": 0.00017538338445126017,
      "loss": 0.7257,
      "step": 4620
    },
    {
      "epoch": 0.36968,
      "grad_norm": 0.3448309302330017,
      "learning_rate": 0.0001753780504067209,
      "loss": 0.7847,
      "step": 4621
    },
    {
      "epoch": 0.36976,
      "grad_norm": 0.4265720844268799,
      "learning_rate": 0.00017537271636218162,
      "loss": 0.5502,
      "step": 4622
    },
    {
      "epoch": 0.36984,
      "grad_norm": 0.4183647036552429,
      "learning_rate": 0.00017536738231764236,
      "loss": 1.1157,
      "step": 4623
    },
    {
      "epoch": 0.36992,
      "grad_norm": 0.33291929960250854,
      "learning_rate": 0.0001753620482731031,
      "loss": 0.8189,
      "step": 4624
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.2908630073070526,
      "learning_rate": 0.00017535671422856382,
      "loss": 0.4293,
      "step": 4625
    },
    {
      "epoch": 0.37008,
      "grad_norm": 0.4498998820781708,
      "learning_rate": 0.00017535138018402456,
      "loss": 0.6836,
      "step": 4626
    },
    {
      "epoch": 0.37016,
      "grad_norm": 0.35564881563186646,
      "learning_rate": 0.00017534604613948527,
      "loss": 0.7914,
      "step": 4627
    },
    {
      "epoch": 0.37024,
      "grad_norm": 0.4022984802722931,
      "learning_rate": 0.000175340712094946,
      "loss": 0.7988,
      "step": 4628
    },
    {
      "epoch": 0.37032,
      "grad_norm": 0.4176187813282013,
      "learning_rate": 0.00017533537805040675,
      "loss": 0.8524,
      "step": 4629
    },
    {
      "epoch": 0.3704,
      "grad_norm": 0.4712640047073364,
      "learning_rate": 0.00017533004400586746,
      "loss": 0.9121,
      "step": 4630
    },
    {
      "epoch": 0.37048,
      "grad_norm": 0.42636749148368835,
      "learning_rate": 0.0001753247099613282,
      "loss": 0.675,
      "step": 4631
    },
    {
      "epoch": 0.37056,
      "grad_norm": 0.3219384253025055,
      "learning_rate": 0.00017531937591678891,
      "loss": 0.6282,
      "step": 4632
    },
    {
      "epoch": 0.37064,
      "grad_norm": 0.4301268458366394,
      "learning_rate": 0.00017531404187224965,
      "loss": 0.8112,
      "step": 4633
    },
    {
      "epoch": 0.37072,
      "grad_norm": 0.49819672107696533,
      "learning_rate": 0.00017530870782771037,
      "loss": 0.76,
      "step": 4634
    },
    {
      "epoch": 0.3708,
      "grad_norm": 0.35971251130104065,
      "learning_rate": 0.0001753033737831711,
      "loss": 0.9088,
      "step": 4635
    },
    {
      "epoch": 0.37088,
      "grad_norm": 0.3501943349838257,
      "learning_rate": 0.00017529803973863185,
      "loss": 0.7184,
      "step": 4636
    },
    {
      "epoch": 0.37096,
      "grad_norm": 0.40640178322792053,
      "learning_rate": 0.00017529270569409256,
      "loss": 0.7165,
      "step": 4637
    },
    {
      "epoch": 0.37104,
      "grad_norm": 0.40119093656539917,
      "learning_rate": 0.0001752873716495533,
      "loss": 0.5575,
      "step": 4638
    },
    {
      "epoch": 0.37112,
      "grad_norm": 0.3619054853916168,
      "learning_rate": 0.000175282037605014,
      "loss": 0.7022,
      "step": 4639
    },
    {
      "epoch": 0.3712,
      "grad_norm": 0.3097795844078064,
      "learning_rate": 0.00017527670356047475,
      "loss": 0.684,
      "step": 4640
    },
    {
      "epoch": 0.37128,
      "grad_norm": 0.5058057308197021,
      "learning_rate": 0.00017527136951593546,
      "loss": 0.9434,
      "step": 4641
    },
    {
      "epoch": 0.37136,
      "grad_norm": 0.29008880257606506,
      "learning_rate": 0.0001752660354713962,
      "loss": 0.792,
      "step": 4642
    },
    {
      "epoch": 0.37144,
      "grad_norm": 0.39203503727912903,
      "learning_rate": 0.00017526070142685694,
      "loss": 0.9747,
      "step": 4643
    },
    {
      "epoch": 0.37152,
      "grad_norm": 0.47220471501350403,
      "learning_rate": 0.00017525536738231766,
      "loss": 0.8562,
      "step": 4644
    },
    {
      "epoch": 0.3716,
      "grad_norm": 0.34665781259536743,
      "learning_rate": 0.0001752500333377784,
      "loss": 0.953,
      "step": 4645
    },
    {
      "epoch": 0.37168,
      "grad_norm": 0.42314422130584717,
      "learning_rate": 0.0001752446992932391,
      "loss": 1.0923,
      "step": 4646
    },
    {
      "epoch": 0.37176,
      "grad_norm": 0.5425708293914795,
      "learning_rate": 0.00017523936524869985,
      "loss": 1.1198,
      "step": 4647
    },
    {
      "epoch": 0.37184,
      "grad_norm": 0.3761043846607208,
      "learning_rate": 0.00017523403120416056,
      "loss": 0.6449,
      "step": 4648
    },
    {
      "epoch": 0.37192,
      "grad_norm": 0.47600415349006653,
      "learning_rate": 0.0001752286971596213,
      "loss": 0.8748,
      "step": 4649
    },
    {
      "epoch": 0.372,
      "grad_norm": 0.395067036151886,
      "learning_rate": 0.00017522336311508204,
      "loss": 0.8076,
      "step": 4650
    },
    {
      "epoch": 0.37208,
      "grad_norm": 0.4462771713733673,
      "learning_rate": 0.00017521802907054275,
      "loss": 0.7845,
      "step": 4651
    },
    {
      "epoch": 0.37216,
      "grad_norm": 0.40717393159866333,
      "learning_rate": 0.0001752126950260035,
      "loss": 0.786,
      "step": 4652
    },
    {
      "epoch": 0.37224,
      "grad_norm": 0.33302566409111023,
      "learning_rate": 0.0001752073609814642,
      "loss": 0.6619,
      "step": 4653
    },
    {
      "epoch": 0.37232,
      "grad_norm": 0.3810095191001892,
      "learning_rate": 0.00017520202693692495,
      "loss": 0.6802,
      "step": 4654
    },
    {
      "epoch": 0.3724,
      "grad_norm": 0.4180354177951813,
      "learning_rate": 0.00017519669289238566,
      "loss": 0.9117,
      "step": 4655
    },
    {
      "epoch": 0.37248,
      "grad_norm": 0.4426875412464142,
      "learning_rate": 0.0001751913588478464,
      "loss": 0.7398,
      "step": 4656
    },
    {
      "epoch": 0.37256,
      "grad_norm": 0.3261738717556,
      "learning_rate": 0.0001751860248033071,
      "loss": 0.9136,
      "step": 4657
    },
    {
      "epoch": 0.37264,
      "grad_norm": 0.33853840827941895,
      "learning_rate": 0.00017518069075876785,
      "loss": 1.0509,
      "step": 4658
    },
    {
      "epoch": 0.37272,
      "grad_norm": 0.35924792289733887,
      "learning_rate": 0.00017517535671422857,
      "loss": 0.9014,
      "step": 4659
    },
    {
      "epoch": 0.3728,
      "grad_norm": 0.4163784086704254,
      "learning_rate": 0.0001751700226696893,
      "loss": 0.757,
      "step": 4660
    },
    {
      "epoch": 0.37288,
      "grad_norm": 0.26071691513061523,
      "learning_rate": 0.00017516468862515002,
      "loss": 0.5751,
      "step": 4661
    },
    {
      "epoch": 0.37296,
      "grad_norm": 0.3814410865306854,
      "learning_rate": 0.00017515935458061076,
      "loss": 0.6905,
      "step": 4662
    },
    {
      "epoch": 0.37304,
      "grad_norm": 0.35033324360847473,
      "learning_rate": 0.00017515402053607147,
      "loss": 1.1373,
      "step": 4663
    },
    {
      "epoch": 0.37312,
      "grad_norm": 0.32002854347229004,
      "learning_rate": 0.0001751486864915322,
      "loss": 0.9863,
      "step": 4664
    },
    {
      "epoch": 0.3732,
      "grad_norm": 0.40524107217788696,
      "learning_rate": 0.00017514335244699292,
      "loss": 0.8423,
      "step": 4665
    },
    {
      "epoch": 0.37328,
      "grad_norm": 0.4624645709991455,
      "learning_rate": 0.00017513801840245366,
      "loss": 0.9646,
      "step": 4666
    },
    {
      "epoch": 0.37336,
      "grad_norm": 0.4213910698890686,
      "learning_rate": 0.0001751326843579144,
      "loss": 0.9774,
      "step": 4667
    },
    {
      "epoch": 0.37344,
      "grad_norm": 0.2843893766403198,
      "learning_rate": 0.00017512735031337512,
      "loss": 0.7748,
      "step": 4668
    },
    {
      "epoch": 0.37352,
      "grad_norm": 0.3409428000450134,
      "learning_rate": 0.00017512201626883586,
      "loss": 0.8567,
      "step": 4669
    },
    {
      "epoch": 0.3736,
      "grad_norm": 0.3979073762893677,
      "learning_rate": 0.00017511668222429657,
      "loss": 0.7914,
      "step": 4670
    },
    {
      "epoch": 0.37368,
      "grad_norm": 0.31944313645362854,
      "learning_rate": 0.0001751113481797573,
      "loss": 0.5489,
      "step": 4671
    },
    {
      "epoch": 0.37376,
      "grad_norm": 0.39375486969947815,
      "learning_rate": 0.00017510601413521802,
      "loss": 0.9162,
      "step": 4672
    },
    {
      "epoch": 0.37384,
      "grad_norm": 0.4278876781463623,
      "learning_rate": 0.00017510068009067876,
      "loss": 0.6744,
      "step": 4673
    },
    {
      "epoch": 0.37392,
      "grad_norm": 0.36794352531433105,
      "learning_rate": 0.00017509534604613947,
      "loss": 1.1789,
      "step": 4674
    },
    {
      "epoch": 0.374,
      "grad_norm": 0.551435649394989,
      "learning_rate": 0.0001750900120016002,
      "loss": 1.0041,
      "step": 4675
    },
    {
      "epoch": 0.37408,
      "grad_norm": 0.3857654929161072,
      "learning_rate": 0.00017508467795706095,
      "loss": 0.8184,
      "step": 4676
    },
    {
      "epoch": 0.37416,
      "grad_norm": 0.3998207747936249,
      "learning_rate": 0.00017507934391252167,
      "loss": 1.0681,
      "step": 4677
    },
    {
      "epoch": 0.37424,
      "grad_norm": 0.3900725841522217,
      "learning_rate": 0.0001750740098679824,
      "loss": 0.7832,
      "step": 4678
    },
    {
      "epoch": 0.37432,
      "grad_norm": 0.3527984619140625,
      "learning_rate": 0.00017506867582344312,
      "loss": 1.0926,
      "step": 4679
    },
    {
      "epoch": 0.3744,
      "grad_norm": 0.5626261830329895,
      "learning_rate": 0.00017506334177890386,
      "loss": 1.0632,
      "step": 4680
    },
    {
      "epoch": 0.37448,
      "grad_norm": 0.3779471516609192,
      "learning_rate": 0.00017505800773436457,
      "loss": 0.5622,
      "step": 4681
    },
    {
      "epoch": 0.37456,
      "grad_norm": 0.35617339611053467,
      "learning_rate": 0.0001750526736898253,
      "loss": 1.007,
      "step": 4682
    },
    {
      "epoch": 0.37464,
      "grad_norm": 0.479696124792099,
      "learning_rate": 0.00017504733964528605,
      "loss": 1.0169,
      "step": 4683
    },
    {
      "epoch": 0.37472,
      "grad_norm": 0.46262896060943604,
      "learning_rate": 0.00017504200560074676,
      "loss": 0.9785,
      "step": 4684
    },
    {
      "epoch": 0.3748,
      "grad_norm": 0.5209002494812012,
      "learning_rate": 0.0001750366715562075,
      "loss": 0.9265,
      "step": 4685
    },
    {
      "epoch": 0.37488,
      "grad_norm": 0.38389211893081665,
      "learning_rate": 0.00017503133751166822,
      "loss": 0.6306,
      "step": 4686
    },
    {
      "epoch": 0.37496,
      "grad_norm": 0.5036189556121826,
      "learning_rate": 0.00017502600346712896,
      "loss": 0.8323,
      "step": 4687
    },
    {
      "epoch": 0.37504,
      "grad_norm": 0.4629255533218384,
      "learning_rate": 0.00017502066942258967,
      "loss": 1.0091,
      "step": 4688
    },
    {
      "epoch": 0.37512,
      "grad_norm": 0.4444971978664398,
      "learning_rate": 0.0001750153353780504,
      "loss": 0.9756,
      "step": 4689
    },
    {
      "epoch": 0.3752,
      "grad_norm": 0.4080909788608551,
      "learning_rate": 0.00017501000133351115,
      "loss": 0.6361,
      "step": 4690
    },
    {
      "epoch": 0.37528,
      "grad_norm": 0.3026799261569977,
      "learning_rate": 0.00017500466728897186,
      "loss": 1.0374,
      "step": 4691
    },
    {
      "epoch": 0.37536,
      "grad_norm": 0.3702032268047333,
      "learning_rate": 0.0001749993332444326,
      "loss": 0.5796,
      "step": 4692
    },
    {
      "epoch": 0.37544,
      "grad_norm": 0.39505186676979065,
      "learning_rate": 0.00017499399919989331,
      "loss": 0.8231,
      "step": 4693
    },
    {
      "epoch": 0.37552,
      "grad_norm": 0.3889012932777405,
      "learning_rate": 0.00017498866515535405,
      "loss": 0.6639,
      "step": 4694
    },
    {
      "epoch": 0.3756,
      "grad_norm": 0.4010252058506012,
      "learning_rate": 0.00017498333111081477,
      "loss": 0.7223,
      "step": 4695
    },
    {
      "epoch": 0.37568,
      "grad_norm": 0.46093857288360596,
      "learning_rate": 0.0001749779970662755,
      "loss": 0.7322,
      "step": 4696
    },
    {
      "epoch": 0.37576,
      "grad_norm": 0.4909261465072632,
      "learning_rate": 0.00017497266302173625,
      "loss": 0.7409,
      "step": 4697
    },
    {
      "epoch": 0.37584,
      "grad_norm": 0.3405039608478546,
      "learning_rate": 0.00017496732897719696,
      "loss": 0.7943,
      "step": 4698
    },
    {
      "epoch": 0.37592,
      "grad_norm": 0.479092538356781,
      "learning_rate": 0.0001749619949326577,
      "loss": 0.8315,
      "step": 4699
    },
    {
      "epoch": 0.376,
      "grad_norm": 0.3782128393650055,
      "learning_rate": 0.0001749566608881184,
      "loss": 0.6852,
      "step": 4700
    },
    {
      "epoch": 0.37608,
      "grad_norm": 0.3175279498100281,
      "learning_rate": 0.00017495132684357915,
      "loss": 0.8231,
      "step": 4701
    },
    {
      "epoch": 0.37616,
      "grad_norm": 0.480468213558197,
      "learning_rate": 0.00017494599279903986,
      "loss": 0.8895,
      "step": 4702
    },
    {
      "epoch": 0.37624,
      "grad_norm": 0.49504587054252625,
      "learning_rate": 0.0001749406587545006,
      "loss": 0.7797,
      "step": 4703
    },
    {
      "epoch": 0.37632,
      "grad_norm": 0.35155102610588074,
      "learning_rate": 0.00017493532470996134,
      "loss": 0.8357,
      "step": 4704
    },
    {
      "epoch": 0.3764,
      "grad_norm": 0.4262375235557556,
      "learning_rate": 0.00017492999066542206,
      "loss": 0.7254,
      "step": 4705
    },
    {
      "epoch": 0.37648,
      "grad_norm": 0.44986364245414734,
      "learning_rate": 0.0001749246566208828,
      "loss": 0.9071,
      "step": 4706
    },
    {
      "epoch": 0.37656,
      "grad_norm": 0.36113405227661133,
      "learning_rate": 0.0001749193225763435,
      "loss": 0.8431,
      "step": 4707
    },
    {
      "epoch": 0.37664,
      "grad_norm": 0.41352951526641846,
      "learning_rate": 0.00017491398853180425,
      "loss": 0.8897,
      "step": 4708
    },
    {
      "epoch": 0.37672,
      "grad_norm": 0.37429380416870117,
      "learning_rate": 0.00017490865448726496,
      "loss": 0.5081,
      "step": 4709
    },
    {
      "epoch": 0.3768,
      "grad_norm": 0.4749155342578888,
      "learning_rate": 0.0001749033204427257,
      "loss": 0.6987,
      "step": 4710
    },
    {
      "epoch": 0.37688,
      "grad_norm": 0.3398142158985138,
      "learning_rate": 0.00017489798639818644,
      "loss": 0.8265,
      "step": 4711
    },
    {
      "epoch": 0.37696,
      "grad_norm": 0.49415332078933716,
      "learning_rate": 0.00017489265235364715,
      "loss": 0.7155,
      "step": 4712
    },
    {
      "epoch": 0.37704,
      "grad_norm": 0.4086730182170868,
      "learning_rate": 0.0001748873183091079,
      "loss": 0.757,
      "step": 4713
    },
    {
      "epoch": 0.37712,
      "grad_norm": 0.39173007011413574,
      "learning_rate": 0.0001748819842645686,
      "loss": 0.5838,
      "step": 4714
    },
    {
      "epoch": 0.3772,
      "grad_norm": 0.307565301656723,
      "learning_rate": 0.00017487665022002935,
      "loss": 1.0588,
      "step": 4715
    },
    {
      "epoch": 0.37728,
      "grad_norm": 0.3001907169818878,
      "learning_rate": 0.00017487131617549006,
      "loss": 1.2281,
      "step": 4716
    },
    {
      "epoch": 0.37736,
      "grad_norm": 0.3427831828594208,
      "learning_rate": 0.0001748659821309508,
      "loss": 0.6012,
      "step": 4717
    },
    {
      "epoch": 0.37744,
      "grad_norm": 0.3924895226955414,
      "learning_rate": 0.00017486064808641154,
      "loss": 0.8948,
      "step": 4718
    },
    {
      "epoch": 0.37752,
      "grad_norm": 0.44967034459114075,
      "learning_rate": 0.00017485531404187225,
      "loss": 0.9782,
      "step": 4719
    },
    {
      "epoch": 0.3776,
      "grad_norm": 0.5300252437591553,
      "learning_rate": 0.000174849979997333,
      "loss": 0.6203,
      "step": 4720
    },
    {
      "epoch": 0.37768,
      "grad_norm": 0.3400428295135498,
      "learning_rate": 0.0001748446459527937,
      "loss": 1.0513,
      "step": 4721
    },
    {
      "epoch": 0.37776,
      "grad_norm": 0.4379081428050995,
      "learning_rate": 0.00017483931190825444,
      "loss": 0.6878,
      "step": 4722
    },
    {
      "epoch": 0.37784,
      "grad_norm": 0.3121089041233063,
      "learning_rate": 0.00017483397786371516,
      "loss": 0.5785,
      "step": 4723
    },
    {
      "epoch": 0.37792,
      "grad_norm": 0.4447416067123413,
      "learning_rate": 0.0001748286438191759,
      "loss": 0.7589,
      "step": 4724
    },
    {
      "epoch": 0.378,
      "grad_norm": 0.3133344352245331,
      "learning_rate": 0.00017482330977463664,
      "loss": 0.5062,
      "step": 4725
    },
    {
      "epoch": 0.37808,
      "grad_norm": 0.26096010208129883,
      "learning_rate": 0.00017481797573009735,
      "loss": 0.7471,
      "step": 4726
    },
    {
      "epoch": 0.37816,
      "grad_norm": 0.3538714051246643,
      "learning_rate": 0.0001748126416855581,
      "loss": 0.6399,
      "step": 4727
    },
    {
      "epoch": 0.37824,
      "grad_norm": 0.30277952551841736,
      "learning_rate": 0.0001748073076410188,
      "loss": 0.7121,
      "step": 4728
    },
    {
      "epoch": 0.37832,
      "grad_norm": 0.4220854938030243,
      "learning_rate": 0.00017480197359647954,
      "loss": 0.6572,
      "step": 4729
    },
    {
      "epoch": 0.3784,
      "grad_norm": 0.4336013197898865,
      "learning_rate": 0.00017479663955194028,
      "loss": 0.9439,
      "step": 4730
    },
    {
      "epoch": 0.37848,
      "grad_norm": 0.495169997215271,
      "learning_rate": 0.000174791305507401,
      "loss": 0.8021,
      "step": 4731
    },
    {
      "epoch": 0.37856,
      "grad_norm": 0.4223473072052002,
      "learning_rate": 0.00017478597146286173,
      "loss": 0.95,
      "step": 4732
    },
    {
      "epoch": 0.37864,
      "grad_norm": 0.4215003252029419,
      "learning_rate": 0.00017478063741832245,
      "loss": 0.9971,
      "step": 4733
    },
    {
      "epoch": 0.37872,
      "grad_norm": 0.39939388632774353,
      "learning_rate": 0.0001747753033737832,
      "loss": 1.0184,
      "step": 4734
    },
    {
      "epoch": 0.3788,
      "grad_norm": 0.3848205506801605,
      "learning_rate": 0.0001747699693292439,
      "loss": 1.0268,
      "step": 4735
    },
    {
      "epoch": 0.37888,
      "grad_norm": 0.3749924600124359,
      "learning_rate": 0.00017476463528470464,
      "loss": 0.7296,
      "step": 4736
    },
    {
      "epoch": 0.37896,
      "grad_norm": 0.2790282070636749,
      "learning_rate": 0.00017475930124016538,
      "loss": 0.9511,
      "step": 4737
    },
    {
      "epoch": 0.37904,
      "grad_norm": 0.4230467975139618,
      "learning_rate": 0.0001747539671956261,
      "loss": 0.8631,
      "step": 4738
    },
    {
      "epoch": 0.37912,
      "grad_norm": 0.590752124786377,
      "learning_rate": 0.00017474863315108683,
      "loss": 1.1085,
      "step": 4739
    },
    {
      "epoch": 0.3792,
      "grad_norm": 0.46776360273361206,
      "learning_rate": 0.00017474329910654754,
      "loss": 0.6927,
      "step": 4740
    },
    {
      "epoch": 0.37928,
      "grad_norm": 0.3400854766368866,
      "learning_rate": 0.00017473796506200828,
      "loss": 0.7312,
      "step": 4741
    },
    {
      "epoch": 0.37936,
      "grad_norm": 0.45025646686553955,
      "learning_rate": 0.000174732631017469,
      "loss": 0.9594,
      "step": 4742
    },
    {
      "epoch": 0.37944,
      "grad_norm": 0.3298785388469696,
      "learning_rate": 0.00017472729697292974,
      "loss": 0.6904,
      "step": 4743
    },
    {
      "epoch": 0.37952,
      "grad_norm": 0.41386571526527405,
      "learning_rate": 0.00017472196292839048,
      "loss": 0.9143,
      "step": 4744
    },
    {
      "epoch": 0.3796,
      "grad_norm": 0.441269189119339,
      "learning_rate": 0.0001747166288838512,
      "loss": 0.7183,
      "step": 4745
    },
    {
      "epoch": 0.37968,
      "grad_norm": 0.3772302567958832,
      "learning_rate": 0.00017471129483931193,
      "loss": 0.7708,
      "step": 4746
    },
    {
      "epoch": 0.37976,
      "grad_norm": 0.3239850401878357,
      "learning_rate": 0.00017470596079477264,
      "loss": 0.5957,
      "step": 4747
    },
    {
      "epoch": 0.37984,
      "grad_norm": 0.4087948799133301,
      "learning_rate": 0.00017470062675023338,
      "loss": 0.9748,
      "step": 4748
    },
    {
      "epoch": 0.37992,
      "grad_norm": 0.4385465085506439,
      "learning_rate": 0.0001746952927056941,
      "loss": 1.1035,
      "step": 4749
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.32489752769470215,
      "learning_rate": 0.00017468995866115483,
      "loss": 0.7814,
      "step": 4750
    },
    {
      "epoch": 0.38008,
      "grad_norm": 0.34768906235694885,
      "learning_rate": 0.00017468462461661557,
      "loss": 0.7775,
      "step": 4751
    },
    {
      "epoch": 0.38016,
      "grad_norm": 0.3675723671913147,
      "learning_rate": 0.0001746792905720763,
      "loss": 1.0131,
      "step": 4752
    },
    {
      "epoch": 0.38024,
      "grad_norm": 0.44118788838386536,
      "learning_rate": 0.00017467395652753703,
      "loss": 0.6086,
      "step": 4753
    },
    {
      "epoch": 0.38032,
      "grad_norm": 0.3664707839488983,
      "learning_rate": 0.00017466862248299774,
      "loss": 0.672,
      "step": 4754
    },
    {
      "epoch": 0.3804,
      "grad_norm": 0.4256531000137329,
      "learning_rate": 0.00017466328843845848,
      "loss": 0.6216,
      "step": 4755
    },
    {
      "epoch": 0.38048,
      "grad_norm": 0.47691959142684937,
      "learning_rate": 0.0001746579543939192,
      "loss": 0.7726,
      "step": 4756
    },
    {
      "epoch": 0.38056,
      "grad_norm": 0.5181361436843872,
      "learning_rate": 0.00017465262034937993,
      "loss": 0.6318,
      "step": 4757
    },
    {
      "epoch": 0.38064,
      "grad_norm": 0.4109749495983124,
      "learning_rate": 0.00017464728630484067,
      "loss": 0.8594,
      "step": 4758
    },
    {
      "epoch": 0.38072,
      "grad_norm": 0.44818106293678284,
      "learning_rate": 0.00017464195226030138,
      "loss": 0.6229,
      "step": 4759
    },
    {
      "epoch": 0.3808,
      "grad_norm": 0.4087936580181122,
      "learning_rate": 0.00017463661821576212,
      "loss": 0.7674,
      "step": 4760
    },
    {
      "epoch": 0.38088,
      "grad_norm": 0.4585002660751343,
      "learning_rate": 0.00017463128417122284,
      "loss": 1.1204,
      "step": 4761
    },
    {
      "epoch": 0.38096,
      "grad_norm": 0.3759806752204895,
      "learning_rate": 0.00017462595012668358,
      "loss": 0.8979,
      "step": 4762
    },
    {
      "epoch": 0.38104,
      "grad_norm": 0.4018392860889435,
      "learning_rate": 0.0001746206160821443,
      "loss": 0.8032,
      "step": 4763
    },
    {
      "epoch": 0.38112,
      "grad_norm": 0.34207049012184143,
      "learning_rate": 0.00017461528203760503,
      "loss": 0.7848,
      "step": 4764
    },
    {
      "epoch": 0.3812,
      "grad_norm": 0.3140849471092224,
      "learning_rate": 0.00017460994799306577,
      "loss": 0.4792,
      "step": 4765
    },
    {
      "epoch": 0.38128,
      "grad_norm": 0.4027181565761566,
      "learning_rate": 0.00017460461394852648,
      "loss": 0.7128,
      "step": 4766
    },
    {
      "epoch": 0.38136,
      "grad_norm": 0.34212878346443176,
      "learning_rate": 0.00017459927990398722,
      "loss": 0.8318,
      "step": 4767
    },
    {
      "epoch": 0.38144,
      "grad_norm": 0.3172037601470947,
      "learning_rate": 0.00017459394585944793,
      "loss": 0.5171,
      "step": 4768
    },
    {
      "epoch": 0.38152,
      "grad_norm": 0.4103982448577881,
      "learning_rate": 0.00017458861181490867,
      "loss": 1.1114,
      "step": 4769
    },
    {
      "epoch": 0.3816,
      "grad_norm": 0.34131425619125366,
      "learning_rate": 0.0001745832777703694,
      "loss": 0.7481,
      "step": 4770
    },
    {
      "epoch": 0.38168,
      "grad_norm": 0.3371087610721588,
      "learning_rate": 0.00017457794372583013,
      "loss": 0.9296,
      "step": 4771
    },
    {
      "epoch": 0.38176,
      "grad_norm": 0.30656924843788147,
      "learning_rate": 0.00017457260968129087,
      "loss": 0.7053,
      "step": 4772
    },
    {
      "epoch": 0.38184,
      "grad_norm": 0.3583935499191284,
      "learning_rate": 0.00017456727563675158,
      "loss": 0.5154,
      "step": 4773
    },
    {
      "epoch": 0.38192,
      "grad_norm": 0.39112570881843567,
      "learning_rate": 0.00017456194159221232,
      "loss": 0.8756,
      "step": 4774
    },
    {
      "epoch": 0.382,
      "grad_norm": 0.43939486145973206,
      "learning_rate": 0.00017455660754767303,
      "loss": 0.7523,
      "step": 4775
    },
    {
      "epoch": 0.38208,
      "grad_norm": 0.4438703954219818,
      "learning_rate": 0.00017455127350313377,
      "loss": 0.8228,
      "step": 4776
    },
    {
      "epoch": 0.38216,
      "grad_norm": 0.3368006944656372,
      "learning_rate": 0.00017454593945859449,
      "loss": 0.6735,
      "step": 4777
    },
    {
      "epoch": 0.38224,
      "grad_norm": 0.4482788145542145,
      "learning_rate": 0.00017454060541405522,
      "loss": 0.823,
      "step": 4778
    },
    {
      "epoch": 0.38232,
      "grad_norm": 0.3000885844230652,
      "learning_rate": 0.00017453527136951596,
      "loss": 0.6432,
      "step": 4779
    },
    {
      "epoch": 0.3824,
      "grad_norm": 0.4115530550479889,
      "learning_rate": 0.00017452993732497668,
      "loss": 1.2119,
      "step": 4780
    },
    {
      "epoch": 0.38248,
      "grad_norm": 0.39553916454315186,
      "learning_rate": 0.00017452460328043742,
      "loss": 0.8872,
      "step": 4781
    },
    {
      "epoch": 0.38256,
      "grad_norm": 0.35547807812690735,
      "learning_rate": 0.00017451926923589813,
      "loss": 0.9273,
      "step": 4782
    },
    {
      "epoch": 0.38264,
      "grad_norm": 0.3511357307434082,
      "learning_rate": 0.00017451393519135887,
      "loss": 0.953,
      "step": 4783
    },
    {
      "epoch": 0.38272,
      "grad_norm": 0.34211963415145874,
      "learning_rate": 0.00017450860114681958,
      "loss": 0.7056,
      "step": 4784
    },
    {
      "epoch": 0.3828,
      "grad_norm": 0.4606077969074249,
      "learning_rate": 0.00017450326710228032,
      "loss": 0.9101,
      "step": 4785
    },
    {
      "epoch": 0.38288,
      "grad_norm": 0.4652477204799652,
      "learning_rate": 0.00017449793305774104,
      "loss": 1.1204,
      "step": 4786
    },
    {
      "epoch": 0.38296,
      "grad_norm": 0.3098006248474121,
      "learning_rate": 0.00017449259901320178,
      "loss": 0.7693,
      "step": 4787
    },
    {
      "epoch": 0.38304,
      "grad_norm": 0.3571571111679077,
      "learning_rate": 0.0001744872649686625,
      "loss": 0.6767,
      "step": 4788
    },
    {
      "epoch": 0.38312,
      "grad_norm": 0.42477917671203613,
      "learning_rate": 0.00017448193092412323,
      "loss": 1.1032,
      "step": 4789
    },
    {
      "epoch": 0.3832,
      "grad_norm": 0.31193631887435913,
      "learning_rate": 0.00017447659687958394,
      "loss": 0.7371,
      "step": 4790
    },
    {
      "epoch": 0.38328,
      "grad_norm": 0.48188188672065735,
      "learning_rate": 0.00017447126283504468,
      "loss": 0.8732,
      "step": 4791
    },
    {
      "epoch": 0.38336,
      "grad_norm": 0.4251278042793274,
      "learning_rate": 0.0001744659287905054,
      "loss": 0.6174,
      "step": 4792
    },
    {
      "epoch": 0.38344,
      "grad_norm": 0.382494181394577,
      "learning_rate": 0.00017446059474596613,
      "loss": 0.6776,
      "step": 4793
    },
    {
      "epoch": 0.38352,
      "grad_norm": 0.41118255257606506,
      "learning_rate": 0.00017445526070142685,
      "loss": 0.8042,
      "step": 4794
    },
    {
      "epoch": 0.3836,
      "grad_norm": 0.4091240465641022,
      "learning_rate": 0.00017444992665688759,
      "loss": 0.7203,
      "step": 4795
    },
    {
      "epoch": 0.38368,
      "grad_norm": 0.43101540207862854,
      "learning_rate": 0.00017444459261234833,
      "loss": 0.5752,
      "step": 4796
    },
    {
      "epoch": 0.38376,
      "grad_norm": 0.39828917384147644,
      "learning_rate": 0.00017443925856780904,
      "loss": 0.5017,
      "step": 4797
    },
    {
      "epoch": 0.38384,
      "grad_norm": 0.3835071325302124,
      "learning_rate": 0.00017443392452326978,
      "loss": 0.6906,
      "step": 4798
    },
    {
      "epoch": 0.38392,
      "grad_norm": 0.3773747682571411,
      "learning_rate": 0.0001744285904787305,
      "loss": 0.909,
      "step": 4799
    },
    {
      "epoch": 0.384,
      "grad_norm": 0.33309298753738403,
      "learning_rate": 0.00017442325643419123,
      "loss": 0.9713,
      "step": 4800
    },
    {
      "epoch": 0.38408,
      "grad_norm": 0.38829901814460754,
      "learning_rate": 0.00017441792238965194,
      "loss": 0.8398,
      "step": 4801
    },
    {
      "epoch": 0.38416,
      "grad_norm": 0.4044899046421051,
      "learning_rate": 0.00017441258834511268,
      "loss": 0.7287,
      "step": 4802
    },
    {
      "epoch": 0.38424,
      "grad_norm": 0.4663568139076233,
      "learning_rate": 0.0001744072543005734,
      "loss": 0.7646,
      "step": 4803
    },
    {
      "epoch": 0.38432,
      "grad_norm": 0.4713459014892578,
      "learning_rate": 0.00017440192025603414,
      "loss": 1.0238,
      "step": 4804
    },
    {
      "epoch": 0.3844,
      "grad_norm": 0.29373371601104736,
      "learning_rate": 0.00017439658621149488,
      "loss": 0.4995,
      "step": 4805
    },
    {
      "epoch": 0.38448,
      "grad_norm": 0.4168400466442108,
      "learning_rate": 0.0001743912521669556,
      "loss": 0.6683,
      "step": 4806
    },
    {
      "epoch": 0.38456,
      "grad_norm": 0.26765578985214233,
      "learning_rate": 0.00017438591812241633,
      "loss": 0.7353,
      "step": 4807
    },
    {
      "epoch": 0.38464,
      "grad_norm": 0.30949416756629944,
      "learning_rate": 0.00017438058407787704,
      "loss": 0.8116,
      "step": 4808
    },
    {
      "epoch": 0.38472,
      "grad_norm": 0.446544885635376,
      "learning_rate": 0.00017437525003333778,
      "loss": 0.9384,
      "step": 4809
    },
    {
      "epoch": 0.3848,
      "grad_norm": 0.3121793866157532,
      "learning_rate": 0.0001743699159887985,
      "loss": 0.7598,
      "step": 4810
    },
    {
      "epoch": 0.38488,
      "grad_norm": 0.44014111161231995,
      "learning_rate": 0.00017436458194425923,
      "loss": 0.742,
      "step": 4811
    },
    {
      "epoch": 0.38496,
      "grad_norm": 0.39545443654060364,
      "learning_rate": 0.00017435924789971997,
      "loss": 1.1063,
      "step": 4812
    },
    {
      "epoch": 0.38504,
      "grad_norm": 0.4115837812423706,
      "learning_rate": 0.00017435391385518069,
      "loss": 0.9882,
      "step": 4813
    },
    {
      "epoch": 0.38512,
      "grad_norm": 0.3967396914958954,
      "learning_rate": 0.00017434857981064143,
      "loss": 1.1876,
      "step": 4814
    },
    {
      "epoch": 0.3852,
      "grad_norm": 0.3795890808105469,
      "learning_rate": 0.00017434324576610214,
      "loss": 0.6942,
      "step": 4815
    },
    {
      "epoch": 0.38528,
      "grad_norm": 0.39033859968185425,
      "learning_rate": 0.00017433791172156288,
      "loss": 1.0359,
      "step": 4816
    },
    {
      "epoch": 0.38536,
      "grad_norm": 0.3706974387168884,
      "learning_rate": 0.0001743325776770236,
      "loss": 1.1302,
      "step": 4817
    },
    {
      "epoch": 0.38544,
      "grad_norm": 0.3652261197566986,
      "learning_rate": 0.00017432724363248433,
      "loss": 0.9948,
      "step": 4818
    },
    {
      "epoch": 0.38552,
      "grad_norm": 0.359131395816803,
      "learning_rate": 0.00017432190958794507,
      "loss": 0.8016,
      "step": 4819
    },
    {
      "epoch": 0.3856,
      "grad_norm": 0.3980560600757599,
      "learning_rate": 0.00017431657554340578,
      "loss": 0.9336,
      "step": 4820
    },
    {
      "epoch": 0.38568,
      "grad_norm": 0.3860676884651184,
      "learning_rate": 0.00017431124149886652,
      "loss": 1.0199,
      "step": 4821
    },
    {
      "epoch": 0.38576,
      "grad_norm": 0.33765658736228943,
      "learning_rate": 0.00017430590745432724,
      "loss": 0.9138,
      "step": 4822
    },
    {
      "epoch": 0.38584,
      "grad_norm": 0.5006104707717896,
      "learning_rate": 0.00017430057340978798,
      "loss": 0.7903,
      "step": 4823
    },
    {
      "epoch": 0.38592,
      "grad_norm": 0.3692794144153595,
      "learning_rate": 0.00017429523936524872,
      "loss": 0.933,
      "step": 4824
    },
    {
      "epoch": 0.386,
      "grad_norm": 0.29791945219039917,
      "learning_rate": 0.00017428990532070943,
      "loss": 0.6274,
      "step": 4825
    },
    {
      "epoch": 0.38608,
      "grad_norm": 0.36664849519729614,
      "learning_rate": 0.00017428457127617017,
      "loss": 0.8341,
      "step": 4826
    },
    {
      "epoch": 0.38616,
      "grad_norm": 0.3585377633571625,
      "learning_rate": 0.00017427923723163088,
      "loss": 0.8371,
      "step": 4827
    },
    {
      "epoch": 0.38624,
      "grad_norm": 0.3162555992603302,
      "learning_rate": 0.00017427390318709162,
      "loss": 0.6181,
      "step": 4828
    },
    {
      "epoch": 0.38632,
      "grad_norm": 0.4064483940601349,
      "learning_rate": 0.00017426856914255233,
      "loss": 0.6464,
      "step": 4829
    },
    {
      "epoch": 0.3864,
      "grad_norm": 0.3794451653957367,
      "learning_rate": 0.00017426323509801307,
      "loss": 0.9011,
      "step": 4830
    },
    {
      "epoch": 0.38648,
      "grad_norm": 0.35777899622917175,
      "learning_rate": 0.0001742579010534738,
      "loss": 0.9998,
      "step": 4831
    },
    {
      "epoch": 0.38656,
      "grad_norm": 0.37569648027420044,
      "learning_rate": 0.00017425256700893453,
      "loss": 0.8676,
      "step": 4832
    },
    {
      "epoch": 0.38664,
      "grad_norm": 0.34913378953933716,
      "learning_rate": 0.00017424723296439527,
      "loss": 1.0381,
      "step": 4833
    },
    {
      "epoch": 0.38672,
      "grad_norm": 0.3927876353263855,
      "learning_rate": 0.00017424189891985598,
      "loss": 0.916,
      "step": 4834
    },
    {
      "epoch": 0.3868,
      "grad_norm": 0.35517358779907227,
      "learning_rate": 0.00017423656487531672,
      "loss": 0.7382,
      "step": 4835
    },
    {
      "epoch": 0.38688,
      "grad_norm": 0.3298077881336212,
      "learning_rate": 0.00017423123083077743,
      "loss": 0.4561,
      "step": 4836
    },
    {
      "epoch": 0.38696,
      "grad_norm": 0.3313029408454895,
      "learning_rate": 0.00017422589678623817,
      "loss": 0.8903,
      "step": 4837
    },
    {
      "epoch": 0.38704,
      "grad_norm": 0.2956591844558716,
      "learning_rate": 0.0001742205627416989,
      "loss": 0.6622,
      "step": 4838
    },
    {
      "epoch": 0.38712,
      "grad_norm": 0.40450170636177063,
      "learning_rate": 0.00017421522869715962,
      "loss": 0.777,
      "step": 4839
    },
    {
      "epoch": 0.3872,
      "grad_norm": 0.3449845612049103,
      "learning_rate": 0.00017420989465262036,
      "loss": 0.6299,
      "step": 4840
    },
    {
      "epoch": 0.38728,
      "grad_norm": 0.35249119997024536,
      "learning_rate": 0.00017420456060808108,
      "loss": 0.7947,
      "step": 4841
    },
    {
      "epoch": 0.38736,
      "grad_norm": 0.3790217339992523,
      "learning_rate": 0.00017419922656354182,
      "loss": 0.8462,
      "step": 4842
    },
    {
      "epoch": 0.38744,
      "grad_norm": 0.40669023990631104,
      "learning_rate": 0.00017419389251900253,
      "loss": 0.7233,
      "step": 4843
    },
    {
      "epoch": 0.38752,
      "grad_norm": 0.43126893043518066,
      "learning_rate": 0.00017418855847446327,
      "loss": 1.0133,
      "step": 4844
    },
    {
      "epoch": 0.3876,
      "grad_norm": 0.5106350779533386,
      "learning_rate": 0.000174183224429924,
      "loss": 1.1768,
      "step": 4845
    },
    {
      "epoch": 0.38768,
      "grad_norm": 0.3660135269165039,
      "learning_rate": 0.00017417789038538472,
      "loss": 0.6206,
      "step": 4846
    },
    {
      "epoch": 0.38776,
      "grad_norm": 0.31647545099258423,
      "learning_rate": 0.00017417255634084546,
      "loss": 0.9267,
      "step": 4847
    },
    {
      "epoch": 0.38784,
      "grad_norm": 0.3942817747592926,
      "learning_rate": 0.00017416722229630617,
      "loss": 0.7397,
      "step": 4848
    },
    {
      "epoch": 0.38792,
      "grad_norm": 0.4375053942203522,
      "learning_rate": 0.00017416188825176691,
      "loss": 0.7443,
      "step": 4849
    },
    {
      "epoch": 0.388,
      "grad_norm": 0.28832194209098816,
      "learning_rate": 0.00017415655420722763,
      "loss": 1.1616,
      "step": 4850
    },
    {
      "epoch": 0.38808,
      "grad_norm": 0.3644852042198181,
      "learning_rate": 0.00017415122016268837,
      "loss": 0.5792,
      "step": 4851
    },
    {
      "epoch": 0.38816,
      "grad_norm": 0.43874725699424744,
      "learning_rate": 0.0001741458861181491,
      "loss": 0.8042,
      "step": 4852
    },
    {
      "epoch": 0.38824,
      "grad_norm": 0.3198116719722748,
      "learning_rate": 0.00017414055207360982,
      "loss": 0.8682,
      "step": 4853
    },
    {
      "epoch": 0.38832,
      "grad_norm": 0.38145869970321655,
      "learning_rate": 0.00017413521802907056,
      "loss": 0.7846,
      "step": 4854
    },
    {
      "epoch": 0.3884,
      "grad_norm": 0.26250705122947693,
      "learning_rate": 0.00017412988398453127,
      "loss": 0.4629,
      "step": 4855
    },
    {
      "epoch": 0.38848,
      "grad_norm": 0.3495771586894989,
      "learning_rate": 0.000174124549939992,
      "loss": 0.6858,
      "step": 4856
    },
    {
      "epoch": 0.38856,
      "grad_norm": 0.3435360789299011,
      "learning_rate": 0.00017411921589545272,
      "loss": 0.8636,
      "step": 4857
    },
    {
      "epoch": 0.38864,
      "grad_norm": 0.46655887365341187,
      "learning_rate": 0.00017411388185091346,
      "loss": 0.9106,
      "step": 4858
    },
    {
      "epoch": 0.38872,
      "grad_norm": 0.4234694242477417,
      "learning_rate": 0.0001741085478063742,
      "loss": 1.076,
      "step": 4859
    },
    {
      "epoch": 0.3888,
      "grad_norm": 0.41994139552116394,
      "learning_rate": 0.00017410321376183492,
      "loss": 0.8338,
      "step": 4860
    },
    {
      "epoch": 0.38888,
      "grad_norm": 0.3455306589603424,
      "learning_rate": 0.00017409787971729566,
      "loss": 0.8842,
      "step": 4861
    },
    {
      "epoch": 0.38896,
      "grad_norm": 0.3312686085700989,
      "learning_rate": 0.00017409254567275637,
      "loss": 0.8444,
      "step": 4862
    },
    {
      "epoch": 0.38904,
      "grad_norm": 0.48168933391571045,
      "learning_rate": 0.0001740872116282171,
      "loss": 0.7589,
      "step": 4863
    },
    {
      "epoch": 0.38912,
      "grad_norm": 0.3436749577522278,
      "learning_rate": 0.00017408187758367782,
      "loss": 0.7506,
      "step": 4864
    },
    {
      "epoch": 0.3892,
      "grad_norm": 0.3132477402687073,
      "learning_rate": 0.00017407654353913856,
      "loss": 0.6412,
      "step": 4865
    },
    {
      "epoch": 0.38928,
      "grad_norm": 0.3797855079174042,
      "learning_rate": 0.0001740712094945993,
      "loss": 0.7679,
      "step": 4866
    },
    {
      "epoch": 0.38936,
      "grad_norm": 0.3214038610458374,
      "learning_rate": 0.00017406587545006001,
      "loss": 0.9836,
      "step": 4867
    },
    {
      "epoch": 0.38944,
      "grad_norm": 0.3711637258529663,
      "learning_rate": 0.00017406054140552075,
      "loss": 0.6222,
      "step": 4868
    },
    {
      "epoch": 0.38952,
      "grad_norm": 0.35904088616371155,
      "learning_rate": 0.00017405520736098147,
      "loss": 0.7627,
      "step": 4869
    },
    {
      "epoch": 0.3896,
      "grad_norm": 0.42937397956848145,
      "learning_rate": 0.0001740498733164422,
      "loss": 0.8599,
      "step": 4870
    },
    {
      "epoch": 0.38968,
      "grad_norm": 0.3664829432964325,
      "learning_rate": 0.00017404453927190292,
      "loss": 0.4267,
      "step": 4871
    },
    {
      "epoch": 0.38976,
      "grad_norm": 0.36077848076820374,
      "learning_rate": 0.00017403920522736366,
      "loss": 0.7581,
      "step": 4872
    },
    {
      "epoch": 0.38984,
      "grad_norm": 0.5513958930969238,
      "learning_rate": 0.0001740338711828244,
      "loss": 0.8519,
      "step": 4873
    },
    {
      "epoch": 0.38992,
      "grad_norm": 0.35916203260421753,
      "learning_rate": 0.0001740285371382851,
      "loss": 1.1195,
      "step": 4874
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.42063629627227783,
      "learning_rate": 0.00017402320309374585,
      "loss": 0.6912,
      "step": 4875
    },
    {
      "epoch": 0.39008,
      "grad_norm": 0.3210727870464325,
      "learning_rate": 0.00017401786904920656,
      "loss": 0.6734,
      "step": 4876
    },
    {
      "epoch": 0.39016,
      "grad_norm": 0.3522937297821045,
      "learning_rate": 0.0001740125350046673,
      "loss": 0.5693,
      "step": 4877
    },
    {
      "epoch": 0.39024,
      "grad_norm": 0.3187103569507599,
      "learning_rate": 0.00017400720096012804,
      "loss": 0.9201,
      "step": 4878
    },
    {
      "epoch": 0.39032,
      "grad_norm": 0.2624568045139313,
      "learning_rate": 0.00017400186691558876,
      "loss": 0.3845,
      "step": 4879
    },
    {
      "epoch": 0.3904,
      "grad_norm": 0.40583163499832153,
      "learning_rate": 0.0001739965328710495,
      "loss": 0.9099,
      "step": 4880
    },
    {
      "epoch": 0.39048,
      "grad_norm": 0.3000205457210541,
      "learning_rate": 0.0001739911988265102,
      "loss": 0.7098,
      "step": 4881
    },
    {
      "epoch": 0.39056,
      "grad_norm": 0.41046255826950073,
      "learning_rate": 0.00017398586478197095,
      "loss": 0.9573,
      "step": 4882
    },
    {
      "epoch": 0.39064,
      "grad_norm": 0.4432033896446228,
      "learning_rate": 0.00017398053073743166,
      "loss": 0.9977,
      "step": 4883
    },
    {
      "epoch": 0.39072,
      "grad_norm": 0.33198198676109314,
      "learning_rate": 0.0001739751966928924,
      "loss": 0.6041,
      "step": 4884
    },
    {
      "epoch": 0.3908,
      "grad_norm": 0.4627629220485687,
      "learning_rate": 0.00017396986264835314,
      "loss": 0.9003,
      "step": 4885
    },
    {
      "epoch": 0.39088,
      "grad_norm": 0.3952891230583191,
      "learning_rate": 0.00017396452860381385,
      "loss": 0.6625,
      "step": 4886
    },
    {
      "epoch": 0.39096,
      "grad_norm": 0.4304913282394409,
      "learning_rate": 0.0001739591945592746,
      "loss": 0.7521,
      "step": 4887
    },
    {
      "epoch": 0.39104,
      "grad_norm": 0.40328481793403625,
      "learning_rate": 0.0001739538605147353,
      "loss": 0.8455,
      "step": 4888
    },
    {
      "epoch": 0.39112,
      "grad_norm": 0.3931845426559448,
      "learning_rate": 0.00017394852647019605,
      "loss": 0.9644,
      "step": 4889
    },
    {
      "epoch": 0.3912,
      "grad_norm": 0.30183783173561096,
      "learning_rate": 0.00017394319242565676,
      "loss": 0.8725,
      "step": 4890
    },
    {
      "epoch": 0.39128,
      "grad_norm": 0.37242257595062256,
      "learning_rate": 0.0001739378583811175,
      "loss": 0.6616,
      "step": 4891
    },
    {
      "epoch": 0.39136,
      "grad_norm": 0.41841644048690796,
      "learning_rate": 0.00017393252433657824,
      "loss": 0.9962,
      "step": 4892
    },
    {
      "epoch": 0.39144,
      "grad_norm": 0.40680480003356934,
      "learning_rate": 0.00017392719029203895,
      "loss": 0.9712,
      "step": 4893
    },
    {
      "epoch": 0.39152,
      "grad_norm": 0.36976855993270874,
      "learning_rate": 0.0001739218562474997,
      "loss": 0.7094,
      "step": 4894
    },
    {
      "epoch": 0.3916,
      "grad_norm": 0.44651350378990173,
      "learning_rate": 0.0001739165222029604,
      "loss": 0.991,
      "step": 4895
    },
    {
      "epoch": 0.39168,
      "grad_norm": 0.5490401387214661,
      "learning_rate": 0.00017391118815842114,
      "loss": 0.749,
      "step": 4896
    },
    {
      "epoch": 0.39176,
      "grad_norm": 0.38697025179862976,
      "learning_rate": 0.00017390585411388186,
      "loss": 0.757,
      "step": 4897
    },
    {
      "epoch": 0.39184,
      "grad_norm": 0.4014015793800354,
      "learning_rate": 0.0001739005200693426,
      "loss": 0.8846,
      "step": 4898
    },
    {
      "epoch": 0.39192,
      "grad_norm": 0.39182162284851074,
      "learning_rate": 0.00017389518602480334,
      "loss": 0.8389,
      "step": 4899
    },
    {
      "epoch": 0.392,
      "grad_norm": 0.33473265171051025,
      "learning_rate": 0.00017388985198026405,
      "loss": 0.5659,
      "step": 4900
    },
    {
      "epoch": 0.39208,
      "grad_norm": 0.41985803842544556,
      "learning_rate": 0.0001738845179357248,
      "loss": 0.7826,
      "step": 4901
    },
    {
      "epoch": 0.39216,
      "grad_norm": 0.471160352230072,
      "learning_rate": 0.0001738791838911855,
      "loss": 0.8725,
      "step": 4902
    },
    {
      "epoch": 0.39224,
      "grad_norm": 0.4001469016075134,
      "learning_rate": 0.00017387384984664624,
      "loss": 0.8181,
      "step": 4903
    },
    {
      "epoch": 0.39232,
      "grad_norm": 0.366171270608902,
      "learning_rate": 0.00017386851580210696,
      "loss": 0.7932,
      "step": 4904
    },
    {
      "epoch": 0.3924,
      "grad_norm": 0.29390692710876465,
      "learning_rate": 0.0001738631817575677,
      "loss": 1.0712,
      "step": 4905
    },
    {
      "epoch": 0.39248,
      "grad_norm": 0.38975825905799866,
      "learning_rate": 0.0001738578477130284,
      "loss": 0.7235,
      "step": 4906
    },
    {
      "epoch": 0.39256,
      "grad_norm": 0.4299238920211792,
      "learning_rate": 0.00017385251366848915,
      "loss": 1.0726,
      "step": 4907
    },
    {
      "epoch": 0.39264,
      "grad_norm": 0.39818504452705383,
      "learning_rate": 0.0001738471796239499,
      "loss": 0.7744,
      "step": 4908
    },
    {
      "epoch": 0.39272,
      "grad_norm": 0.4014713168144226,
      "learning_rate": 0.0001738418455794106,
      "loss": 0.6663,
      "step": 4909
    },
    {
      "epoch": 0.3928,
      "grad_norm": 0.37798869609832764,
      "learning_rate": 0.00017383651153487134,
      "loss": 0.8299,
      "step": 4910
    },
    {
      "epoch": 0.39288,
      "grad_norm": 0.31393855810165405,
      "learning_rate": 0.00017383117749033205,
      "loss": 0.7407,
      "step": 4911
    },
    {
      "epoch": 0.39296,
      "grad_norm": 0.3106299638748169,
      "learning_rate": 0.0001738258434457928,
      "loss": 1.071,
      "step": 4912
    },
    {
      "epoch": 0.39304,
      "grad_norm": 0.33277395367622375,
      "learning_rate": 0.0001738205094012535,
      "loss": 0.8962,
      "step": 4913
    },
    {
      "epoch": 0.39312,
      "grad_norm": 0.30343636870384216,
      "learning_rate": 0.00017381517535671425,
      "loss": 0.7304,
      "step": 4914
    },
    {
      "epoch": 0.3932,
      "grad_norm": 0.3608342409133911,
      "learning_rate": 0.00017380984131217496,
      "loss": 0.6979,
      "step": 4915
    },
    {
      "epoch": 0.39328,
      "grad_norm": 0.3237588107585907,
      "learning_rate": 0.0001738045072676357,
      "loss": 0.5788,
      "step": 4916
    },
    {
      "epoch": 0.39336,
      "grad_norm": 0.3599993884563446,
      "learning_rate": 0.0001737991732230964,
      "loss": 0.5029,
      "step": 4917
    },
    {
      "epoch": 0.39344,
      "grad_norm": 0.30245649814605713,
      "learning_rate": 0.00017379383917855715,
      "loss": 0.7539,
      "step": 4918
    },
    {
      "epoch": 0.39352,
      "grad_norm": 0.38130834698677063,
      "learning_rate": 0.00017378850513401786,
      "loss": 0.7964,
      "step": 4919
    },
    {
      "epoch": 0.3936,
      "grad_norm": 0.33451569080352783,
      "learning_rate": 0.0001737831710894786,
      "loss": 1.3399,
      "step": 4920
    },
    {
      "epoch": 0.39368,
      "grad_norm": 0.3239588439464569,
      "learning_rate": 0.00017377783704493932,
      "loss": 0.7386,
      "step": 4921
    },
    {
      "epoch": 0.39376,
      "grad_norm": 0.44961804151535034,
      "learning_rate": 0.00017377250300040006,
      "loss": 0.8222,
      "step": 4922
    },
    {
      "epoch": 0.39384,
      "grad_norm": 0.4796714782714844,
      "learning_rate": 0.0001737671689558608,
      "loss": 1.4474,
      "step": 4923
    },
    {
      "epoch": 0.39392,
      "grad_norm": 0.4641675055027008,
      "learning_rate": 0.0001737618349113215,
      "loss": 0.9281,
      "step": 4924
    },
    {
      "epoch": 0.394,
      "grad_norm": 0.4922877550125122,
      "learning_rate": 0.00017375650086678225,
      "loss": 0.5975,
      "step": 4925
    },
    {
      "epoch": 0.39408,
      "grad_norm": 0.4938255250453949,
      "learning_rate": 0.00017375116682224296,
      "loss": 0.8864,
      "step": 4926
    },
    {
      "epoch": 0.39416,
      "grad_norm": 0.3900351822376251,
      "learning_rate": 0.0001737458327777037,
      "loss": 0.8594,
      "step": 4927
    },
    {
      "epoch": 0.39424,
      "grad_norm": 0.5873403549194336,
      "learning_rate": 0.0001737404987331644,
      "loss": 1.138,
      "step": 4928
    },
    {
      "epoch": 0.39432,
      "grad_norm": 0.4081473648548126,
      "learning_rate": 0.00017373516468862515,
      "loss": 0.7483,
      "step": 4929
    },
    {
      "epoch": 0.3944,
      "grad_norm": 0.3198063373565674,
      "learning_rate": 0.00017372983064408587,
      "loss": 0.654,
      "step": 4930
    },
    {
      "epoch": 0.39448,
      "grad_norm": 0.4037259519100189,
      "learning_rate": 0.0001737244965995466,
      "loss": 0.7254,
      "step": 4931
    },
    {
      "epoch": 0.39456,
      "grad_norm": 0.40505924820899963,
      "learning_rate": 0.00017371916255500735,
      "loss": 0.6296,
      "step": 4932
    },
    {
      "epoch": 0.39464,
      "grad_norm": 0.5438205003738403,
      "learning_rate": 0.00017371382851046806,
      "loss": 1.0105,
      "step": 4933
    },
    {
      "epoch": 0.39472,
      "grad_norm": 0.40538185834884644,
      "learning_rate": 0.0001737084944659288,
      "loss": 0.8174,
      "step": 4934
    },
    {
      "epoch": 0.3948,
      "grad_norm": 0.29798731207847595,
      "learning_rate": 0.0001737031604213895,
      "loss": 1.0557,
      "step": 4935
    },
    {
      "epoch": 0.39488,
      "grad_norm": 0.3576415181159973,
      "learning_rate": 0.00017369782637685025,
      "loss": 0.8116,
      "step": 4936
    },
    {
      "epoch": 0.39496,
      "grad_norm": 0.4297386407852173,
      "learning_rate": 0.00017369249233231096,
      "loss": 0.7983,
      "step": 4937
    },
    {
      "epoch": 0.39504,
      "grad_norm": 0.3308737576007843,
      "learning_rate": 0.0001736871582877717,
      "loss": 0.8131,
      "step": 4938
    },
    {
      "epoch": 0.39512,
      "grad_norm": 0.35185641050338745,
      "learning_rate": 0.00017368182424323244,
      "loss": 0.7453,
      "step": 4939
    },
    {
      "epoch": 0.3952,
      "grad_norm": 0.5189251899719238,
      "learning_rate": 0.00017367649019869316,
      "loss": 0.7753,
      "step": 4940
    },
    {
      "epoch": 0.39528,
      "grad_norm": 0.47227761149406433,
      "learning_rate": 0.0001736711561541539,
      "loss": 0.8095,
      "step": 4941
    },
    {
      "epoch": 0.39536,
      "grad_norm": 0.43360838294029236,
      "learning_rate": 0.0001736658221096146,
      "loss": 0.92,
      "step": 4942
    },
    {
      "epoch": 0.39544,
      "grad_norm": 0.3710939288139343,
      "learning_rate": 0.00017366048806507535,
      "loss": 0.7036,
      "step": 4943
    },
    {
      "epoch": 0.39552,
      "grad_norm": 0.4811662435531616,
      "learning_rate": 0.00017365515402053606,
      "loss": 0.8691,
      "step": 4944
    },
    {
      "epoch": 0.3956,
      "grad_norm": 0.3544960916042328,
      "learning_rate": 0.0001736498199759968,
      "loss": 0.7683,
      "step": 4945
    },
    {
      "epoch": 0.39568,
      "grad_norm": 0.3382553160190582,
      "learning_rate": 0.00017364448593145754,
      "loss": 0.6244,
      "step": 4946
    },
    {
      "epoch": 0.39576,
      "grad_norm": 0.4369339942932129,
      "learning_rate": 0.00017363915188691825,
      "loss": 0.7204,
      "step": 4947
    },
    {
      "epoch": 0.39584,
      "grad_norm": 0.39602896571159363,
      "learning_rate": 0.000173633817842379,
      "loss": 0.9028,
      "step": 4948
    },
    {
      "epoch": 0.39592,
      "grad_norm": 0.42017608880996704,
      "learning_rate": 0.0001736284837978397,
      "loss": 0.9811,
      "step": 4949
    },
    {
      "epoch": 0.396,
      "grad_norm": 0.445487380027771,
      "learning_rate": 0.00017362314975330045,
      "loss": 0.9033,
      "step": 4950
    },
    {
      "epoch": 0.39608,
      "grad_norm": 0.3694496750831604,
      "learning_rate": 0.00017361781570876116,
      "loss": 0.7502,
      "step": 4951
    },
    {
      "epoch": 0.39616,
      "grad_norm": 0.3830738961696625,
      "learning_rate": 0.0001736124816642219,
      "loss": 0.6534,
      "step": 4952
    },
    {
      "epoch": 0.39624,
      "grad_norm": 0.3048141896724701,
      "learning_rate": 0.00017360714761968264,
      "loss": 0.5036,
      "step": 4953
    },
    {
      "epoch": 0.39632,
      "grad_norm": 0.5149901509284973,
      "learning_rate": 0.00017360181357514335,
      "loss": 0.6854,
      "step": 4954
    },
    {
      "epoch": 0.3964,
      "grad_norm": 0.40642303228378296,
      "learning_rate": 0.0001735964795306041,
      "loss": 0.9828,
      "step": 4955
    },
    {
      "epoch": 0.39648,
      "grad_norm": 0.26869821548461914,
      "learning_rate": 0.0001735911454860648,
      "loss": 0.4086,
      "step": 4956
    },
    {
      "epoch": 0.39656,
      "grad_norm": 0.3607490658760071,
      "learning_rate": 0.00017358581144152554,
      "loss": 0.6027,
      "step": 4957
    },
    {
      "epoch": 0.39664,
      "grad_norm": 0.3882293105125427,
      "learning_rate": 0.00017358047739698626,
      "loss": 0.7191,
      "step": 4958
    },
    {
      "epoch": 0.39672,
      "grad_norm": 0.2717534303665161,
      "learning_rate": 0.000173575143352447,
      "loss": 0.7935,
      "step": 4959
    },
    {
      "epoch": 0.3968,
      "grad_norm": 0.3738398849964142,
      "learning_rate": 0.00017356980930790774,
      "loss": 0.7699,
      "step": 4960
    },
    {
      "epoch": 0.39688,
      "grad_norm": 0.3206197917461395,
      "learning_rate": 0.00017356447526336845,
      "loss": 0.5675,
      "step": 4961
    },
    {
      "epoch": 0.39696,
      "grad_norm": 0.39551231265068054,
      "learning_rate": 0.0001735591412188292,
      "loss": 0.6871,
      "step": 4962
    },
    {
      "epoch": 0.39704,
      "grad_norm": 0.39382103085517883,
      "learning_rate": 0.0001735538071742899,
      "loss": 1.0591,
      "step": 4963
    },
    {
      "epoch": 0.39712,
      "grad_norm": 0.35914263129234314,
      "learning_rate": 0.00017354847312975064,
      "loss": 0.955,
      "step": 4964
    },
    {
      "epoch": 0.3972,
      "grad_norm": 0.4088948667049408,
      "learning_rate": 0.00017354313908521135,
      "loss": 0.8752,
      "step": 4965
    },
    {
      "epoch": 0.39728,
      "grad_norm": 0.4198160469532013,
      "learning_rate": 0.0001735378050406721,
      "loss": 0.997,
      "step": 4966
    },
    {
      "epoch": 0.39736,
      "grad_norm": 0.5247440338134766,
      "learning_rate": 0.00017353247099613283,
      "loss": 0.7632,
      "step": 4967
    },
    {
      "epoch": 0.39744,
      "grad_norm": 0.38010552525520325,
      "learning_rate": 0.00017352713695159355,
      "loss": 0.8271,
      "step": 4968
    },
    {
      "epoch": 0.39752,
      "grad_norm": 0.42932385206222534,
      "learning_rate": 0.00017352180290705429,
      "loss": 0.7663,
      "step": 4969
    },
    {
      "epoch": 0.3976,
      "grad_norm": 0.3834689259529114,
      "learning_rate": 0.000173516468862515,
      "loss": 1.0967,
      "step": 4970
    },
    {
      "epoch": 0.39768,
      "grad_norm": 0.4100133776664734,
      "learning_rate": 0.00017351113481797574,
      "loss": 0.8408,
      "step": 4971
    },
    {
      "epoch": 0.39776,
      "grad_norm": 0.34156689047813416,
      "learning_rate": 0.00017350580077343645,
      "loss": 1.0161,
      "step": 4972
    },
    {
      "epoch": 0.39784,
      "grad_norm": 0.3973350524902344,
      "learning_rate": 0.0001735004667288972,
      "loss": 0.6484,
      "step": 4973
    },
    {
      "epoch": 0.39792,
      "grad_norm": 0.3841746747493744,
      "learning_rate": 0.00017349513268435793,
      "loss": 0.9109,
      "step": 4974
    },
    {
      "epoch": 0.398,
      "grad_norm": 0.4168250858783722,
      "learning_rate": 0.00017348979863981864,
      "loss": 0.6738,
      "step": 4975
    },
    {
      "epoch": 0.39808,
      "grad_norm": 0.3222728669643402,
      "learning_rate": 0.00017348446459527938,
      "loss": 1.0068,
      "step": 4976
    },
    {
      "epoch": 0.39816,
      "grad_norm": 0.4008067846298218,
      "learning_rate": 0.0001734791305507401,
      "loss": 0.8323,
      "step": 4977
    },
    {
      "epoch": 0.39824,
      "grad_norm": 0.40554437041282654,
      "learning_rate": 0.00017347379650620084,
      "loss": 1.0351,
      "step": 4978
    },
    {
      "epoch": 0.39832,
      "grad_norm": 0.40347790718078613,
      "learning_rate": 0.00017346846246166158,
      "loss": 0.7263,
      "step": 4979
    },
    {
      "epoch": 0.3984,
      "grad_norm": 0.39889028668403625,
      "learning_rate": 0.0001734631284171223,
      "loss": 0.8549,
      "step": 4980
    },
    {
      "epoch": 0.39848,
      "grad_norm": 0.44184499979019165,
      "learning_rate": 0.00017345779437258303,
      "loss": 0.9976,
      "step": 4981
    },
    {
      "epoch": 0.39856,
      "grad_norm": 0.3788354694843292,
      "learning_rate": 0.00017345246032804374,
      "loss": 0.762,
      "step": 4982
    },
    {
      "epoch": 0.39864,
      "grad_norm": 0.3632429242134094,
      "learning_rate": 0.00017344712628350448,
      "loss": 0.6592,
      "step": 4983
    },
    {
      "epoch": 0.39872,
      "grad_norm": 0.42470481991767883,
      "learning_rate": 0.0001734417922389652,
      "loss": 0.6326,
      "step": 4984
    },
    {
      "epoch": 0.3988,
      "grad_norm": 0.3496183156967163,
      "learning_rate": 0.00017343645819442593,
      "loss": 1.0675,
      "step": 4985
    },
    {
      "epoch": 0.39888,
      "grad_norm": 0.31247106194496155,
      "learning_rate": 0.00017343112414988667,
      "loss": 0.9882,
      "step": 4986
    },
    {
      "epoch": 0.39896,
      "grad_norm": 0.46405839920043945,
      "learning_rate": 0.0001734257901053474,
      "loss": 0.8461,
      "step": 4987
    },
    {
      "epoch": 0.39904,
      "grad_norm": 0.4374478757381439,
      "learning_rate": 0.00017342045606080813,
      "loss": 0.8268,
      "step": 4988
    },
    {
      "epoch": 0.39912,
      "grad_norm": 0.3362429141998291,
      "learning_rate": 0.00017341512201626884,
      "loss": 1.1832,
      "step": 4989
    },
    {
      "epoch": 0.3992,
      "grad_norm": 0.3982652425765991,
      "learning_rate": 0.00017340978797172958,
      "loss": 0.7508,
      "step": 4990
    },
    {
      "epoch": 0.39928,
      "grad_norm": 0.40634778141975403,
      "learning_rate": 0.0001734044539271903,
      "loss": 1.0167,
      "step": 4991
    },
    {
      "epoch": 0.39936,
      "grad_norm": 0.4068259298801422,
      "learning_rate": 0.00017339911988265103,
      "loss": 0.9326,
      "step": 4992
    },
    {
      "epoch": 0.39944,
      "grad_norm": 0.4148286283016205,
      "learning_rate": 0.00017339378583811177,
      "loss": 0.8729,
      "step": 4993
    },
    {
      "epoch": 0.39952,
      "grad_norm": 0.4181981086730957,
      "learning_rate": 0.00017338845179357248,
      "loss": 0.7355,
      "step": 4994
    },
    {
      "epoch": 0.3996,
      "grad_norm": 0.4932575225830078,
      "learning_rate": 0.00017338311774903322,
      "loss": 0.8288,
      "step": 4995
    },
    {
      "epoch": 0.39968,
      "grad_norm": 0.38480842113494873,
      "learning_rate": 0.00017337778370449394,
      "loss": 0.6636,
      "step": 4996
    },
    {
      "epoch": 0.39976,
      "grad_norm": 0.422046422958374,
      "learning_rate": 0.00017337244965995468,
      "loss": 0.7255,
      "step": 4997
    },
    {
      "epoch": 0.39984,
      "grad_norm": 0.44478079676628113,
      "learning_rate": 0.0001733671156154154,
      "loss": 0.7947,
      "step": 4998
    },
    {
      "epoch": 0.39992,
      "grad_norm": 0.35176748037338257,
      "learning_rate": 0.00017336178157087613,
      "loss": 0.6842,
      "step": 4999
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.30734625458717346,
      "learning_rate": 0.00017335644752633687,
      "loss": 0.9811,
      "step": 5000
    },
    {
      "epoch": 0.40008,
      "grad_norm": 0.42232510447502136,
      "learning_rate": 0.00017335111348179758,
      "loss": 0.7335,
      "step": 5001
    },
    {
      "epoch": 0.40016,
      "grad_norm": 0.3825691342353821,
      "learning_rate": 0.00017334577943725832,
      "loss": 0.6309,
      "step": 5002
    },
    {
      "epoch": 0.40024,
      "grad_norm": 0.3891444206237793,
      "learning_rate": 0.00017334044539271903,
      "loss": 0.7393,
      "step": 5003
    },
    {
      "epoch": 0.40032,
      "grad_norm": 0.4607672393321991,
      "learning_rate": 0.00017333511134817977,
      "loss": 0.997,
      "step": 5004
    },
    {
      "epoch": 0.4004,
      "grad_norm": 0.3811265528202057,
      "learning_rate": 0.0001733297773036405,
      "loss": 0.7247,
      "step": 5005
    },
    {
      "epoch": 0.40048,
      "grad_norm": 0.45882293581962585,
      "learning_rate": 0.00017332444325910123,
      "loss": 0.9922,
      "step": 5006
    },
    {
      "epoch": 0.40056,
      "grad_norm": 0.3389635384082794,
      "learning_rate": 0.00017331910921456197,
      "loss": 0.8783,
      "step": 5007
    },
    {
      "epoch": 0.40064,
      "grad_norm": 0.3197241723537445,
      "learning_rate": 0.00017331377517002268,
      "loss": 0.8175,
      "step": 5008
    },
    {
      "epoch": 0.40072,
      "grad_norm": 0.47498008608818054,
      "learning_rate": 0.00017330844112548342,
      "loss": 0.7883,
      "step": 5009
    },
    {
      "epoch": 0.4008,
      "grad_norm": 0.527726948261261,
      "learning_rate": 0.00017330310708094413,
      "loss": 1.1019,
      "step": 5010
    },
    {
      "epoch": 0.40088,
      "grad_norm": 0.40811046957969666,
      "learning_rate": 0.00017329777303640487,
      "loss": 0.749,
      "step": 5011
    },
    {
      "epoch": 0.40096,
      "grad_norm": 0.28107619285583496,
      "learning_rate": 0.00017329243899186558,
      "loss": 0.5685,
      "step": 5012
    },
    {
      "epoch": 0.40104,
      "grad_norm": 0.4379882514476776,
      "learning_rate": 0.00017328710494732632,
      "loss": 1.0125,
      "step": 5013
    },
    {
      "epoch": 0.40112,
      "grad_norm": 0.38256174325942993,
      "learning_rate": 0.00017328177090278706,
      "loss": 0.6486,
      "step": 5014
    },
    {
      "epoch": 0.4012,
      "grad_norm": 0.34534209966659546,
      "learning_rate": 0.00017327643685824778,
      "loss": 0.9296,
      "step": 5015
    },
    {
      "epoch": 0.40128,
      "grad_norm": 0.3236074447631836,
      "learning_rate": 0.00017327110281370852,
      "loss": 0.9632,
      "step": 5016
    },
    {
      "epoch": 0.40136,
      "grad_norm": 0.4791642129421234,
      "learning_rate": 0.00017326576876916923,
      "loss": 0.7388,
      "step": 5017
    },
    {
      "epoch": 0.40144,
      "grad_norm": 0.37307631969451904,
      "learning_rate": 0.00017326043472462997,
      "loss": 1.1817,
      "step": 5018
    },
    {
      "epoch": 0.40152,
      "grad_norm": 0.3670262396335602,
      "learning_rate": 0.00017325510068009068,
      "loss": 0.5997,
      "step": 5019
    },
    {
      "epoch": 0.4016,
      "grad_norm": 0.4698672592639923,
      "learning_rate": 0.00017324976663555142,
      "loss": 0.8146,
      "step": 5020
    },
    {
      "epoch": 0.40168,
      "grad_norm": 0.3213866949081421,
      "learning_rate": 0.00017324443259101216,
      "loss": 0.7068,
      "step": 5021
    },
    {
      "epoch": 0.40176,
      "grad_norm": 0.3903267979621887,
      "learning_rate": 0.00017323909854647287,
      "loss": 1.1224,
      "step": 5022
    },
    {
      "epoch": 0.40184,
      "grad_norm": 0.332104355096817,
      "learning_rate": 0.00017323376450193361,
      "loss": 0.6814,
      "step": 5023
    },
    {
      "epoch": 0.40192,
      "grad_norm": 0.35854098200798035,
      "learning_rate": 0.00017322843045739433,
      "loss": 0.992,
      "step": 5024
    },
    {
      "epoch": 0.402,
      "grad_norm": 0.38515689969062805,
      "learning_rate": 0.00017322309641285507,
      "loss": 0.7465,
      "step": 5025
    },
    {
      "epoch": 0.40208,
      "grad_norm": 0.3499232530593872,
      "learning_rate": 0.0001732177623683158,
      "loss": 0.8504,
      "step": 5026
    },
    {
      "epoch": 0.40216,
      "grad_norm": 0.4539937973022461,
      "learning_rate": 0.00017321242832377652,
      "loss": 1.1079,
      "step": 5027
    },
    {
      "epoch": 0.40224,
      "grad_norm": 0.36693063378334045,
      "learning_rate": 0.00017320709427923726,
      "loss": 0.6703,
      "step": 5028
    },
    {
      "epoch": 0.40232,
      "grad_norm": 0.48692336678504944,
      "learning_rate": 0.00017320176023469797,
      "loss": 0.6058,
      "step": 5029
    },
    {
      "epoch": 0.4024,
      "grad_norm": 0.43251654505729675,
      "learning_rate": 0.0001731964261901587,
      "loss": 0.8818,
      "step": 5030
    },
    {
      "epoch": 0.40248,
      "grad_norm": 0.33608901500701904,
      "learning_rate": 0.00017319109214561942,
      "loss": 1.048,
      "step": 5031
    },
    {
      "epoch": 0.40256,
      "grad_norm": 0.47737154364585876,
      "learning_rate": 0.00017318575810108016,
      "loss": 0.6375,
      "step": 5032
    },
    {
      "epoch": 0.40264,
      "grad_norm": 0.3741039037704468,
      "learning_rate": 0.00017318042405654088,
      "loss": 0.9019,
      "step": 5033
    },
    {
      "epoch": 0.40272,
      "grad_norm": 0.4143213629722595,
      "learning_rate": 0.00017317509001200162,
      "loss": 1.1239,
      "step": 5034
    },
    {
      "epoch": 0.4028,
      "grad_norm": 0.37319663166999817,
      "learning_rate": 0.00017316975596746236,
      "loss": 0.8653,
      "step": 5035
    },
    {
      "epoch": 0.40288,
      "grad_norm": 0.583186686038971,
      "learning_rate": 0.00017316442192292307,
      "loss": 0.8198,
      "step": 5036
    },
    {
      "epoch": 0.40296,
      "grad_norm": 0.3400324583053589,
      "learning_rate": 0.0001731590878783838,
      "loss": 0.838,
      "step": 5037
    },
    {
      "epoch": 0.40304,
      "grad_norm": 0.4906749129295349,
      "learning_rate": 0.00017315375383384452,
      "loss": 1.1101,
      "step": 5038
    },
    {
      "epoch": 0.40312,
      "grad_norm": 0.41824719309806824,
      "learning_rate": 0.00017314841978930526,
      "loss": 0.7056,
      "step": 5039
    },
    {
      "epoch": 0.4032,
      "grad_norm": 0.4442153573036194,
      "learning_rate": 0.00017314308574476598,
      "loss": 0.6714,
      "step": 5040
    },
    {
      "epoch": 0.40328,
      "grad_norm": 0.3813701272010803,
      "learning_rate": 0.00017313775170022672,
      "loss": 1.1228,
      "step": 5041
    },
    {
      "epoch": 0.40336,
      "grad_norm": 0.4331877827644348,
      "learning_rate": 0.00017313241765568743,
      "loss": 0.9449,
      "step": 5042
    },
    {
      "epoch": 0.40344,
      "grad_norm": 0.3287075161933899,
      "learning_rate": 0.00017312708361114817,
      "loss": 0.6181,
      "step": 5043
    },
    {
      "epoch": 0.40352,
      "grad_norm": 0.3507404327392578,
      "learning_rate": 0.00017312174956660888,
      "loss": 0.9277,
      "step": 5044
    },
    {
      "epoch": 0.4036,
      "grad_norm": 0.32359060645103455,
      "learning_rate": 0.00017311641552206962,
      "loss": 0.832,
      "step": 5045
    },
    {
      "epoch": 0.40368,
      "grad_norm": 0.5395233035087585,
      "learning_rate": 0.00017311108147753033,
      "loss": 0.7249,
      "step": 5046
    },
    {
      "epoch": 0.40376,
      "grad_norm": 0.381327360868454,
      "learning_rate": 0.00017310574743299107,
      "loss": 1.0402,
      "step": 5047
    },
    {
      "epoch": 0.40384,
      "grad_norm": 0.43623706698417664,
      "learning_rate": 0.00017310041338845179,
      "loss": 0.8438,
      "step": 5048
    },
    {
      "epoch": 0.40392,
      "grad_norm": 0.4310707151889801,
      "learning_rate": 0.00017309507934391253,
      "loss": 0.6319,
      "step": 5049
    },
    {
      "epoch": 0.404,
      "grad_norm": 0.35835301876068115,
      "learning_rate": 0.00017308974529937324,
      "loss": 0.9095,
      "step": 5050
    },
    {
      "epoch": 0.40408,
      "grad_norm": 0.41497400403022766,
      "learning_rate": 0.00017308441125483398,
      "loss": 0.9169,
      "step": 5051
    },
    {
      "epoch": 0.40416,
      "grad_norm": 0.4376242458820343,
      "learning_rate": 0.00017307907721029472,
      "loss": 0.6765,
      "step": 5052
    },
    {
      "epoch": 0.40424,
      "grad_norm": 0.44340646266937256,
      "learning_rate": 0.00017307374316575543,
      "loss": 1.0026,
      "step": 5053
    },
    {
      "epoch": 0.40432,
      "grad_norm": 0.3887343108654022,
      "learning_rate": 0.00017306840912121617,
      "loss": 0.9033,
      "step": 5054
    },
    {
      "epoch": 0.4044,
      "grad_norm": 0.42281973361968994,
      "learning_rate": 0.00017306307507667688,
      "loss": 0.943,
      "step": 5055
    },
    {
      "epoch": 0.40448,
      "grad_norm": 0.650097668170929,
      "learning_rate": 0.00017305774103213762,
      "loss": 0.6426,
      "step": 5056
    },
    {
      "epoch": 0.40456,
      "grad_norm": 0.41863903403282166,
      "learning_rate": 0.00017305240698759834,
      "loss": 0.6631,
      "step": 5057
    },
    {
      "epoch": 0.40464,
      "grad_norm": 0.41033026576042175,
      "learning_rate": 0.00017304707294305908,
      "loss": 1.069,
      "step": 5058
    },
    {
      "epoch": 0.40472,
      "grad_norm": 0.4101766347885132,
      "learning_rate": 0.0001730417388985198,
      "loss": 0.9381,
      "step": 5059
    },
    {
      "epoch": 0.4048,
      "grad_norm": 0.43011248111724854,
      "learning_rate": 0.00017303640485398053,
      "loss": 0.7937,
      "step": 5060
    },
    {
      "epoch": 0.40488,
      "grad_norm": 0.5066043734550476,
      "learning_rate": 0.00017303107080944127,
      "loss": 1.0246,
      "step": 5061
    },
    {
      "epoch": 0.40496,
      "grad_norm": 0.3491501808166504,
      "learning_rate": 0.00017302573676490198,
      "loss": 0.9277,
      "step": 5062
    },
    {
      "epoch": 0.40504,
      "grad_norm": 0.45468559861183167,
      "learning_rate": 0.00017302040272036272,
      "loss": 0.9263,
      "step": 5063
    },
    {
      "epoch": 0.40512,
      "grad_norm": 0.39396607875823975,
      "learning_rate": 0.00017301506867582343,
      "loss": 1.0731,
      "step": 5064
    },
    {
      "epoch": 0.4052,
      "grad_norm": 0.384965181350708,
      "learning_rate": 0.00017300973463128417,
      "loss": 0.8116,
      "step": 5065
    },
    {
      "epoch": 0.40528,
      "grad_norm": 0.3115992248058319,
      "learning_rate": 0.00017300440058674489,
      "loss": 0.6152,
      "step": 5066
    },
    {
      "epoch": 0.40536,
      "grad_norm": 0.3290834426879883,
      "learning_rate": 0.00017299906654220563,
      "loss": 0.9039,
      "step": 5067
    },
    {
      "epoch": 0.40544,
      "grad_norm": 0.3169378638267517,
      "learning_rate": 0.00017299373249766637,
      "loss": 0.7465,
      "step": 5068
    },
    {
      "epoch": 0.40552,
      "grad_norm": 0.3092956840991974,
      "learning_rate": 0.00017298839845312708,
      "loss": 0.8595,
      "step": 5069
    },
    {
      "epoch": 0.4056,
      "grad_norm": 0.36681699752807617,
      "learning_rate": 0.00017298306440858782,
      "loss": 0.9051,
      "step": 5070
    },
    {
      "epoch": 0.40568,
      "grad_norm": 0.4337921440601349,
      "learning_rate": 0.00017297773036404853,
      "loss": 0.8779,
      "step": 5071
    },
    {
      "epoch": 0.40576,
      "grad_norm": 0.339521586894989,
      "learning_rate": 0.00017297239631950927,
      "loss": 0.7767,
      "step": 5072
    },
    {
      "epoch": 0.40584,
      "grad_norm": 0.33210137486457825,
      "learning_rate": 0.00017296706227497,
      "loss": 0.5964,
      "step": 5073
    },
    {
      "epoch": 0.40592,
      "grad_norm": 0.39877408742904663,
      "learning_rate": 0.00017296172823043072,
      "loss": 0.5688,
      "step": 5074
    },
    {
      "epoch": 0.406,
      "grad_norm": 0.33323973417282104,
      "learning_rate": 0.00017295639418589146,
      "loss": 0.8697,
      "step": 5075
    },
    {
      "epoch": 0.40608,
      "grad_norm": 0.40364184975624084,
      "learning_rate": 0.00017295106014135218,
      "loss": 0.5904,
      "step": 5076
    },
    {
      "epoch": 0.40616,
      "grad_norm": 0.4452289044857025,
      "learning_rate": 0.00017294572609681292,
      "loss": 1.0664,
      "step": 5077
    },
    {
      "epoch": 0.40624,
      "grad_norm": 0.3763377368450165,
      "learning_rate": 0.00017294039205227363,
      "loss": 0.7377,
      "step": 5078
    },
    {
      "epoch": 0.40632,
      "grad_norm": 0.34172970056533813,
      "learning_rate": 0.00017293505800773437,
      "loss": 0.8453,
      "step": 5079
    },
    {
      "epoch": 0.4064,
      "grad_norm": 0.49321943521499634,
      "learning_rate": 0.0001729297239631951,
      "loss": 0.7908,
      "step": 5080
    },
    {
      "epoch": 0.40648,
      "grad_norm": 0.3320203721523285,
      "learning_rate": 0.00017292438991865582,
      "loss": 0.6487,
      "step": 5081
    },
    {
      "epoch": 0.40656,
      "grad_norm": 0.39628279209136963,
      "learning_rate": 0.00017291905587411656,
      "loss": 0.7627,
      "step": 5082
    },
    {
      "epoch": 0.40664,
      "grad_norm": 0.37523773312568665,
      "learning_rate": 0.00017291372182957727,
      "loss": 0.6827,
      "step": 5083
    },
    {
      "epoch": 0.40672,
      "grad_norm": 0.3410053253173828,
      "learning_rate": 0.000172908387785038,
      "loss": 0.6312,
      "step": 5084
    },
    {
      "epoch": 0.4068,
      "grad_norm": 0.4818708598613739,
      "learning_rate": 0.00017290305374049873,
      "loss": 0.8024,
      "step": 5085
    },
    {
      "epoch": 0.40688,
      "grad_norm": 0.44075241684913635,
      "learning_rate": 0.00017289771969595947,
      "loss": 0.8008,
      "step": 5086
    },
    {
      "epoch": 0.40696,
      "grad_norm": 0.3131903409957886,
      "learning_rate": 0.0001728923856514202,
      "loss": 0.6644,
      "step": 5087
    },
    {
      "epoch": 0.40704,
      "grad_norm": 0.3824846148490906,
      "learning_rate": 0.00017288705160688092,
      "loss": 1.0529,
      "step": 5088
    },
    {
      "epoch": 0.40712,
      "grad_norm": 0.406246155500412,
      "learning_rate": 0.00017288171756234166,
      "loss": 0.7557,
      "step": 5089
    },
    {
      "epoch": 0.4072,
      "grad_norm": 0.3726663589477539,
      "learning_rate": 0.00017287638351780237,
      "loss": 0.8986,
      "step": 5090
    },
    {
      "epoch": 0.40728,
      "grad_norm": 0.3599150478839874,
      "learning_rate": 0.0001728710494732631,
      "loss": 0.635,
      "step": 5091
    },
    {
      "epoch": 0.40736,
      "grad_norm": 0.4185258448123932,
      "learning_rate": 0.00017286571542872382,
      "loss": 0.9768,
      "step": 5092
    },
    {
      "epoch": 0.40744,
      "grad_norm": 0.35846516489982605,
      "learning_rate": 0.00017286038138418456,
      "loss": 0.7,
      "step": 5093
    },
    {
      "epoch": 0.40752,
      "grad_norm": 0.3572273850440979,
      "learning_rate": 0.0001728550473396453,
      "loss": 0.6093,
      "step": 5094
    },
    {
      "epoch": 0.4076,
      "grad_norm": 0.5137220621109009,
      "learning_rate": 0.00017284971329510602,
      "loss": 1.2069,
      "step": 5095
    },
    {
      "epoch": 0.40768,
      "grad_norm": 0.4858686923980713,
      "learning_rate": 0.00017284437925056676,
      "loss": 1.016,
      "step": 5096
    },
    {
      "epoch": 0.40776,
      "grad_norm": 0.28445547819137573,
      "learning_rate": 0.00017283904520602747,
      "loss": 0.3625,
      "step": 5097
    },
    {
      "epoch": 0.40784,
      "grad_norm": 0.5262496471405029,
      "learning_rate": 0.0001728337111614882,
      "loss": 1.0284,
      "step": 5098
    },
    {
      "epoch": 0.40792,
      "grad_norm": 0.4672645926475525,
      "learning_rate": 0.00017282837711694892,
      "loss": 0.9316,
      "step": 5099
    },
    {
      "epoch": 0.408,
      "grad_norm": 0.35397276282310486,
      "learning_rate": 0.00017282304307240966,
      "loss": 0.5646,
      "step": 5100
    },
    {
      "epoch": 0.40808,
      "grad_norm": 0.351775586605072,
      "learning_rate": 0.0001728177090278704,
      "loss": 0.5731,
      "step": 5101
    },
    {
      "epoch": 0.40816,
      "grad_norm": 0.39703425765037537,
      "learning_rate": 0.00017281237498333111,
      "loss": 0.8581,
      "step": 5102
    },
    {
      "epoch": 0.40824,
      "grad_norm": 0.33789873123168945,
      "learning_rate": 0.00017280704093879185,
      "loss": 0.62,
      "step": 5103
    },
    {
      "epoch": 0.40832,
      "grad_norm": 0.38265883922576904,
      "learning_rate": 0.00017280170689425257,
      "loss": 0.8899,
      "step": 5104
    },
    {
      "epoch": 0.4084,
      "grad_norm": 0.3750900626182556,
      "learning_rate": 0.0001727963728497133,
      "loss": 0.9595,
      "step": 5105
    },
    {
      "epoch": 0.40848,
      "grad_norm": 0.4085179269313812,
      "learning_rate": 0.00017279103880517402,
      "loss": 0.6137,
      "step": 5106
    },
    {
      "epoch": 0.40856,
      "grad_norm": 0.39564281702041626,
      "learning_rate": 0.00017278570476063476,
      "loss": 0.7124,
      "step": 5107
    },
    {
      "epoch": 0.40864,
      "grad_norm": 0.4168053865432739,
      "learning_rate": 0.0001727803707160955,
      "loss": 0.898,
      "step": 5108
    },
    {
      "epoch": 0.40872,
      "grad_norm": 0.4468442499637604,
      "learning_rate": 0.0001727750366715562,
      "loss": 0.7203,
      "step": 5109
    },
    {
      "epoch": 0.4088,
      "grad_norm": 0.37754693627357483,
      "learning_rate": 0.00017276970262701695,
      "loss": 0.7517,
      "step": 5110
    },
    {
      "epoch": 0.40888,
      "grad_norm": 0.35631200671195984,
      "learning_rate": 0.00017276436858247766,
      "loss": 0.6044,
      "step": 5111
    },
    {
      "epoch": 0.40896,
      "grad_norm": 0.4702557921409607,
      "learning_rate": 0.0001727590345379384,
      "loss": 0.4335,
      "step": 5112
    },
    {
      "epoch": 0.40904,
      "grad_norm": 0.34253641963005066,
      "learning_rate": 0.00017275370049339912,
      "loss": 0.7717,
      "step": 5113
    },
    {
      "epoch": 0.40912,
      "grad_norm": 0.2938051223754883,
      "learning_rate": 0.00017274836644885986,
      "loss": 0.645,
      "step": 5114
    },
    {
      "epoch": 0.4092,
      "grad_norm": 0.37144502997398376,
      "learning_rate": 0.0001727430324043206,
      "loss": 1.0393,
      "step": 5115
    },
    {
      "epoch": 0.40928,
      "grad_norm": 0.533578634262085,
      "learning_rate": 0.0001727376983597813,
      "loss": 0.9286,
      "step": 5116
    },
    {
      "epoch": 0.40936,
      "grad_norm": 0.40343886613845825,
      "learning_rate": 0.00017273236431524205,
      "loss": 1.0378,
      "step": 5117
    },
    {
      "epoch": 0.40944,
      "grad_norm": 0.36291012167930603,
      "learning_rate": 0.00017272703027070276,
      "loss": 1.3606,
      "step": 5118
    },
    {
      "epoch": 0.40952,
      "grad_norm": 0.35617345571517944,
      "learning_rate": 0.0001727216962261635,
      "loss": 0.6374,
      "step": 5119
    },
    {
      "epoch": 0.4096,
      "grad_norm": 0.3241598904132843,
      "learning_rate": 0.00017271636218162421,
      "loss": 0.8755,
      "step": 5120
    },
    {
      "epoch": 0.40968,
      "grad_norm": 0.2625575363636017,
      "learning_rate": 0.00017271102813708495,
      "loss": 0.3865,
      "step": 5121
    },
    {
      "epoch": 0.40976,
      "grad_norm": 0.3142780065536499,
      "learning_rate": 0.0001727056940925457,
      "loss": 1.0104,
      "step": 5122
    },
    {
      "epoch": 0.40984,
      "grad_norm": 0.37475132942199707,
      "learning_rate": 0.0001727003600480064,
      "loss": 0.9686,
      "step": 5123
    },
    {
      "epoch": 0.40992,
      "grad_norm": 0.3448415994644165,
      "learning_rate": 0.00017269502600346715,
      "loss": 0.7231,
      "step": 5124
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.3190717399120331,
      "learning_rate": 0.00017268969195892786,
      "loss": 0.7905,
      "step": 5125
    },
    {
      "epoch": 0.41008,
      "grad_norm": 0.3507414758205414,
      "learning_rate": 0.0001726843579143886,
      "loss": 0.9094,
      "step": 5126
    },
    {
      "epoch": 0.41016,
      "grad_norm": 0.4243970513343811,
      "learning_rate": 0.00017267902386984934,
      "loss": 1.0304,
      "step": 5127
    },
    {
      "epoch": 0.41024,
      "grad_norm": 0.409658282995224,
      "learning_rate": 0.00017267368982531005,
      "loss": 0.6307,
      "step": 5128
    },
    {
      "epoch": 0.41032,
      "grad_norm": 0.35774028301239014,
      "learning_rate": 0.0001726683557807708,
      "loss": 0.5963,
      "step": 5129
    },
    {
      "epoch": 0.4104,
      "grad_norm": 0.3332609534263611,
      "learning_rate": 0.0001726630217362315,
      "loss": 0.7109,
      "step": 5130
    },
    {
      "epoch": 0.41048,
      "grad_norm": 0.3329797089099884,
      "learning_rate": 0.00017265768769169224,
      "loss": 0.9995,
      "step": 5131
    },
    {
      "epoch": 0.41056,
      "grad_norm": 0.32501375675201416,
      "learning_rate": 0.00017265235364715296,
      "loss": 0.9381,
      "step": 5132
    },
    {
      "epoch": 0.41064,
      "grad_norm": 0.39903056621551514,
      "learning_rate": 0.0001726470196026137,
      "loss": 0.8639,
      "step": 5133
    },
    {
      "epoch": 0.41072,
      "grad_norm": 0.3167252540588379,
      "learning_rate": 0.00017264168555807444,
      "loss": 1.1552,
      "step": 5134
    },
    {
      "epoch": 0.4108,
      "grad_norm": 0.42406851053237915,
      "learning_rate": 0.00017263635151353515,
      "loss": 0.7794,
      "step": 5135
    },
    {
      "epoch": 0.41088,
      "grad_norm": 0.3531454801559448,
      "learning_rate": 0.0001726310174689959,
      "loss": 0.9808,
      "step": 5136
    },
    {
      "epoch": 0.41096,
      "grad_norm": 0.44657662510871887,
      "learning_rate": 0.0001726256834244566,
      "loss": 0.9685,
      "step": 5137
    },
    {
      "epoch": 0.41104,
      "grad_norm": 0.4266294836997986,
      "learning_rate": 0.00017262034937991734,
      "loss": 0.7094,
      "step": 5138
    },
    {
      "epoch": 0.41112,
      "grad_norm": 0.42070063948631287,
      "learning_rate": 0.00017261501533537805,
      "loss": 0.739,
      "step": 5139
    },
    {
      "epoch": 0.4112,
      "grad_norm": 0.2806926965713501,
      "learning_rate": 0.0001726096812908388,
      "loss": 0.7533,
      "step": 5140
    },
    {
      "epoch": 0.41128,
      "grad_norm": 0.37761038541793823,
      "learning_rate": 0.00017260434724629953,
      "loss": 0.8934,
      "step": 5141
    },
    {
      "epoch": 0.41136,
      "grad_norm": 0.3784823715686798,
      "learning_rate": 0.00017259901320176025,
      "loss": 0.8209,
      "step": 5142
    },
    {
      "epoch": 0.41144,
      "grad_norm": 0.3625284731388092,
      "learning_rate": 0.000172593679157221,
      "loss": 0.8805,
      "step": 5143
    },
    {
      "epoch": 0.41152,
      "grad_norm": 0.3230898082256317,
      "learning_rate": 0.0001725883451126817,
      "loss": 0.8322,
      "step": 5144
    },
    {
      "epoch": 0.4116,
      "grad_norm": 0.4208146333694458,
      "learning_rate": 0.00017258301106814244,
      "loss": 0.7413,
      "step": 5145
    },
    {
      "epoch": 0.41168,
      "grad_norm": 0.39504677057266235,
      "learning_rate": 0.00017257767702360315,
      "loss": 0.9837,
      "step": 5146
    },
    {
      "epoch": 0.41176,
      "grad_norm": 0.45465087890625,
      "learning_rate": 0.0001725723429790639,
      "loss": 0.7192,
      "step": 5147
    },
    {
      "epoch": 0.41184,
      "grad_norm": 0.40256983041763306,
      "learning_rate": 0.00017256700893452463,
      "loss": 0.93,
      "step": 5148
    },
    {
      "epoch": 0.41192,
      "grad_norm": 0.4681049585342407,
      "learning_rate": 0.00017256167488998534,
      "loss": 1.0549,
      "step": 5149
    },
    {
      "epoch": 0.412,
      "grad_norm": 0.39424094557762146,
      "learning_rate": 0.00017255634084544608,
      "loss": 1.0161,
      "step": 5150
    },
    {
      "epoch": 0.41208,
      "grad_norm": 0.44545599818229675,
      "learning_rate": 0.0001725510068009068,
      "loss": 0.7437,
      "step": 5151
    },
    {
      "epoch": 0.41216,
      "grad_norm": 0.4432127773761749,
      "learning_rate": 0.00017254567275636754,
      "loss": 0.7965,
      "step": 5152
    },
    {
      "epoch": 0.41224,
      "grad_norm": 0.4663059413433075,
      "learning_rate": 0.00017254033871182825,
      "loss": 0.9151,
      "step": 5153
    },
    {
      "epoch": 0.41232,
      "grad_norm": 0.4619808495044708,
      "learning_rate": 0.000172535004667289,
      "loss": 0.8829,
      "step": 5154
    },
    {
      "epoch": 0.4124,
      "grad_norm": 0.47610029578208923,
      "learning_rate": 0.00017252967062274973,
      "loss": 0.9866,
      "step": 5155
    },
    {
      "epoch": 0.41248,
      "grad_norm": 0.3599960207939148,
      "learning_rate": 0.00017252433657821044,
      "loss": 0.6843,
      "step": 5156
    },
    {
      "epoch": 0.41256,
      "grad_norm": 0.3517369031906128,
      "learning_rate": 0.00017251900253367118,
      "loss": 0.9131,
      "step": 5157
    },
    {
      "epoch": 0.41264,
      "grad_norm": 0.45005762577056885,
      "learning_rate": 0.0001725136684891319,
      "loss": 0.9509,
      "step": 5158
    },
    {
      "epoch": 0.41272,
      "grad_norm": 0.3528590202331543,
      "learning_rate": 0.00017250833444459263,
      "loss": 0.8581,
      "step": 5159
    },
    {
      "epoch": 0.4128,
      "grad_norm": 0.32335400581359863,
      "learning_rate": 0.00017250300040005335,
      "loss": 0.6257,
      "step": 5160
    },
    {
      "epoch": 0.41288,
      "grad_norm": 0.47099176049232483,
      "learning_rate": 0.0001724976663555141,
      "loss": 0.7932,
      "step": 5161
    },
    {
      "epoch": 0.41296,
      "grad_norm": 0.3642826974391937,
      "learning_rate": 0.0001724923323109748,
      "loss": 0.719,
      "step": 5162
    },
    {
      "epoch": 0.41304,
      "grad_norm": 0.37351104617118835,
      "learning_rate": 0.00017248699826643554,
      "loss": 0.6652,
      "step": 5163
    },
    {
      "epoch": 0.41312,
      "grad_norm": 0.32584917545318604,
      "learning_rate": 0.00017248166422189628,
      "loss": 0.7305,
      "step": 5164
    },
    {
      "epoch": 0.4132,
      "grad_norm": 0.43376752734184265,
      "learning_rate": 0.000172476330177357,
      "loss": 0.6356,
      "step": 5165
    },
    {
      "epoch": 0.41328,
      "grad_norm": 0.39490801095962524,
      "learning_rate": 0.00017247099613281773,
      "loss": 1.0261,
      "step": 5166
    },
    {
      "epoch": 0.41336,
      "grad_norm": 0.46457281708717346,
      "learning_rate": 0.00017246566208827845,
      "loss": 1.0383,
      "step": 5167
    },
    {
      "epoch": 0.41344,
      "grad_norm": 0.29397979378700256,
      "learning_rate": 0.00017246032804373918,
      "loss": 1.1026,
      "step": 5168
    },
    {
      "epoch": 0.41352,
      "grad_norm": 0.34308475255966187,
      "learning_rate": 0.0001724549939991999,
      "loss": 0.9778,
      "step": 5169
    },
    {
      "epoch": 0.4136,
      "grad_norm": 0.41553813219070435,
      "learning_rate": 0.00017244965995466064,
      "loss": 0.6776,
      "step": 5170
    },
    {
      "epoch": 0.41368,
      "grad_norm": 0.3666290044784546,
      "learning_rate": 0.00017244432591012135,
      "loss": 0.6986,
      "step": 5171
    },
    {
      "epoch": 0.41376,
      "grad_norm": 0.44708800315856934,
      "learning_rate": 0.0001724389918655821,
      "loss": 1.1389,
      "step": 5172
    },
    {
      "epoch": 0.41384,
      "grad_norm": 0.40100041031837463,
      "learning_rate": 0.0001724336578210428,
      "loss": 0.8961,
      "step": 5173
    },
    {
      "epoch": 0.41392,
      "grad_norm": 0.4117909073829651,
      "learning_rate": 0.00017242832377650354,
      "loss": 0.7483,
      "step": 5174
    },
    {
      "epoch": 0.414,
      "grad_norm": 0.46776703000068665,
      "learning_rate": 0.00017242298973196426,
      "loss": 0.7618,
      "step": 5175
    },
    {
      "epoch": 0.41408,
      "grad_norm": 0.4468920826911926,
      "learning_rate": 0.000172417655687425,
      "loss": 0.9771,
      "step": 5176
    },
    {
      "epoch": 0.41416,
      "grad_norm": 0.49723491072654724,
      "learning_rate": 0.0001724123216428857,
      "loss": 0.9226,
      "step": 5177
    },
    {
      "epoch": 0.41424,
      "grad_norm": 0.3497505486011505,
      "learning_rate": 0.00017240698759834645,
      "loss": 0.9279,
      "step": 5178
    },
    {
      "epoch": 0.41432,
      "grad_norm": 0.369699090719223,
      "learning_rate": 0.00017240165355380716,
      "loss": 0.6791,
      "step": 5179
    },
    {
      "epoch": 0.4144,
      "grad_norm": 0.4182247817516327,
      "learning_rate": 0.0001723963195092679,
      "loss": 0.7363,
      "step": 5180
    },
    {
      "epoch": 0.41448,
      "grad_norm": 0.4061051905155182,
      "learning_rate": 0.00017239098546472864,
      "loss": 0.7689,
      "step": 5181
    },
    {
      "epoch": 0.41456,
      "grad_norm": 0.4389728307723999,
      "learning_rate": 0.00017238565142018935,
      "loss": 0.8608,
      "step": 5182
    },
    {
      "epoch": 0.41464,
      "grad_norm": 0.3862912952899933,
      "learning_rate": 0.0001723803173756501,
      "loss": 0.6964,
      "step": 5183
    },
    {
      "epoch": 0.41472,
      "grad_norm": 0.3357957601547241,
      "learning_rate": 0.0001723749833311108,
      "loss": 0.7605,
      "step": 5184
    },
    {
      "epoch": 0.4148,
      "grad_norm": 0.41169771552085876,
      "learning_rate": 0.00017236964928657155,
      "loss": 0.884,
      "step": 5185
    },
    {
      "epoch": 0.41488,
      "grad_norm": 0.461872935295105,
      "learning_rate": 0.00017236431524203226,
      "loss": 1.1894,
      "step": 5186
    },
    {
      "epoch": 0.41496,
      "grad_norm": 0.37131786346435547,
      "learning_rate": 0.000172358981197493,
      "loss": 0.949,
      "step": 5187
    },
    {
      "epoch": 0.41504,
      "grad_norm": 0.26375409960746765,
      "learning_rate": 0.00017235364715295374,
      "loss": 0.4748,
      "step": 5188
    },
    {
      "epoch": 0.41512,
      "grad_norm": 0.43077895045280457,
      "learning_rate": 0.00017234831310841445,
      "loss": 1.1808,
      "step": 5189
    },
    {
      "epoch": 0.4152,
      "grad_norm": 0.4185217022895813,
      "learning_rate": 0.0001723429790638752,
      "loss": 0.9034,
      "step": 5190
    },
    {
      "epoch": 0.41528,
      "grad_norm": 0.3784106373786926,
      "learning_rate": 0.0001723376450193359,
      "loss": 0.9124,
      "step": 5191
    },
    {
      "epoch": 0.41536,
      "grad_norm": 0.3297477960586548,
      "learning_rate": 0.00017233231097479664,
      "loss": 0.7352,
      "step": 5192
    },
    {
      "epoch": 0.41544,
      "grad_norm": 0.3687553405761719,
      "learning_rate": 0.00017232697693025736,
      "loss": 0.7579,
      "step": 5193
    },
    {
      "epoch": 0.41552,
      "grad_norm": 0.34654709696769714,
      "learning_rate": 0.0001723216428857181,
      "loss": 0.6977,
      "step": 5194
    },
    {
      "epoch": 0.4156,
      "grad_norm": 0.4860289692878723,
      "learning_rate": 0.00017231630884117884,
      "loss": 0.9287,
      "step": 5195
    },
    {
      "epoch": 0.41568,
      "grad_norm": 0.439292848110199,
      "learning_rate": 0.00017231097479663955,
      "loss": 0.9328,
      "step": 5196
    },
    {
      "epoch": 0.41576,
      "grad_norm": 0.41416826844215393,
      "learning_rate": 0.0001723056407521003,
      "loss": 0.774,
      "step": 5197
    },
    {
      "epoch": 0.41584,
      "grad_norm": 0.470621794462204,
      "learning_rate": 0.000172300306707561,
      "loss": 0.931,
      "step": 5198
    },
    {
      "epoch": 0.41592,
      "grad_norm": 0.43068787455558777,
      "learning_rate": 0.00017229497266302174,
      "loss": 0.8665,
      "step": 5199
    },
    {
      "epoch": 0.416,
      "grad_norm": 0.4059549570083618,
      "learning_rate": 0.00017228963861848245,
      "loss": 0.7097,
      "step": 5200
    },
    {
      "epoch": 0.41608,
      "grad_norm": 0.3776406943798065,
      "learning_rate": 0.0001722843045739432,
      "loss": 0.6479,
      "step": 5201
    },
    {
      "epoch": 0.41616,
      "grad_norm": 0.46254196763038635,
      "learning_rate": 0.00017227897052940393,
      "loss": 0.7996,
      "step": 5202
    },
    {
      "epoch": 0.41624,
      "grad_norm": 0.49012917280197144,
      "learning_rate": 0.00017227363648486465,
      "loss": 0.9409,
      "step": 5203
    },
    {
      "epoch": 0.41632,
      "grad_norm": 0.34730473160743713,
      "learning_rate": 0.00017226830244032539,
      "loss": 0.878,
      "step": 5204
    },
    {
      "epoch": 0.4164,
      "grad_norm": 0.3122757375240326,
      "learning_rate": 0.0001722629683957861,
      "loss": 0.5219,
      "step": 5205
    },
    {
      "epoch": 0.41648,
      "grad_norm": 0.3748197853565216,
      "learning_rate": 0.00017225763435124684,
      "loss": 1.0146,
      "step": 5206
    },
    {
      "epoch": 0.41656,
      "grad_norm": 0.4724923074245453,
      "learning_rate": 0.00017225230030670755,
      "loss": 1.0066,
      "step": 5207
    },
    {
      "epoch": 0.41664,
      "grad_norm": 0.485324889421463,
      "learning_rate": 0.0001722469662621683,
      "loss": 0.6746,
      "step": 5208
    },
    {
      "epoch": 0.41672,
      "grad_norm": 0.5213467478752136,
      "learning_rate": 0.00017224163221762903,
      "loss": 0.9598,
      "step": 5209
    },
    {
      "epoch": 0.4168,
      "grad_norm": 0.4736062288284302,
      "learning_rate": 0.00017223629817308974,
      "loss": 0.6786,
      "step": 5210
    },
    {
      "epoch": 0.41688,
      "grad_norm": 0.4217277765274048,
      "learning_rate": 0.00017223096412855048,
      "loss": 0.7117,
      "step": 5211
    },
    {
      "epoch": 0.41696,
      "grad_norm": 0.3780895173549652,
      "learning_rate": 0.0001722256300840112,
      "loss": 0.7381,
      "step": 5212
    },
    {
      "epoch": 0.41704,
      "grad_norm": 0.35777169466018677,
      "learning_rate": 0.00017222029603947194,
      "loss": 0.894,
      "step": 5213
    },
    {
      "epoch": 0.41712,
      "grad_norm": 0.42923375964164734,
      "learning_rate": 0.00017221496199493265,
      "loss": 1.0356,
      "step": 5214
    },
    {
      "epoch": 0.4172,
      "grad_norm": 0.547301173210144,
      "learning_rate": 0.0001722096279503934,
      "loss": 0.7776,
      "step": 5215
    },
    {
      "epoch": 0.41728,
      "grad_norm": 0.4662647843360901,
      "learning_rate": 0.00017220429390585413,
      "loss": 0.9811,
      "step": 5216
    },
    {
      "epoch": 0.41736,
      "grad_norm": 0.4961126446723938,
      "learning_rate": 0.00017219895986131484,
      "loss": 1.2376,
      "step": 5217
    },
    {
      "epoch": 0.41744,
      "grad_norm": 0.3649228811264038,
      "learning_rate": 0.00017219362581677558,
      "loss": 0.8546,
      "step": 5218
    },
    {
      "epoch": 0.41752,
      "grad_norm": 0.2763589322566986,
      "learning_rate": 0.0001721882917722363,
      "loss": 1.0864,
      "step": 5219
    },
    {
      "epoch": 0.4176,
      "grad_norm": 0.3685493767261505,
      "learning_rate": 0.00017218295772769703,
      "loss": 0.6617,
      "step": 5220
    },
    {
      "epoch": 0.41768,
      "grad_norm": 0.3564198911190033,
      "learning_rate": 0.00017217762368315775,
      "loss": 0.8743,
      "step": 5221
    },
    {
      "epoch": 0.41776,
      "grad_norm": 0.37911155819892883,
      "learning_rate": 0.00017217228963861849,
      "loss": 0.7753,
      "step": 5222
    },
    {
      "epoch": 0.41784,
      "grad_norm": 0.4237334430217743,
      "learning_rate": 0.00017216695559407923,
      "loss": 0.6079,
      "step": 5223
    },
    {
      "epoch": 0.41792,
      "grad_norm": 0.4568150043487549,
      "learning_rate": 0.00017216162154953994,
      "loss": 0.7536,
      "step": 5224
    },
    {
      "epoch": 0.418,
      "grad_norm": 0.4953700006008148,
      "learning_rate": 0.00017215628750500068,
      "loss": 0.8014,
      "step": 5225
    },
    {
      "epoch": 0.41808,
      "grad_norm": 0.3468970060348511,
      "learning_rate": 0.0001721509534604614,
      "loss": 1.0357,
      "step": 5226
    },
    {
      "epoch": 0.41816,
      "grad_norm": 0.31930676102638245,
      "learning_rate": 0.00017214561941592213,
      "loss": 0.991,
      "step": 5227
    },
    {
      "epoch": 0.41824,
      "grad_norm": 0.4564669132232666,
      "learning_rate": 0.00017214028537138287,
      "loss": 1.0828,
      "step": 5228
    },
    {
      "epoch": 0.41832,
      "grad_norm": 0.46081554889678955,
      "learning_rate": 0.00017213495132684358,
      "loss": 0.7783,
      "step": 5229
    },
    {
      "epoch": 0.4184,
      "grad_norm": 0.35519474744796753,
      "learning_rate": 0.00017212961728230432,
      "loss": 0.7428,
      "step": 5230
    },
    {
      "epoch": 0.41848,
      "grad_norm": 0.3213856518268585,
      "learning_rate": 0.00017212428323776504,
      "loss": 0.5289,
      "step": 5231
    },
    {
      "epoch": 0.41856,
      "grad_norm": 0.4689493775367737,
      "learning_rate": 0.00017211894919322578,
      "loss": 0.9179,
      "step": 5232
    },
    {
      "epoch": 0.41864,
      "grad_norm": 0.4210139811038971,
      "learning_rate": 0.0001721136151486865,
      "loss": 0.944,
      "step": 5233
    },
    {
      "epoch": 0.41872,
      "grad_norm": 0.4525901973247528,
      "learning_rate": 0.00017210828110414723,
      "loss": 1.0479,
      "step": 5234
    },
    {
      "epoch": 0.4188,
      "grad_norm": 0.4420229494571686,
      "learning_rate": 0.00017210294705960797,
      "loss": 0.9853,
      "step": 5235
    },
    {
      "epoch": 0.41888,
      "grad_norm": 0.3971114754676819,
      "learning_rate": 0.00017209761301506868,
      "loss": 0.9081,
      "step": 5236
    },
    {
      "epoch": 0.41896,
      "grad_norm": 0.34425944089889526,
      "learning_rate": 0.00017209227897052942,
      "loss": 0.8051,
      "step": 5237
    },
    {
      "epoch": 0.41904,
      "grad_norm": 0.23994359374046326,
      "learning_rate": 0.00017208694492599013,
      "loss": 0.6976,
      "step": 5238
    },
    {
      "epoch": 0.41912,
      "grad_norm": 0.475339412689209,
      "learning_rate": 0.00017208161088145087,
      "loss": 0.7203,
      "step": 5239
    },
    {
      "epoch": 0.4192,
      "grad_norm": 0.3566196858882904,
      "learning_rate": 0.0001720762768369116,
      "loss": 1.0274,
      "step": 5240
    },
    {
      "epoch": 0.41928,
      "grad_norm": 0.3676914572715759,
      "learning_rate": 0.00017207094279237233,
      "loss": 0.7117,
      "step": 5241
    },
    {
      "epoch": 0.41936,
      "grad_norm": 0.4379293918609619,
      "learning_rate": 0.00017206560874783307,
      "loss": 0.716,
      "step": 5242
    },
    {
      "epoch": 0.41944,
      "grad_norm": 0.3445058763027191,
      "learning_rate": 0.00017206027470329378,
      "loss": 0.9008,
      "step": 5243
    },
    {
      "epoch": 0.41952,
      "grad_norm": 0.32152873277664185,
      "learning_rate": 0.00017205494065875452,
      "loss": 0.937,
      "step": 5244
    },
    {
      "epoch": 0.4196,
      "grad_norm": 0.3881634473800659,
      "learning_rate": 0.00017204960661421523,
      "loss": 0.594,
      "step": 5245
    },
    {
      "epoch": 0.41968,
      "grad_norm": 0.2885564863681793,
      "learning_rate": 0.00017204427256967597,
      "loss": 0.5632,
      "step": 5246
    },
    {
      "epoch": 0.41976,
      "grad_norm": 0.43244338035583496,
      "learning_rate": 0.00017203893852513668,
      "loss": 0.9559,
      "step": 5247
    },
    {
      "epoch": 0.41984,
      "grad_norm": 0.4532220661640167,
      "learning_rate": 0.00017203360448059742,
      "loss": 0.9857,
      "step": 5248
    },
    {
      "epoch": 0.41992,
      "grad_norm": 0.4004870057106018,
      "learning_rate": 0.00017202827043605816,
      "loss": 0.9155,
      "step": 5249
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.3870096802711487,
      "learning_rate": 0.00017202293639151888,
      "loss": 1.0503,
      "step": 5250
    },
    {
      "epoch": 0.42008,
      "grad_norm": 0.31234273314476013,
      "learning_rate": 0.00017201760234697962,
      "loss": 1.0823,
      "step": 5251
    },
    {
      "epoch": 0.42016,
      "grad_norm": 0.3546447455883026,
      "learning_rate": 0.00017201226830244033,
      "loss": 0.7529,
      "step": 5252
    },
    {
      "epoch": 0.42024,
      "grad_norm": 0.4600354731082916,
      "learning_rate": 0.00017200693425790107,
      "loss": 0.5186,
      "step": 5253
    },
    {
      "epoch": 0.42032,
      "grad_norm": 0.46213898062705994,
      "learning_rate": 0.00017200160021336178,
      "loss": 0.941,
      "step": 5254
    },
    {
      "epoch": 0.4204,
      "grad_norm": 0.41789451241493225,
      "learning_rate": 0.00017199626616882252,
      "loss": 0.8142,
      "step": 5255
    },
    {
      "epoch": 0.42048,
      "grad_norm": 0.3586781322956085,
      "learning_rate": 0.00017199093212428326,
      "loss": 0.9168,
      "step": 5256
    },
    {
      "epoch": 0.42056,
      "grad_norm": 0.4285909831523895,
      "learning_rate": 0.00017198559807974397,
      "loss": 0.6972,
      "step": 5257
    },
    {
      "epoch": 0.42064,
      "grad_norm": 0.561850368976593,
      "learning_rate": 0.00017198026403520471,
      "loss": 0.9238,
      "step": 5258
    },
    {
      "epoch": 0.42072,
      "grad_norm": 0.44237613677978516,
      "learning_rate": 0.00017197492999066543,
      "loss": 0.9147,
      "step": 5259
    },
    {
      "epoch": 0.4208,
      "grad_norm": 0.4035990238189697,
      "learning_rate": 0.00017196959594612617,
      "loss": 0.9381,
      "step": 5260
    },
    {
      "epoch": 0.42088,
      "grad_norm": 0.32234829664230347,
      "learning_rate": 0.00017196426190158688,
      "loss": 0.6792,
      "step": 5261
    },
    {
      "epoch": 0.42096,
      "grad_norm": 0.3563219904899597,
      "learning_rate": 0.00017195892785704762,
      "loss": 0.6502,
      "step": 5262
    },
    {
      "epoch": 0.42104,
      "grad_norm": 0.3041801154613495,
      "learning_rate": 0.00017195359381250836,
      "loss": 0.5084,
      "step": 5263
    },
    {
      "epoch": 0.42112,
      "grad_norm": 0.4208466708660126,
      "learning_rate": 0.00017194825976796907,
      "loss": 0.6045,
      "step": 5264
    },
    {
      "epoch": 0.4212,
      "grad_norm": 0.3626661002635956,
      "learning_rate": 0.0001719429257234298,
      "loss": 0.7207,
      "step": 5265
    },
    {
      "epoch": 0.42128,
      "grad_norm": 0.3157767653465271,
      "learning_rate": 0.00017193759167889052,
      "loss": 0.7214,
      "step": 5266
    },
    {
      "epoch": 0.42136,
      "grad_norm": 0.3551448881626129,
      "learning_rate": 0.00017193225763435126,
      "loss": 0.8582,
      "step": 5267
    },
    {
      "epoch": 0.42144,
      "grad_norm": 0.32414114475250244,
      "learning_rate": 0.00017192692358981198,
      "loss": 0.8635,
      "step": 5268
    },
    {
      "epoch": 0.42152,
      "grad_norm": 0.37961331009864807,
      "learning_rate": 0.00017192158954527272,
      "loss": 0.8706,
      "step": 5269
    },
    {
      "epoch": 0.4216,
      "grad_norm": 0.35127201676368713,
      "learning_rate": 0.00017191625550073346,
      "loss": 0.9086,
      "step": 5270
    },
    {
      "epoch": 0.42168,
      "grad_norm": 0.34855422377586365,
      "learning_rate": 0.00017191092145619417,
      "loss": 0.6284,
      "step": 5271
    },
    {
      "epoch": 0.42176,
      "grad_norm": 0.3535406291484833,
      "learning_rate": 0.0001719055874116549,
      "loss": 0.9536,
      "step": 5272
    },
    {
      "epoch": 0.42184,
      "grad_norm": 0.42795050144195557,
      "learning_rate": 0.00017190025336711562,
      "loss": 0.723,
      "step": 5273
    },
    {
      "epoch": 0.42192,
      "grad_norm": 0.3182898759841919,
      "learning_rate": 0.00017189491932257636,
      "loss": 0.8171,
      "step": 5274
    },
    {
      "epoch": 0.422,
      "grad_norm": 0.3411947190761566,
      "learning_rate": 0.0001718895852780371,
      "loss": 1.2346,
      "step": 5275
    },
    {
      "epoch": 0.42208,
      "grad_norm": 0.48034051060676575,
      "learning_rate": 0.00017188425123349781,
      "loss": 0.9531,
      "step": 5276
    },
    {
      "epoch": 0.42216,
      "grad_norm": 0.35946300625801086,
      "learning_rate": 0.00017187891718895855,
      "loss": 0.6635,
      "step": 5277
    },
    {
      "epoch": 0.42224,
      "grad_norm": 0.3419342339038849,
      "learning_rate": 0.00017187358314441927,
      "loss": 1.0559,
      "step": 5278
    },
    {
      "epoch": 0.42232,
      "grad_norm": 0.2843591570854187,
      "learning_rate": 0.00017186824909988,
      "loss": 0.5144,
      "step": 5279
    },
    {
      "epoch": 0.4224,
      "grad_norm": 0.3744608163833618,
      "learning_rate": 0.00017186291505534072,
      "loss": 0.634,
      "step": 5280
    },
    {
      "epoch": 0.42248,
      "grad_norm": 0.365047425031662,
      "learning_rate": 0.00017185758101080146,
      "loss": 0.9064,
      "step": 5281
    },
    {
      "epoch": 0.42256,
      "grad_norm": 0.3804619014263153,
      "learning_rate": 0.0001718522469662622,
      "loss": 0.8346,
      "step": 5282
    },
    {
      "epoch": 0.42264,
      "grad_norm": 0.49355027079582214,
      "learning_rate": 0.0001718469129217229,
      "loss": 0.9903,
      "step": 5283
    },
    {
      "epoch": 0.42272,
      "grad_norm": 0.5264601707458496,
      "learning_rate": 0.00017184157887718365,
      "loss": 0.7464,
      "step": 5284
    },
    {
      "epoch": 0.4228,
      "grad_norm": 0.39714011549949646,
      "learning_rate": 0.00017183624483264436,
      "loss": 0.8216,
      "step": 5285
    },
    {
      "epoch": 0.42288,
      "grad_norm": 0.32689833641052246,
      "learning_rate": 0.0001718309107881051,
      "loss": 0.8133,
      "step": 5286
    },
    {
      "epoch": 0.42296,
      "grad_norm": 0.3873029351234436,
      "learning_rate": 0.00017182557674356582,
      "loss": 0.7094,
      "step": 5287
    },
    {
      "epoch": 0.42304,
      "grad_norm": 0.3577357828617096,
      "learning_rate": 0.00017182024269902656,
      "loss": 0.7864,
      "step": 5288
    },
    {
      "epoch": 0.42312,
      "grad_norm": 0.4377426505088806,
      "learning_rate": 0.00017181490865448727,
      "loss": 0.9085,
      "step": 5289
    },
    {
      "epoch": 0.4232,
      "grad_norm": 0.43366539478302,
      "learning_rate": 0.000171809574609948,
      "loss": 0.9675,
      "step": 5290
    },
    {
      "epoch": 0.42328,
      "grad_norm": 0.3174733817577362,
      "learning_rate": 0.00017180424056540872,
      "loss": 0.9147,
      "step": 5291
    },
    {
      "epoch": 0.42336,
      "grad_norm": 0.3344191908836365,
      "learning_rate": 0.00017179890652086946,
      "loss": 0.5696,
      "step": 5292
    },
    {
      "epoch": 0.42344,
      "grad_norm": 0.40535029768943787,
      "learning_rate": 0.0001717935724763302,
      "loss": 0.6673,
      "step": 5293
    },
    {
      "epoch": 0.42352,
      "grad_norm": 0.38875317573547363,
      "learning_rate": 0.00017178823843179092,
      "loss": 0.6468,
      "step": 5294
    },
    {
      "epoch": 0.4236,
      "grad_norm": 0.38828861713409424,
      "learning_rate": 0.00017178290438725165,
      "loss": 1.0938,
      "step": 5295
    },
    {
      "epoch": 0.42368,
      "grad_norm": 0.49701911211013794,
      "learning_rate": 0.00017177757034271237,
      "loss": 0.8368,
      "step": 5296
    },
    {
      "epoch": 0.42376,
      "grad_norm": 0.292983740568161,
      "learning_rate": 0.0001717722362981731,
      "loss": 0.7712,
      "step": 5297
    },
    {
      "epoch": 0.42384,
      "grad_norm": 0.25631386041641235,
      "learning_rate": 0.00017176690225363382,
      "loss": 0.4524,
      "step": 5298
    },
    {
      "epoch": 0.42392,
      "grad_norm": 0.41705289483070374,
      "learning_rate": 0.00017176156820909456,
      "loss": 0.833,
      "step": 5299
    },
    {
      "epoch": 0.424,
      "grad_norm": 0.33929991722106934,
      "learning_rate": 0.00017175623416455527,
      "loss": 0.9179,
      "step": 5300
    },
    {
      "epoch": 0.42408,
      "grad_norm": 0.3515143394470215,
      "learning_rate": 0.000171750900120016,
      "loss": 1.1226,
      "step": 5301
    },
    {
      "epoch": 0.42416,
      "grad_norm": 0.39045849442481995,
      "learning_rate": 0.00017174556607547673,
      "loss": 0.8208,
      "step": 5302
    },
    {
      "epoch": 0.42424,
      "grad_norm": 0.40412962436676025,
      "learning_rate": 0.00017174023203093747,
      "loss": 0.8854,
      "step": 5303
    },
    {
      "epoch": 0.42432,
      "grad_norm": 0.46624282002449036,
      "learning_rate": 0.00017173489798639818,
      "loss": 0.6592,
      "step": 5304
    },
    {
      "epoch": 0.4244,
      "grad_norm": 0.38052627444267273,
      "learning_rate": 0.00017172956394185892,
      "loss": 0.9885,
      "step": 5305
    },
    {
      "epoch": 0.42448,
      "grad_norm": 0.34685832262039185,
      "learning_rate": 0.00017172422989731963,
      "loss": 0.6142,
      "step": 5306
    },
    {
      "epoch": 0.42456,
      "grad_norm": 0.46215516328811646,
      "learning_rate": 0.00017171889585278037,
      "loss": 0.8127,
      "step": 5307
    },
    {
      "epoch": 0.42464,
      "grad_norm": 0.35702580213546753,
      "learning_rate": 0.0001717135618082411,
      "loss": 0.7843,
      "step": 5308
    },
    {
      "epoch": 0.42472,
      "grad_norm": 0.35984593629837036,
      "learning_rate": 0.00017170822776370182,
      "loss": 0.9198,
      "step": 5309
    },
    {
      "epoch": 0.4248,
      "grad_norm": 0.3957608640193939,
      "learning_rate": 0.00017170289371916256,
      "loss": 0.8202,
      "step": 5310
    },
    {
      "epoch": 0.42488,
      "grad_norm": 0.4685216546058655,
      "learning_rate": 0.00017169755967462328,
      "loss": 0.9047,
      "step": 5311
    },
    {
      "epoch": 0.42496,
      "grad_norm": 0.41919979453086853,
      "learning_rate": 0.00017169222563008402,
      "loss": 0.6656,
      "step": 5312
    },
    {
      "epoch": 0.42504,
      "grad_norm": 0.39464282989501953,
      "learning_rate": 0.00017168689158554473,
      "loss": 0.7047,
      "step": 5313
    },
    {
      "epoch": 0.42512,
      "grad_norm": 0.3467644155025482,
      "learning_rate": 0.00017168155754100547,
      "loss": 1.1772,
      "step": 5314
    },
    {
      "epoch": 0.4252,
      "grad_norm": 0.41390979290008545,
      "learning_rate": 0.00017167622349646618,
      "loss": 0.593,
      "step": 5315
    },
    {
      "epoch": 0.42528,
      "grad_norm": 0.3696548342704773,
      "learning_rate": 0.00017167088945192692,
      "loss": 0.9016,
      "step": 5316
    },
    {
      "epoch": 0.42536,
      "grad_norm": 0.3066486716270447,
      "learning_rate": 0.00017166555540738766,
      "loss": 0.5092,
      "step": 5317
    },
    {
      "epoch": 0.42544,
      "grad_norm": 0.4177928864955902,
      "learning_rate": 0.00017166022136284837,
      "loss": 0.6957,
      "step": 5318
    },
    {
      "epoch": 0.42552,
      "grad_norm": 0.49220460653305054,
      "learning_rate": 0.0001716548873183091,
      "loss": 0.9103,
      "step": 5319
    },
    {
      "epoch": 0.4256,
      "grad_norm": 0.38201484084129333,
      "learning_rate": 0.00017164955327376983,
      "loss": 0.7341,
      "step": 5320
    },
    {
      "epoch": 0.42568,
      "grad_norm": 0.3970271050930023,
      "learning_rate": 0.00017164421922923057,
      "loss": 0.9397,
      "step": 5321
    },
    {
      "epoch": 0.42576,
      "grad_norm": 0.3537176847457886,
      "learning_rate": 0.00017163888518469128,
      "loss": 0.6451,
      "step": 5322
    },
    {
      "epoch": 0.42584,
      "grad_norm": 0.47067564725875854,
      "learning_rate": 0.00017163355114015202,
      "loss": 1.1625,
      "step": 5323
    },
    {
      "epoch": 0.42592,
      "grad_norm": 0.31762924790382385,
      "learning_rate": 0.00017162821709561276,
      "loss": 0.7446,
      "step": 5324
    },
    {
      "epoch": 0.426,
      "grad_norm": 0.3792651891708374,
      "learning_rate": 0.00017162288305107347,
      "loss": 0.6867,
      "step": 5325
    },
    {
      "epoch": 0.42608,
      "grad_norm": 0.43520230054855347,
      "learning_rate": 0.0001716175490065342,
      "loss": 0.795,
      "step": 5326
    },
    {
      "epoch": 0.42616,
      "grad_norm": 0.41507673263549805,
      "learning_rate": 0.00017161221496199492,
      "loss": 0.7815,
      "step": 5327
    },
    {
      "epoch": 0.42624,
      "grad_norm": 0.554017186164856,
      "learning_rate": 0.00017160688091745566,
      "loss": 1.5052,
      "step": 5328
    },
    {
      "epoch": 0.42632,
      "grad_norm": 0.36691269278526306,
      "learning_rate": 0.0001716015468729164,
      "loss": 0.6455,
      "step": 5329
    },
    {
      "epoch": 0.4264,
      "grad_norm": 0.34971803426742554,
      "learning_rate": 0.00017159621282837712,
      "loss": 0.8442,
      "step": 5330
    },
    {
      "epoch": 0.42648,
      "grad_norm": 0.4519405961036682,
      "learning_rate": 0.00017159087878383786,
      "loss": 0.6322,
      "step": 5331
    },
    {
      "epoch": 0.42656,
      "grad_norm": 0.37074342370033264,
      "learning_rate": 0.00017158554473929857,
      "loss": 1.0121,
      "step": 5332
    },
    {
      "epoch": 0.42664,
      "grad_norm": 0.30893614888191223,
      "learning_rate": 0.0001715802106947593,
      "loss": 0.8363,
      "step": 5333
    },
    {
      "epoch": 0.42672,
      "grad_norm": 0.4939398169517517,
      "learning_rate": 0.00017157487665022002,
      "loss": 0.9366,
      "step": 5334
    },
    {
      "epoch": 0.4268,
      "grad_norm": 0.3981059789657593,
      "learning_rate": 0.00017156954260568076,
      "loss": 0.6226,
      "step": 5335
    },
    {
      "epoch": 0.42688,
      "grad_norm": 0.4188864529132843,
      "learning_rate": 0.0001715642085611415,
      "loss": 1.1325,
      "step": 5336
    },
    {
      "epoch": 0.42696,
      "grad_norm": 0.385043203830719,
      "learning_rate": 0.00017155887451660221,
      "loss": 0.7481,
      "step": 5337
    },
    {
      "epoch": 0.42704,
      "grad_norm": 0.4918714165687561,
      "learning_rate": 0.00017155354047206295,
      "loss": 0.7137,
      "step": 5338
    },
    {
      "epoch": 0.42712,
      "grad_norm": 0.45074397325515747,
      "learning_rate": 0.00017154820642752367,
      "loss": 1.1418,
      "step": 5339
    },
    {
      "epoch": 0.4272,
      "grad_norm": 0.4809913635253906,
      "learning_rate": 0.0001715428723829844,
      "loss": 1.1352,
      "step": 5340
    },
    {
      "epoch": 0.42728,
      "grad_norm": 0.41359660029411316,
      "learning_rate": 0.00017153753833844512,
      "loss": 0.9306,
      "step": 5341
    },
    {
      "epoch": 0.42736,
      "grad_norm": 0.4322482943534851,
      "learning_rate": 0.00017153220429390586,
      "loss": 0.6131,
      "step": 5342
    },
    {
      "epoch": 0.42744,
      "grad_norm": 0.3355712890625,
      "learning_rate": 0.0001715268702493666,
      "loss": 0.8944,
      "step": 5343
    },
    {
      "epoch": 0.42752,
      "grad_norm": 0.4636426568031311,
      "learning_rate": 0.0001715215362048273,
      "loss": 0.7278,
      "step": 5344
    },
    {
      "epoch": 0.4276,
      "grad_norm": 0.3264223635196686,
      "learning_rate": 0.00017151620216028805,
      "loss": 0.6682,
      "step": 5345
    },
    {
      "epoch": 0.42768,
      "grad_norm": 0.42370277643203735,
      "learning_rate": 0.00017151086811574876,
      "loss": 0.7465,
      "step": 5346
    },
    {
      "epoch": 0.42776,
      "grad_norm": 0.324350506067276,
      "learning_rate": 0.0001715055340712095,
      "loss": 0.6476,
      "step": 5347
    },
    {
      "epoch": 0.42784,
      "grad_norm": 0.3961274325847626,
      "learning_rate": 0.00017150020002667022,
      "loss": 0.6616,
      "step": 5348
    },
    {
      "epoch": 0.42792,
      "grad_norm": 0.3627927303314209,
      "learning_rate": 0.00017149486598213096,
      "loss": 0.97,
      "step": 5349
    },
    {
      "epoch": 0.428,
      "grad_norm": 0.4032374620437622,
      "learning_rate": 0.0001714895319375917,
      "loss": 0.735,
      "step": 5350
    },
    {
      "epoch": 0.42808,
      "grad_norm": 0.35676202178001404,
      "learning_rate": 0.0001714841978930524,
      "loss": 1.0156,
      "step": 5351
    },
    {
      "epoch": 0.42816,
      "grad_norm": 0.3582102358341217,
      "learning_rate": 0.00017147886384851315,
      "loss": 0.7105,
      "step": 5352
    },
    {
      "epoch": 0.42824,
      "grad_norm": 0.34301263093948364,
      "learning_rate": 0.00017147352980397386,
      "loss": 0.7147,
      "step": 5353
    },
    {
      "epoch": 0.42832,
      "grad_norm": 0.40490031242370605,
      "learning_rate": 0.0001714681957594346,
      "loss": 0.7896,
      "step": 5354
    },
    {
      "epoch": 0.4284,
      "grad_norm": 0.35559913516044617,
      "learning_rate": 0.00017146286171489531,
      "loss": 0.7947,
      "step": 5355
    },
    {
      "epoch": 0.42848,
      "grad_norm": 0.31822043657302856,
      "learning_rate": 0.00017145752767035605,
      "loss": 1.0879,
      "step": 5356
    },
    {
      "epoch": 0.42856,
      "grad_norm": 0.41478466987609863,
      "learning_rate": 0.0001714521936258168,
      "loss": 0.7352,
      "step": 5357
    },
    {
      "epoch": 0.42864,
      "grad_norm": 0.3637951612472534,
      "learning_rate": 0.0001714468595812775,
      "loss": 0.8134,
      "step": 5358
    },
    {
      "epoch": 0.42872,
      "grad_norm": 0.6589521169662476,
      "learning_rate": 0.00017144152553673825,
      "loss": 1.2557,
      "step": 5359
    },
    {
      "epoch": 0.4288,
      "grad_norm": 0.5062434673309326,
      "learning_rate": 0.00017143619149219896,
      "loss": 0.8168,
      "step": 5360
    },
    {
      "epoch": 0.42888,
      "grad_norm": 0.3974234461784363,
      "learning_rate": 0.0001714308574476597,
      "loss": 1.0083,
      "step": 5361
    },
    {
      "epoch": 0.42896,
      "grad_norm": 0.35317134857177734,
      "learning_rate": 0.0001714255234031204,
      "loss": 0.5309,
      "step": 5362
    },
    {
      "epoch": 0.42904,
      "grad_norm": 0.49688079953193665,
      "learning_rate": 0.00017142018935858115,
      "loss": 1.2727,
      "step": 5363
    },
    {
      "epoch": 0.42912,
      "grad_norm": 0.5113208293914795,
      "learning_rate": 0.0001714148553140419,
      "loss": 0.9533,
      "step": 5364
    },
    {
      "epoch": 0.4292,
      "grad_norm": 0.4014342427253723,
      "learning_rate": 0.0001714095212695026,
      "loss": 0.8549,
      "step": 5365
    },
    {
      "epoch": 0.42928,
      "grad_norm": 0.28569936752319336,
      "learning_rate": 0.00017140418722496334,
      "loss": 0.7039,
      "step": 5366
    },
    {
      "epoch": 0.42936,
      "grad_norm": 0.3311566710472107,
      "learning_rate": 0.00017139885318042406,
      "loss": 0.8497,
      "step": 5367
    },
    {
      "epoch": 0.42944,
      "grad_norm": 0.3700433075428009,
      "learning_rate": 0.0001713935191358848,
      "loss": 1.1903,
      "step": 5368
    },
    {
      "epoch": 0.42952,
      "grad_norm": 0.4026128053665161,
      "learning_rate": 0.0001713881850913455,
      "loss": 0.8606,
      "step": 5369
    },
    {
      "epoch": 0.4296,
      "grad_norm": 0.3752747178077698,
      "learning_rate": 0.00017138285104680625,
      "loss": 0.649,
      "step": 5370
    },
    {
      "epoch": 0.42968,
      "grad_norm": 0.4425264894962311,
      "learning_rate": 0.000171377517002267,
      "loss": 0.809,
      "step": 5371
    },
    {
      "epoch": 0.42976,
      "grad_norm": 0.3667674660682678,
      "learning_rate": 0.0001713721829577277,
      "loss": 1.0666,
      "step": 5372
    },
    {
      "epoch": 0.42984,
      "grad_norm": 0.4568033516407013,
      "learning_rate": 0.00017136684891318844,
      "loss": 0.6872,
      "step": 5373
    },
    {
      "epoch": 0.42992,
      "grad_norm": 0.43213748931884766,
      "learning_rate": 0.00017136151486864915,
      "loss": 0.969,
      "step": 5374
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.4252469539642334,
      "learning_rate": 0.0001713561808241099,
      "loss": 0.7583,
      "step": 5375
    },
    {
      "epoch": 0.43008,
      "grad_norm": 0.41153669357299805,
      "learning_rate": 0.00017135084677957063,
      "loss": 0.6635,
      "step": 5376
    },
    {
      "epoch": 0.43016,
      "grad_norm": 0.45099180936813354,
      "learning_rate": 0.00017134551273503135,
      "loss": 0.8897,
      "step": 5377
    },
    {
      "epoch": 0.43024,
      "grad_norm": 0.47782042622566223,
      "learning_rate": 0.0001713401786904921,
      "loss": 0.9196,
      "step": 5378
    },
    {
      "epoch": 0.43032,
      "grad_norm": 0.37258219718933105,
      "learning_rate": 0.0001713348446459528,
      "loss": 1.1111,
      "step": 5379
    },
    {
      "epoch": 0.4304,
      "grad_norm": 0.5789052248001099,
      "learning_rate": 0.00017132951060141354,
      "loss": 0.7151,
      "step": 5380
    },
    {
      "epoch": 0.43048,
      "grad_norm": 0.3266003429889679,
      "learning_rate": 0.00017132417655687425,
      "loss": 0.6357,
      "step": 5381
    },
    {
      "epoch": 0.43056,
      "grad_norm": 0.4215143620967865,
      "learning_rate": 0.000171318842512335,
      "loss": 0.7477,
      "step": 5382
    },
    {
      "epoch": 0.43064,
      "grad_norm": 0.475942999124527,
      "learning_rate": 0.00017131350846779573,
      "loss": 0.9261,
      "step": 5383
    },
    {
      "epoch": 0.43072,
      "grad_norm": 0.35644614696502686,
      "learning_rate": 0.00017130817442325644,
      "loss": 0.995,
      "step": 5384
    },
    {
      "epoch": 0.4308,
      "grad_norm": 0.4026000201702118,
      "learning_rate": 0.00017130284037871718,
      "loss": 0.6809,
      "step": 5385
    },
    {
      "epoch": 0.43088,
      "grad_norm": 0.3361063301563263,
      "learning_rate": 0.0001712975063341779,
      "loss": 0.7761,
      "step": 5386
    },
    {
      "epoch": 0.43096,
      "grad_norm": 0.38059473037719727,
      "learning_rate": 0.00017129217228963864,
      "loss": 0.8344,
      "step": 5387
    },
    {
      "epoch": 0.43104,
      "grad_norm": 0.5267733335494995,
      "learning_rate": 0.00017128683824509935,
      "loss": 0.794,
      "step": 5388
    },
    {
      "epoch": 0.43112,
      "grad_norm": 0.4050527811050415,
      "learning_rate": 0.0001712815042005601,
      "loss": 0.7245,
      "step": 5389
    },
    {
      "epoch": 0.4312,
      "grad_norm": 0.3444284200668335,
      "learning_rate": 0.00017127617015602083,
      "loss": 0.5823,
      "step": 5390
    },
    {
      "epoch": 0.43128,
      "grad_norm": 0.48547688126564026,
      "learning_rate": 0.00017127083611148154,
      "loss": 0.8168,
      "step": 5391
    },
    {
      "epoch": 0.43136,
      "grad_norm": 0.5336244702339172,
      "learning_rate": 0.00017126550206694228,
      "loss": 0.8912,
      "step": 5392
    },
    {
      "epoch": 0.43144,
      "grad_norm": 0.4034925699234009,
      "learning_rate": 0.000171260168022403,
      "loss": 0.9497,
      "step": 5393
    },
    {
      "epoch": 0.43152,
      "grad_norm": 0.48515596985816956,
      "learning_rate": 0.00017125483397786373,
      "loss": 0.8769,
      "step": 5394
    },
    {
      "epoch": 0.4316,
      "grad_norm": 0.30671417713165283,
      "learning_rate": 0.00017124949993332445,
      "loss": 0.528,
      "step": 5395
    },
    {
      "epoch": 0.43168,
      "grad_norm": 0.43545541167259216,
      "learning_rate": 0.0001712441658887852,
      "loss": 0.8254,
      "step": 5396
    },
    {
      "epoch": 0.43176,
      "grad_norm": 0.35615408420562744,
      "learning_rate": 0.00017123883184424593,
      "loss": 0.802,
      "step": 5397
    },
    {
      "epoch": 0.43184,
      "grad_norm": 0.4320562779903412,
      "learning_rate": 0.00017123349779970664,
      "loss": 0.9354,
      "step": 5398
    },
    {
      "epoch": 0.43192,
      "grad_norm": 0.343828946352005,
      "learning_rate": 0.00017122816375516738,
      "loss": 0.5593,
      "step": 5399
    },
    {
      "epoch": 0.432,
      "grad_norm": 0.41269129514694214,
      "learning_rate": 0.0001712228297106281,
      "loss": 1.1743,
      "step": 5400
    },
    {
      "epoch": 0.43208,
      "grad_norm": 0.5078882575035095,
      "learning_rate": 0.00017121749566608883,
      "loss": 0.6773,
      "step": 5401
    },
    {
      "epoch": 0.43216,
      "grad_norm": 0.4699852764606476,
      "learning_rate": 0.00017121216162154954,
      "loss": 0.8406,
      "step": 5402
    },
    {
      "epoch": 0.43224,
      "grad_norm": 0.38609278202056885,
      "learning_rate": 0.00017120682757701028,
      "loss": 0.6859,
      "step": 5403
    },
    {
      "epoch": 0.43232,
      "grad_norm": 0.409496545791626,
      "learning_rate": 0.00017120149353247102,
      "loss": 0.8188,
      "step": 5404
    },
    {
      "epoch": 0.4324,
      "grad_norm": 0.32896682620048523,
      "learning_rate": 0.00017119615948793174,
      "loss": 0.5905,
      "step": 5405
    },
    {
      "epoch": 0.43248,
      "grad_norm": 0.35264852643013,
      "learning_rate": 0.00017119082544339248,
      "loss": 0.7119,
      "step": 5406
    },
    {
      "epoch": 0.43256,
      "grad_norm": 0.3174992501735687,
      "learning_rate": 0.0001711854913988532,
      "loss": 0.9559,
      "step": 5407
    },
    {
      "epoch": 0.43264,
      "grad_norm": 0.41052600741386414,
      "learning_rate": 0.00017118015735431393,
      "loss": 0.5817,
      "step": 5408
    },
    {
      "epoch": 0.43272,
      "grad_norm": 0.7031033635139465,
      "learning_rate": 0.00017117482330977464,
      "loss": 1.2237,
      "step": 5409
    },
    {
      "epoch": 0.4328,
      "grad_norm": 0.3770110309123993,
      "learning_rate": 0.00017116948926523538,
      "loss": 0.8113,
      "step": 5410
    },
    {
      "epoch": 0.43288,
      "grad_norm": 0.38392314314842224,
      "learning_rate": 0.00017116415522069612,
      "loss": 0.6824,
      "step": 5411
    },
    {
      "epoch": 0.43296,
      "grad_norm": 0.334708571434021,
      "learning_rate": 0.00017115882117615683,
      "loss": 0.7103,
      "step": 5412
    },
    {
      "epoch": 0.43304,
      "grad_norm": 0.38761377334594727,
      "learning_rate": 0.00017115348713161757,
      "loss": 0.7004,
      "step": 5413
    },
    {
      "epoch": 0.43312,
      "grad_norm": 0.27589794993400574,
      "learning_rate": 0.0001711481530870783,
      "loss": 0.5434,
      "step": 5414
    },
    {
      "epoch": 0.4332,
      "grad_norm": 0.48910269141197205,
      "learning_rate": 0.00017114281904253903,
      "loss": 0.6312,
      "step": 5415
    },
    {
      "epoch": 0.43328,
      "grad_norm": 0.4575088918209076,
      "learning_rate": 0.00017113748499799974,
      "loss": 0.96,
      "step": 5416
    },
    {
      "epoch": 0.43336,
      "grad_norm": 0.5034757852554321,
      "learning_rate": 0.00017113215095346048,
      "loss": 0.8697,
      "step": 5417
    },
    {
      "epoch": 0.43344,
      "grad_norm": 0.3653506636619568,
      "learning_rate": 0.0001711268169089212,
      "loss": 0.6505,
      "step": 5418
    },
    {
      "epoch": 0.43352,
      "grad_norm": 0.4164965748786926,
      "learning_rate": 0.00017112148286438193,
      "loss": 0.9216,
      "step": 5419
    },
    {
      "epoch": 0.4336,
      "grad_norm": 0.34780415892601013,
      "learning_rate": 0.00017111614881984267,
      "loss": 0.5217,
      "step": 5420
    },
    {
      "epoch": 0.43368,
      "grad_norm": 0.3797891438007355,
      "learning_rate": 0.00017111081477530339,
      "loss": 0.5594,
      "step": 5421
    },
    {
      "epoch": 0.43376,
      "grad_norm": 0.31378281116485596,
      "learning_rate": 0.00017110548073076412,
      "loss": 1.0196,
      "step": 5422
    },
    {
      "epoch": 0.43384,
      "grad_norm": 0.3307196795940399,
      "learning_rate": 0.00017110014668622484,
      "loss": 0.5508,
      "step": 5423
    },
    {
      "epoch": 0.43392,
      "grad_norm": 0.45781466364860535,
      "learning_rate": 0.00017109481264168558,
      "loss": 0.9029,
      "step": 5424
    },
    {
      "epoch": 0.434,
      "grad_norm": 0.34183269739151,
      "learning_rate": 0.0001710894785971463,
      "loss": 0.9758,
      "step": 5425
    },
    {
      "epoch": 0.43408,
      "grad_norm": 0.42637550830841064,
      "learning_rate": 0.00017108414455260703,
      "loss": 0.709,
      "step": 5426
    },
    {
      "epoch": 0.43416,
      "grad_norm": 0.31847676634788513,
      "learning_rate": 0.00017107881050806774,
      "loss": 0.6039,
      "step": 5427
    },
    {
      "epoch": 0.43424,
      "grad_norm": 0.26056984066963196,
      "learning_rate": 0.00017107347646352848,
      "loss": 0.8669,
      "step": 5428
    },
    {
      "epoch": 0.43432,
      "grad_norm": 0.4690410792827606,
      "learning_rate": 0.0001710681424189892,
      "loss": 0.7118,
      "step": 5429
    },
    {
      "epoch": 0.4344,
      "grad_norm": 0.4334600567817688,
      "learning_rate": 0.00017106280837444994,
      "loss": 0.7811,
      "step": 5430
    },
    {
      "epoch": 0.43448,
      "grad_norm": 0.5097971558570862,
      "learning_rate": 0.00017105747432991065,
      "loss": 0.4979,
      "step": 5431
    },
    {
      "epoch": 0.43456,
      "grad_norm": 0.4724022150039673,
      "learning_rate": 0.0001710521402853714,
      "loss": 0.8405,
      "step": 5432
    },
    {
      "epoch": 0.43464,
      "grad_norm": 0.3215419054031372,
      "learning_rate": 0.0001710468062408321,
      "loss": 0.6617,
      "step": 5433
    },
    {
      "epoch": 0.43472,
      "grad_norm": 0.5301048755645752,
      "learning_rate": 0.00017104147219629284,
      "loss": 0.9496,
      "step": 5434
    },
    {
      "epoch": 0.4348,
      "grad_norm": 0.30333268642425537,
      "learning_rate": 0.00017103613815175355,
      "loss": 0.5972,
      "step": 5435
    },
    {
      "epoch": 0.43488,
      "grad_norm": 0.43403008580207825,
      "learning_rate": 0.0001710308041072143,
      "loss": 1.0661,
      "step": 5436
    },
    {
      "epoch": 0.43496,
      "grad_norm": 0.5020341277122498,
      "learning_rate": 0.00017102547006267503,
      "loss": 1.0676,
      "step": 5437
    },
    {
      "epoch": 0.43504,
      "grad_norm": 0.38310471177101135,
      "learning_rate": 0.00017102013601813575,
      "loss": 0.7995,
      "step": 5438
    },
    {
      "epoch": 0.43512,
      "grad_norm": 0.3625934422016144,
      "learning_rate": 0.00017101480197359649,
      "loss": 0.9686,
      "step": 5439
    },
    {
      "epoch": 0.4352,
      "grad_norm": 0.24132777750492096,
      "learning_rate": 0.0001710094679290572,
      "loss": 0.7117,
      "step": 5440
    },
    {
      "epoch": 0.43528,
      "grad_norm": 0.3640701472759247,
      "learning_rate": 0.00017100413388451794,
      "loss": 0.523,
      "step": 5441
    },
    {
      "epoch": 0.43536,
      "grad_norm": 0.3774835467338562,
      "learning_rate": 0.00017099879983997865,
      "loss": 1.2486,
      "step": 5442
    },
    {
      "epoch": 0.43544,
      "grad_norm": 0.43901294469833374,
      "learning_rate": 0.0001709934657954394,
      "loss": 0.8887,
      "step": 5443
    },
    {
      "epoch": 0.43552,
      "grad_norm": 0.42453163862228394,
      "learning_rate": 0.00017098813175090013,
      "loss": 0.9768,
      "step": 5444
    },
    {
      "epoch": 0.4356,
      "grad_norm": 0.4049890637397766,
      "learning_rate": 0.00017098279770636084,
      "loss": 0.7304,
      "step": 5445
    },
    {
      "epoch": 0.43568,
      "grad_norm": 0.27437520027160645,
      "learning_rate": 0.00017097746366182158,
      "loss": 0.5927,
      "step": 5446
    },
    {
      "epoch": 0.43576,
      "grad_norm": 0.45581871271133423,
      "learning_rate": 0.0001709721296172823,
      "loss": 1.0586,
      "step": 5447
    },
    {
      "epoch": 0.43584,
      "grad_norm": 0.39242425560951233,
      "learning_rate": 0.00017096679557274304,
      "loss": 0.5828,
      "step": 5448
    },
    {
      "epoch": 0.43592,
      "grad_norm": 0.31972795724868774,
      "learning_rate": 0.00017096146152820375,
      "loss": 0.5477,
      "step": 5449
    },
    {
      "epoch": 0.436,
      "grad_norm": 0.46795597672462463,
      "learning_rate": 0.0001709561274836645,
      "loss": 1.0576,
      "step": 5450
    },
    {
      "epoch": 0.43608,
      "grad_norm": 0.3515172302722931,
      "learning_rate": 0.00017095079343912523,
      "loss": 1.0166,
      "step": 5451
    },
    {
      "epoch": 0.43616,
      "grad_norm": 0.40185099840164185,
      "learning_rate": 0.00017094545939458594,
      "loss": 0.7286,
      "step": 5452
    },
    {
      "epoch": 0.43624,
      "grad_norm": 0.47569307684898376,
      "learning_rate": 0.00017094012535004668,
      "loss": 0.9197,
      "step": 5453
    },
    {
      "epoch": 0.43632,
      "grad_norm": 0.3404146730899811,
      "learning_rate": 0.0001709347913055074,
      "loss": 0.574,
      "step": 5454
    },
    {
      "epoch": 0.4364,
      "grad_norm": 0.4013937711715698,
      "learning_rate": 0.00017092945726096813,
      "loss": 1.1073,
      "step": 5455
    },
    {
      "epoch": 0.43648,
      "grad_norm": 0.32985907793045044,
      "learning_rate": 0.00017092412321642885,
      "loss": 0.5235,
      "step": 5456
    },
    {
      "epoch": 0.43656,
      "grad_norm": 0.532765805721283,
      "learning_rate": 0.00017091878917188959,
      "loss": 1.2419,
      "step": 5457
    },
    {
      "epoch": 0.43664,
      "grad_norm": 0.3727361559867859,
      "learning_rate": 0.00017091345512735033,
      "loss": 0.8014,
      "step": 5458
    },
    {
      "epoch": 0.43672,
      "grad_norm": 0.4215126037597656,
      "learning_rate": 0.00017090812108281104,
      "loss": 0.7332,
      "step": 5459
    },
    {
      "epoch": 0.4368,
      "grad_norm": 0.3108595609664917,
      "learning_rate": 0.00017090278703827178,
      "loss": 0.7041,
      "step": 5460
    },
    {
      "epoch": 0.43688,
      "grad_norm": 0.40144091844558716,
      "learning_rate": 0.0001708974529937325,
      "loss": 0.9204,
      "step": 5461
    },
    {
      "epoch": 0.43696,
      "grad_norm": 0.5392239689826965,
      "learning_rate": 0.00017089211894919323,
      "loss": 0.8328,
      "step": 5462
    },
    {
      "epoch": 0.43704,
      "grad_norm": 0.42280858755111694,
      "learning_rate": 0.00017088678490465394,
      "loss": 1.0517,
      "step": 5463
    },
    {
      "epoch": 0.43712,
      "grad_norm": 0.37284713983535767,
      "learning_rate": 0.00017088145086011468,
      "loss": 0.6721,
      "step": 5464
    },
    {
      "epoch": 0.4372,
      "grad_norm": 0.3586730360984802,
      "learning_rate": 0.00017087611681557542,
      "loss": 0.8248,
      "step": 5465
    },
    {
      "epoch": 0.43728,
      "grad_norm": 0.48320236802101135,
      "learning_rate": 0.00017087078277103614,
      "loss": 0.9008,
      "step": 5466
    },
    {
      "epoch": 0.43736,
      "grad_norm": 0.34463974833488464,
      "learning_rate": 0.00017086544872649688,
      "loss": 0.6955,
      "step": 5467
    },
    {
      "epoch": 0.43744,
      "grad_norm": 0.33940476179122925,
      "learning_rate": 0.0001708601146819576,
      "loss": 0.755,
      "step": 5468
    },
    {
      "epoch": 0.43752,
      "grad_norm": 0.3998878598213196,
      "learning_rate": 0.00017085478063741833,
      "loss": 0.9252,
      "step": 5469
    },
    {
      "epoch": 0.4376,
      "grad_norm": 0.3841701149940491,
      "learning_rate": 0.00017084944659287904,
      "loss": 0.8055,
      "step": 5470
    },
    {
      "epoch": 0.43768,
      "grad_norm": 0.45750415325164795,
      "learning_rate": 0.00017084411254833978,
      "loss": 0.6581,
      "step": 5471
    },
    {
      "epoch": 0.43776,
      "grad_norm": 0.393541157245636,
      "learning_rate": 0.00017083877850380052,
      "loss": 0.7429,
      "step": 5472
    },
    {
      "epoch": 0.43784,
      "grad_norm": 0.3613515794277191,
      "learning_rate": 0.00017083344445926123,
      "loss": 0.513,
      "step": 5473
    },
    {
      "epoch": 0.43792,
      "grad_norm": 0.3683450222015381,
      "learning_rate": 0.00017082811041472197,
      "loss": 0.6673,
      "step": 5474
    },
    {
      "epoch": 0.438,
      "grad_norm": 0.3684106469154358,
      "learning_rate": 0.00017082277637018269,
      "loss": 1.1768,
      "step": 5475
    },
    {
      "epoch": 0.43808,
      "grad_norm": 0.38203734159469604,
      "learning_rate": 0.00017081744232564343,
      "loss": 0.6916,
      "step": 5476
    },
    {
      "epoch": 0.43816,
      "grad_norm": 0.35571447014808655,
      "learning_rate": 0.00017081210828110417,
      "loss": 1.1243,
      "step": 5477
    },
    {
      "epoch": 0.43824,
      "grad_norm": 0.3808983564376831,
      "learning_rate": 0.00017080677423656488,
      "loss": 0.7199,
      "step": 5478
    },
    {
      "epoch": 0.43832,
      "grad_norm": 0.48357799649238586,
      "learning_rate": 0.00017080144019202562,
      "loss": 0.8934,
      "step": 5479
    },
    {
      "epoch": 0.4384,
      "grad_norm": 0.4482925236225128,
      "learning_rate": 0.00017079610614748633,
      "loss": 1.1222,
      "step": 5480
    },
    {
      "epoch": 0.43848,
      "grad_norm": 0.3651987910270691,
      "learning_rate": 0.00017079077210294707,
      "loss": 1.2634,
      "step": 5481
    },
    {
      "epoch": 0.43856,
      "grad_norm": 0.32797372341156006,
      "learning_rate": 0.00017078543805840778,
      "loss": 1.0447,
      "step": 5482
    },
    {
      "epoch": 0.43864,
      "grad_norm": 0.35869815945625305,
      "learning_rate": 0.00017078010401386852,
      "loss": 0.7827,
      "step": 5483
    },
    {
      "epoch": 0.43872,
      "grad_norm": 0.34875714778900146,
      "learning_rate": 0.00017077476996932926,
      "loss": 0.7853,
      "step": 5484
    },
    {
      "epoch": 0.4388,
      "grad_norm": 0.47066009044647217,
      "learning_rate": 0.00017076943592478998,
      "loss": 0.8069,
      "step": 5485
    },
    {
      "epoch": 0.43888,
      "grad_norm": 0.3131355345249176,
      "learning_rate": 0.00017076410188025072,
      "loss": 0.6101,
      "step": 5486
    },
    {
      "epoch": 0.43896,
      "grad_norm": 0.43058323860168457,
      "learning_rate": 0.00017075876783571143,
      "loss": 0.7866,
      "step": 5487
    },
    {
      "epoch": 0.43904,
      "grad_norm": 0.36404410004615784,
      "learning_rate": 0.00017075343379117217,
      "loss": 0.6835,
      "step": 5488
    },
    {
      "epoch": 0.43912,
      "grad_norm": 0.40923696756362915,
      "learning_rate": 0.00017074809974663288,
      "loss": 0.7996,
      "step": 5489
    },
    {
      "epoch": 0.4392,
      "grad_norm": 0.36631402373313904,
      "learning_rate": 0.00017074276570209362,
      "loss": 0.737,
      "step": 5490
    },
    {
      "epoch": 0.43928,
      "grad_norm": 0.4261589050292969,
      "learning_rate": 0.00017073743165755436,
      "loss": 0.8684,
      "step": 5491
    },
    {
      "epoch": 0.43936,
      "grad_norm": 0.4012772738933563,
      "learning_rate": 0.00017073209761301507,
      "loss": 0.7707,
      "step": 5492
    },
    {
      "epoch": 0.43944,
      "grad_norm": 0.35977110266685486,
      "learning_rate": 0.00017072676356847581,
      "loss": 0.7477,
      "step": 5493
    },
    {
      "epoch": 0.43952,
      "grad_norm": 0.32875388860702515,
      "learning_rate": 0.00017072142952393653,
      "loss": 0.5519,
      "step": 5494
    },
    {
      "epoch": 0.4396,
      "grad_norm": 0.4059532582759857,
      "learning_rate": 0.00017071609547939727,
      "loss": 0.7115,
      "step": 5495
    },
    {
      "epoch": 0.43968,
      "grad_norm": 0.3752444088459015,
      "learning_rate": 0.00017071076143485798,
      "loss": 0.593,
      "step": 5496
    },
    {
      "epoch": 0.43976,
      "grad_norm": 0.30515411496162415,
      "learning_rate": 0.00017070542739031872,
      "loss": 0.759,
      "step": 5497
    },
    {
      "epoch": 0.43984,
      "grad_norm": 0.3781038522720337,
      "learning_rate": 0.00017070009334577946,
      "loss": 0.7444,
      "step": 5498
    },
    {
      "epoch": 0.43992,
      "grad_norm": 0.3209350109100342,
      "learning_rate": 0.00017069475930124017,
      "loss": 0.8658,
      "step": 5499
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.44933369755744934,
      "learning_rate": 0.0001706894252567009,
      "loss": 0.8597,
      "step": 5500
    },
    {
      "epoch": 0.44008,
      "grad_norm": 0.4848630428314209,
      "learning_rate": 0.00017068409121216162,
      "loss": 0.9734,
      "step": 5501
    },
    {
      "epoch": 0.44016,
      "grad_norm": 0.3561289310455322,
      "learning_rate": 0.00017067875716762236,
      "loss": 0.8923,
      "step": 5502
    },
    {
      "epoch": 0.44024,
      "grad_norm": 0.34738242626190186,
      "learning_rate": 0.00017067342312308308,
      "loss": 0.5829,
      "step": 5503
    },
    {
      "epoch": 0.44032,
      "grad_norm": 0.33376508951187134,
      "learning_rate": 0.00017066808907854382,
      "loss": 0.7344,
      "step": 5504
    },
    {
      "epoch": 0.4404,
      "grad_norm": 0.33423668146133423,
      "learning_rate": 0.00017066275503400456,
      "loss": 0.628,
      "step": 5505
    },
    {
      "epoch": 0.44048,
      "grad_norm": 0.4224252700805664,
      "learning_rate": 0.00017065742098946527,
      "loss": 0.8648,
      "step": 5506
    },
    {
      "epoch": 0.44056,
      "grad_norm": 0.3725473880767822,
      "learning_rate": 0.000170652086944926,
      "loss": 0.9908,
      "step": 5507
    },
    {
      "epoch": 0.44064,
      "grad_norm": 0.44102707505226135,
      "learning_rate": 0.00017064675290038672,
      "loss": 0.8026,
      "step": 5508
    },
    {
      "epoch": 0.44072,
      "grad_norm": 0.34311506152153015,
      "learning_rate": 0.00017064141885584746,
      "loss": 0.8525,
      "step": 5509
    },
    {
      "epoch": 0.4408,
      "grad_norm": 0.3920186460018158,
      "learning_rate": 0.00017063608481130817,
      "loss": 0.8705,
      "step": 5510
    },
    {
      "epoch": 0.44088,
      "grad_norm": 0.366610050201416,
      "learning_rate": 0.00017063075076676891,
      "loss": 0.6596,
      "step": 5511
    },
    {
      "epoch": 0.44096,
      "grad_norm": 0.3505793511867523,
      "learning_rate": 0.00017062541672222965,
      "loss": 0.7548,
      "step": 5512
    },
    {
      "epoch": 0.44104,
      "grad_norm": 0.3284051716327667,
      "learning_rate": 0.00017062008267769037,
      "loss": 0.6279,
      "step": 5513
    },
    {
      "epoch": 0.44112,
      "grad_norm": 0.35875391960144043,
      "learning_rate": 0.0001706147486331511,
      "loss": 0.7686,
      "step": 5514
    },
    {
      "epoch": 0.4412,
      "grad_norm": 0.2693282663822174,
      "learning_rate": 0.00017060941458861182,
      "loss": 1.0113,
      "step": 5515
    },
    {
      "epoch": 0.44128,
      "grad_norm": 0.32407134771347046,
      "learning_rate": 0.00017060408054407256,
      "loss": 0.782,
      "step": 5516
    },
    {
      "epoch": 0.44136,
      "grad_norm": 0.5438771843910217,
      "learning_rate": 0.00017059874649953327,
      "loss": 0.9271,
      "step": 5517
    },
    {
      "epoch": 0.44144,
      "grad_norm": 0.5317896604537964,
      "learning_rate": 0.000170593412454994,
      "loss": 0.9475,
      "step": 5518
    },
    {
      "epoch": 0.44152,
      "grad_norm": 0.3877860903739929,
      "learning_rate": 0.00017058807841045475,
      "loss": 0.8055,
      "step": 5519
    },
    {
      "epoch": 0.4416,
      "grad_norm": 0.37620437145233154,
      "learning_rate": 0.00017058274436591546,
      "loss": 0.734,
      "step": 5520
    },
    {
      "epoch": 0.44168,
      "grad_norm": 0.40599122643470764,
      "learning_rate": 0.0001705774103213762,
      "loss": 0.8111,
      "step": 5521
    },
    {
      "epoch": 0.44176,
      "grad_norm": 0.44782236218452454,
      "learning_rate": 0.00017057207627683692,
      "loss": 0.5765,
      "step": 5522
    },
    {
      "epoch": 0.44184,
      "grad_norm": 0.3740035891532898,
      "learning_rate": 0.00017056674223229766,
      "loss": 0.8136,
      "step": 5523
    },
    {
      "epoch": 0.44192,
      "grad_norm": 0.29914775490760803,
      "learning_rate": 0.0001705614081877584,
      "loss": 0.5987,
      "step": 5524
    },
    {
      "epoch": 0.442,
      "grad_norm": 0.4073583483695984,
      "learning_rate": 0.0001705560741432191,
      "loss": 0.7772,
      "step": 5525
    },
    {
      "epoch": 0.44208,
      "grad_norm": 0.379452109336853,
      "learning_rate": 0.00017055074009867985,
      "loss": 0.9031,
      "step": 5526
    },
    {
      "epoch": 0.44216,
      "grad_norm": 0.372884064912796,
      "learning_rate": 0.00017054540605414056,
      "loss": 0.8816,
      "step": 5527
    },
    {
      "epoch": 0.44224,
      "grad_norm": 0.4148860573768616,
      "learning_rate": 0.0001705400720096013,
      "loss": 0.846,
      "step": 5528
    },
    {
      "epoch": 0.44232,
      "grad_norm": 0.4239623248577118,
      "learning_rate": 0.00017053473796506201,
      "loss": 0.9624,
      "step": 5529
    },
    {
      "epoch": 0.4424,
      "grad_norm": 0.3042215406894684,
      "learning_rate": 0.00017052940392052275,
      "loss": 0.7568,
      "step": 5530
    },
    {
      "epoch": 0.44248,
      "grad_norm": 0.3706419765949249,
      "learning_rate": 0.0001705240698759835,
      "loss": 0.6422,
      "step": 5531
    },
    {
      "epoch": 0.44256,
      "grad_norm": 0.4711874723434448,
      "learning_rate": 0.0001705187358314442,
      "loss": 0.9011,
      "step": 5532
    },
    {
      "epoch": 0.44264,
      "grad_norm": 0.4067491590976715,
      "learning_rate": 0.00017051340178690495,
      "loss": 0.997,
      "step": 5533
    },
    {
      "epoch": 0.44272,
      "grad_norm": 0.3017062246799469,
      "learning_rate": 0.00017050806774236566,
      "loss": 0.9052,
      "step": 5534
    },
    {
      "epoch": 0.4428,
      "grad_norm": 0.3040798306465149,
      "learning_rate": 0.0001705027336978264,
      "loss": 0.3954,
      "step": 5535
    },
    {
      "epoch": 0.44288,
      "grad_norm": 0.3918868899345398,
      "learning_rate": 0.0001704973996532871,
      "loss": 0.7499,
      "step": 5536
    },
    {
      "epoch": 0.44296,
      "grad_norm": 0.34597155451774597,
      "learning_rate": 0.00017049206560874785,
      "loss": 1.1691,
      "step": 5537
    },
    {
      "epoch": 0.44304,
      "grad_norm": 0.31959110498428345,
      "learning_rate": 0.0001704867315642086,
      "loss": 0.9415,
      "step": 5538
    },
    {
      "epoch": 0.44312,
      "grad_norm": 0.3652060329914093,
      "learning_rate": 0.0001704813975196693,
      "loss": 0.8049,
      "step": 5539
    },
    {
      "epoch": 0.4432,
      "grad_norm": 0.41121411323547363,
      "learning_rate": 0.00017047606347513004,
      "loss": 0.9538,
      "step": 5540
    },
    {
      "epoch": 0.44328,
      "grad_norm": 0.33827170729637146,
      "learning_rate": 0.00017047072943059076,
      "loss": 1.0683,
      "step": 5541
    },
    {
      "epoch": 0.44336,
      "grad_norm": 0.5015279650688171,
      "learning_rate": 0.0001704653953860515,
      "loss": 0.9192,
      "step": 5542
    },
    {
      "epoch": 0.44344,
      "grad_norm": 0.3781043291091919,
      "learning_rate": 0.0001704600613415122,
      "loss": 0.7837,
      "step": 5543
    },
    {
      "epoch": 0.44352,
      "grad_norm": 0.403012752532959,
      "learning_rate": 0.00017045472729697295,
      "loss": 0.6827,
      "step": 5544
    },
    {
      "epoch": 0.4436,
      "grad_norm": 0.39139166474342346,
      "learning_rate": 0.00017044939325243366,
      "loss": 0.8017,
      "step": 5545
    },
    {
      "epoch": 0.44368,
      "grad_norm": 0.392252117395401,
      "learning_rate": 0.0001704440592078944,
      "loss": 0.8179,
      "step": 5546
    },
    {
      "epoch": 0.44376,
      "grad_norm": 0.39346858859062195,
      "learning_rate": 0.00017043872516335512,
      "loss": 0.9822,
      "step": 5547
    },
    {
      "epoch": 0.44384,
      "grad_norm": 0.4657106399536133,
      "learning_rate": 0.00017043339111881585,
      "loss": 0.9832,
      "step": 5548
    },
    {
      "epoch": 0.44392,
      "grad_norm": 0.4261366128921509,
      "learning_rate": 0.0001704280570742766,
      "loss": 0.8361,
      "step": 5549
    },
    {
      "epoch": 0.444,
      "grad_norm": 0.5279421806335449,
      "learning_rate": 0.0001704227230297373,
      "loss": 1.1233,
      "step": 5550
    },
    {
      "epoch": 0.44408,
      "grad_norm": 0.4090079069137573,
      "learning_rate": 0.00017041738898519805,
      "loss": 0.8092,
      "step": 5551
    },
    {
      "epoch": 0.44416,
      "grad_norm": 0.4076101779937744,
      "learning_rate": 0.00017041205494065876,
      "loss": 0.8454,
      "step": 5552
    },
    {
      "epoch": 0.44424,
      "grad_norm": 0.361983984708786,
      "learning_rate": 0.0001704067208961195,
      "loss": 0.7685,
      "step": 5553
    },
    {
      "epoch": 0.44432,
      "grad_norm": 0.319449245929718,
      "learning_rate": 0.0001704013868515802,
      "loss": 0.5906,
      "step": 5554
    },
    {
      "epoch": 0.4444,
      "grad_norm": 0.43022897839546204,
      "learning_rate": 0.00017039605280704095,
      "loss": 0.7439,
      "step": 5555
    },
    {
      "epoch": 0.44448,
      "grad_norm": 0.3659500777721405,
      "learning_rate": 0.00017039071876250167,
      "loss": 0.8717,
      "step": 5556
    },
    {
      "epoch": 0.44456,
      "grad_norm": 0.28890690207481384,
      "learning_rate": 0.0001703853847179624,
      "loss": 0.8947,
      "step": 5557
    },
    {
      "epoch": 0.44464,
      "grad_norm": 0.3701373040676117,
      "learning_rate": 0.00017038005067342312,
      "loss": 0.797,
      "step": 5558
    },
    {
      "epoch": 0.44472,
      "grad_norm": 0.3969142436981201,
      "learning_rate": 0.00017037471662888386,
      "loss": 0.7882,
      "step": 5559
    },
    {
      "epoch": 0.4448,
      "grad_norm": 0.3054351508617401,
      "learning_rate": 0.00017036938258434457,
      "loss": 0.4793,
      "step": 5560
    },
    {
      "epoch": 0.44488,
      "grad_norm": 0.46673616766929626,
      "learning_rate": 0.0001703640485398053,
      "loss": 0.7748,
      "step": 5561
    },
    {
      "epoch": 0.44496,
      "grad_norm": 0.38955363631248474,
      "learning_rate": 0.00017035871449526602,
      "loss": 0.9106,
      "step": 5562
    },
    {
      "epoch": 0.44504,
      "grad_norm": 0.4345884323120117,
      "learning_rate": 0.00017035338045072676,
      "loss": 0.7997,
      "step": 5563
    },
    {
      "epoch": 0.44512,
      "grad_norm": 0.3269849419593811,
      "learning_rate": 0.00017034804640618748,
      "loss": 0.9599,
      "step": 5564
    },
    {
      "epoch": 0.4452,
      "grad_norm": 0.3465007245540619,
      "learning_rate": 0.00017034271236164822,
      "loss": 0.7568,
      "step": 5565
    },
    {
      "epoch": 0.44528,
      "grad_norm": 0.39662981033325195,
      "learning_rate": 0.00017033737831710896,
      "loss": 0.5574,
      "step": 5566
    },
    {
      "epoch": 0.44536,
      "grad_norm": 0.3631998598575592,
      "learning_rate": 0.00017033204427256967,
      "loss": 0.6605,
      "step": 5567
    },
    {
      "epoch": 0.44544,
      "grad_norm": 0.4490339457988739,
      "learning_rate": 0.0001703267102280304,
      "loss": 0.7392,
      "step": 5568
    },
    {
      "epoch": 0.44552,
      "grad_norm": 0.3513402044773102,
      "learning_rate": 0.00017032137618349112,
      "loss": 0.8018,
      "step": 5569
    },
    {
      "epoch": 0.4456,
      "grad_norm": 0.3585966229438782,
      "learning_rate": 0.00017031604213895186,
      "loss": 0.4566,
      "step": 5570
    },
    {
      "epoch": 0.44568,
      "grad_norm": 0.4249609410762787,
      "learning_rate": 0.00017031070809441257,
      "loss": 0.6922,
      "step": 5571
    },
    {
      "epoch": 0.44576,
      "grad_norm": 0.34445273876190186,
      "learning_rate": 0.0001703053740498733,
      "loss": 0.5551,
      "step": 5572
    },
    {
      "epoch": 0.44584,
      "grad_norm": 0.38690415024757385,
      "learning_rate": 0.00017030004000533405,
      "loss": 0.9847,
      "step": 5573
    },
    {
      "epoch": 0.44592,
      "grad_norm": 0.36498886346817017,
      "learning_rate": 0.00017029470596079477,
      "loss": 0.6041,
      "step": 5574
    },
    {
      "epoch": 0.446,
      "grad_norm": 0.46350958943367004,
      "learning_rate": 0.0001702893719162555,
      "loss": 0.8664,
      "step": 5575
    },
    {
      "epoch": 0.44608,
      "grad_norm": 0.3529062271118164,
      "learning_rate": 0.00017028403787171622,
      "loss": 0.5557,
      "step": 5576
    },
    {
      "epoch": 0.44616,
      "grad_norm": 0.6342979073524475,
      "learning_rate": 0.00017027870382717696,
      "loss": 0.829,
      "step": 5577
    },
    {
      "epoch": 0.44624,
      "grad_norm": 0.37227877974510193,
      "learning_rate": 0.0001702733697826377,
      "loss": 0.7942,
      "step": 5578
    },
    {
      "epoch": 0.44632,
      "grad_norm": 0.3571690320968628,
      "learning_rate": 0.0001702680357380984,
      "loss": 1.2249,
      "step": 5579
    },
    {
      "epoch": 0.4464,
      "grad_norm": 0.4060366749763489,
      "learning_rate": 0.00017026270169355915,
      "loss": 0.6349,
      "step": 5580
    },
    {
      "epoch": 0.44648,
      "grad_norm": 0.37960025668144226,
      "learning_rate": 0.00017025736764901986,
      "loss": 0.6732,
      "step": 5581
    },
    {
      "epoch": 0.44656,
      "grad_norm": 0.34061190485954285,
      "learning_rate": 0.0001702520336044806,
      "loss": 0.8882,
      "step": 5582
    },
    {
      "epoch": 0.44664,
      "grad_norm": 0.3433324098587036,
      "learning_rate": 0.00017024669955994132,
      "loss": 0.5738,
      "step": 5583
    },
    {
      "epoch": 0.44672,
      "grad_norm": 0.5648616552352905,
      "learning_rate": 0.00017024136551540206,
      "loss": 0.9099,
      "step": 5584
    },
    {
      "epoch": 0.4468,
      "grad_norm": 0.3987882137298584,
      "learning_rate": 0.0001702360314708628,
      "loss": 0.6955,
      "step": 5585
    },
    {
      "epoch": 0.44688,
      "grad_norm": 0.2894172966480255,
      "learning_rate": 0.0001702306974263235,
      "loss": 0.8422,
      "step": 5586
    },
    {
      "epoch": 0.44696,
      "grad_norm": 0.5120110511779785,
      "learning_rate": 0.00017022536338178425,
      "loss": 1.0041,
      "step": 5587
    },
    {
      "epoch": 0.44704,
      "grad_norm": 0.3650740683078766,
      "learning_rate": 0.00017022002933724496,
      "loss": 0.7447,
      "step": 5588
    },
    {
      "epoch": 0.44712,
      "grad_norm": 0.374493271112442,
      "learning_rate": 0.0001702146952927057,
      "loss": 0.9067,
      "step": 5589
    },
    {
      "epoch": 0.4472,
      "grad_norm": 0.34222909808158875,
      "learning_rate": 0.00017020936124816641,
      "loss": 0.5078,
      "step": 5590
    },
    {
      "epoch": 0.44728,
      "grad_norm": 0.4336300492286682,
      "learning_rate": 0.00017020402720362715,
      "loss": 0.687,
      "step": 5591
    },
    {
      "epoch": 0.44736,
      "grad_norm": 0.3956967294216156,
      "learning_rate": 0.0001701986931590879,
      "loss": 0.9522,
      "step": 5592
    },
    {
      "epoch": 0.44744,
      "grad_norm": 0.37824854254722595,
      "learning_rate": 0.0001701933591145486,
      "loss": 0.9844,
      "step": 5593
    },
    {
      "epoch": 0.44752,
      "grad_norm": 0.4051394462585449,
      "learning_rate": 0.00017018802507000935,
      "loss": 0.9621,
      "step": 5594
    },
    {
      "epoch": 0.4476,
      "grad_norm": 0.5139783620834351,
      "learning_rate": 0.00017018269102547006,
      "loss": 0.7726,
      "step": 5595
    },
    {
      "epoch": 0.44768,
      "grad_norm": 0.42028945684432983,
      "learning_rate": 0.0001701773569809308,
      "loss": 0.7031,
      "step": 5596
    },
    {
      "epoch": 0.44776,
      "grad_norm": 0.5114749073982239,
      "learning_rate": 0.0001701720229363915,
      "loss": 0.9161,
      "step": 5597
    },
    {
      "epoch": 0.44784,
      "grad_norm": 0.3147904872894287,
      "learning_rate": 0.00017016668889185225,
      "loss": 0.5209,
      "step": 5598
    },
    {
      "epoch": 0.44792,
      "grad_norm": 0.38044479489326477,
      "learning_rate": 0.000170161354847313,
      "loss": 0.7168,
      "step": 5599
    },
    {
      "epoch": 0.448,
      "grad_norm": 0.5034229755401611,
      "learning_rate": 0.0001701560208027737,
      "loss": 0.6537,
      "step": 5600
    },
    {
      "epoch": 0.44808,
      "grad_norm": 0.40026435256004333,
      "learning_rate": 0.00017015068675823444,
      "loss": 0.9108,
      "step": 5601
    },
    {
      "epoch": 0.44816,
      "grad_norm": 0.4525355100631714,
      "learning_rate": 0.00017014535271369516,
      "loss": 0.5981,
      "step": 5602
    },
    {
      "epoch": 0.44824,
      "grad_norm": 0.3028205633163452,
      "learning_rate": 0.0001701400186691559,
      "loss": 0.8438,
      "step": 5603
    },
    {
      "epoch": 0.44832,
      "grad_norm": 0.2702490985393524,
      "learning_rate": 0.0001701346846246166,
      "loss": 0.3451,
      "step": 5604
    },
    {
      "epoch": 0.4484,
      "grad_norm": 0.33819326758384705,
      "learning_rate": 0.00017012935058007735,
      "loss": 0.5854,
      "step": 5605
    },
    {
      "epoch": 0.44848,
      "grad_norm": 0.37683677673339844,
      "learning_rate": 0.0001701240165355381,
      "loss": 0.9556,
      "step": 5606
    },
    {
      "epoch": 0.44856,
      "grad_norm": 0.522104799747467,
      "learning_rate": 0.0001701186824909988,
      "loss": 0.6792,
      "step": 5607
    },
    {
      "epoch": 0.44864,
      "grad_norm": 0.41530942916870117,
      "learning_rate": 0.00017011334844645954,
      "loss": 0.8515,
      "step": 5608
    },
    {
      "epoch": 0.44872,
      "grad_norm": 0.46088141202926636,
      "learning_rate": 0.00017010801440192025,
      "loss": 0.6887,
      "step": 5609
    },
    {
      "epoch": 0.4488,
      "grad_norm": 0.3342978060245514,
      "learning_rate": 0.000170102680357381,
      "loss": 0.7376,
      "step": 5610
    },
    {
      "epoch": 0.44888,
      "grad_norm": 0.420962393283844,
      "learning_rate": 0.0001700973463128417,
      "loss": 0.902,
      "step": 5611
    },
    {
      "epoch": 0.44896,
      "grad_norm": 0.40934523940086365,
      "learning_rate": 0.00017009201226830245,
      "loss": 0.6457,
      "step": 5612
    },
    {
      "epoch": 0.44904,
      "grad_norm": 0.42585062980651855,
      "learning_rate": 0.00017008667822376319,
      "loss": 0.667,
      "step": 5613
    },
    {
      "epoch": 0.44912,
      "grad_norm": 0.37982723116874695,
      "learning_rate": 0.0001700813441792239,
      "loss": 0.8814,
      "step": 5614
    },
    {
      "epoch": 0.4492,
      "grad_norm": 0.30184102058410645,
      "learning_rate": 0.00017007601013468464,
      "loss": 0.7344,
      "step": 5615
    },
    {
      "epoch": 0.44928,
      "grad_norm": 0.37994155287742615,
      "learning_rate": 0.00017007067609014535,
      "loss": 0.7893,
      "step": 5616
    },
    {
      "epoch": 0.44936,
      "grad_norm": 0.38282087445259094,
      "learning_rate": 0.0001700653420456061,
      "loss": 0.9456,
      "step": 5617
    },
    {
      "epoch": 0.44944,
      "grad_norm": 0.43643903732299805,
      "learning_rate": 0.0001700600080010668,
      "loss": 0.8462,
      "step": 5618
    },
    {
      "epoch": 0.44952,
      "grad_norm": 0.4827268719673157,
      "learning_rate": 0.00017005467395652754,
      "loss": 0.8735,
      "step": 5619
    },
    {
      "epoch": 0.4496,
      "grad_norm": 0.3831128180027008,
      "learning_rate": 0.00017004933991198828,
      "loss": 0.7428,
      "step": 5620
    },
    {
      "epoch": 0.44968,
      "grad_norm": 0.5331843495368958,
      "learning_rate": 0.000170044005867449,
      "loss": 1.0439,
      "step": 5621
    },
    {
      "epoch": 0.44976,
      "grad_norm": 0.5524519681930542,
      "learning_rate": 0.00017003867182290974,
      "loss": 1.1302,
      "step": 5622
    },
    {
      "epoch": 0.44984,
      "grad_norm": 0.3815839886665344,
      "learning_rate": 0.00017003333777837045,
      "loss": 0.7325,
      "step": 5623
    },
    {
      "epoch": 0.44992,
      "grad_norm": 0.48575302958488464,
      "learning_rate": 0.0001700280037338312,
      "loss": 1.0133,
      "step": 5624
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.35774680972099304,
      "learning_rate": 0.00017002266968929193,
      "loss": 0.8383,
      "step": 5625
    },
    {
      "epoch": 0.45008,
      "grad_norm": 0.4208594858646393,
      "learning_rate": 0.00017001733564475264,
      "loss": 0.5616,
      "step": 5626
    },
    {
      "epoch": 0.45016,
      "grad_norm": 0.36558401584625244,
      "learning_rate": 0.00017001200160021338,
      "loss": 0.7103,
      "step": 5627
    },
    {
      "epoch": 0.45024,
      "grad_norm": 0.30968573689460754,
      "learning_rate": 0.0001700066675556741,
      "loss": 0.6389,
      "step": 5628
    },
    {
      "epoch": 0.45032,
      "grad_norm": 0.2664125859737396,
      "learning_rate": 0.00017000133351113483,
      "loss": 0.4708,
      "step": 5629
    },
    {
      "epoch": 0.4504,
      "grad_norm": 0.32853153347969055,
      "learning_rate": 0.00016999599946659555,
      "loss": 0.4861,
      "step": 5630
    },
    {
      "epoch": 0.45048,
      "grad_norm": 0.3835284113883972,
      "learning_rate": 0.0001699906654220563,
      "loss": 0.7677,
      "step": 5631
    },
    {
      "epoch": 0.45056,
      "grad_norm": 0.5856212377548218,
      "learning_rate": 0.00016998533137751703,
      "loss": 1.0139,
      "step": 5632
    },
    {
      "epoch": 0.45064,
      "grad_norm": 0.40143126249313354,
      "learning_rate": 0.00016997999733297774,
      "loss": 1.0786,
      "step": 5633
    },
    {
      "epoch": 0.45072,
      "grad_norm": 0.39560237526893616,
      "learning_rate": 0.00016997466328843848,
      "loss": 1.3742,
      "step": 5634
    },
    {
      "epoch": 0.4508,
      "grad_norm": 0.3573266863822937,
      "learning_rate": 0.0001699693292438992,
      "loss": 0.7276,
      "step": 5635
    },
    {
      "epoch": 0.45088,
      "grad_norm": 0.5027313828468323,
      "learning_rate": 0.00016996399519935993,
      "loss": 0.9011,
      "step": 5636
    },
    {
      "epoch": 0.45096,
      "grad_norm": 0.38755372166633606,
      "learning_rate": 0.00016995866115482064,
      "loss": 0.6921,
      "step": 5637
    },
    {
      "epoch": 0.45104,
      "grad_norm": 0.3799501061439514,
      "learning_rate": 0.00016995332711028138,
      "loss": 0.8171,
      "step": 5638
    },
    {
      "epoch": 0.45112,
      "grad_norm": 0.41784751415252686,
      "learning_rate": 0.00016994799306574212,
      "loss": 0.9119,
      "step": 5639
    },
    {
      "epoch": 0.4512,
      "grad_norm": 0.4126741886138916,
      "learning_rate": 0.00016994265902120284,
      "loss": 0.7126,
      "step": 5640
    },
    {
      "epoch": 0.45128,
      "grad_norm": 0.4229947030544281,
      "learning_rate": 0.00016993732497666358,
      "loss": 0.94,
      "step": 5641
    },
    {
      "epoch": 0.45136,
      "grad_norm": 0.43420034646987915,
      "learning_rate": 0.0001699319909321243,
      "loss": 0.8265,
      "step": 5642
    },
    {
      "epoch": 0.45144,
      "grad_norm": 0.49771708250045776,
      "learning_rate": 0.00016992665688758503,
      "loss": 0.6664,
      "step": 5643
    },
    {
      "epoch": 0.45152,
      "grad_norm": 0.3396826684474945,
      "learning_rate": 0.00016992132284304574,
      "loss": 0.8445,
      "step": 5644
    },
    {
      "epoch": 0.4516,
      "grad_norm": 0.4384091794490814,
      "learning_rate": 0.00016991598879850648,
      "loss": 0.6494,
      "step": 5645
    },
    {
      "epoch": 0.45168,
      "grad_norm": 0.3922569155693054,
      "learning_rate": 0.00016991065475396722,
      "loss": 1.1213,
      "step": 5646
    },
    {
      "epoch": 0.45176,
      "grad_norm": 0.33677250146865845,
      "learning_rate": 0.00016990532070942793,
      "loss": 0.7709,
      "step": 5647
    },
    {
      "epoch": 0.45184,
      "grad_norm": 0.329889178276062,
      "learning_rate": 0.00016989998666488867,
      "loss": 0.804,
      "step": 5648
    },
    {
      "epoch": 0.45192,
      "grad_norm": 0.4939168095588684,
      "learning_rate": 0.0001698946526203494,
      "loss": 0.8371,
      "step": 5649
    },
    {
      "epoch": 0.452,
      "grad_norm": 0.5597169399261475,
      "learning_rate": 0.00016988931857581013,
      "loss": 1.088,
      "step": 5650
    },
    {
      "epoch": 0.45208,
      "grad_norm": 0.3566450774669647,
      "learning_rate": 0.00016988398453127084,
      "loss": 0.6786,
      "step": 5651
    },
    {
      "epoch": 0.45216,
      "grad_norm": 0.5053348541259766,
      "learning_rate": 0.00016987865048673158,
      "loss": 0.7579,
      "step": 5652
    },
    {
      "epoch": 0.45224,
      "grad_norm": 0.5050205588340759,
      "learning_rate": 0.00016987331644219232,
      "loss": 0.9248,
      "step": 5653
    },
    {
      "epoch": 0.45232,
      "grad_norm": 0.46194207668304443,
      "learning_rate": 0.00016986798239765303,
      "loss": 0.8197,
      "step": 5654
    },
    {
      "epoch": 0.4524,
      "grad_norm": 0.4583951532840729,
      "learning_rate": 0.00016986264835311377,
      "loss": 0.9555,
      "step": 5655
    },
    {
      "epoch": 0.45248,
      "grad_norm": 0.4212396442890167,
      "learning_rate": 0.00016985731430857448,
      "loss": 0.6873,
      "step": 5656
    },
    {
      "epoch": 0.45256,
      "grad_norm": 0.45682528614997864,
      "learning_rate": 0.00016985198026403522,
      "loss": 0.8746,
      "step": 5657
    },
    {
      "epoch": 0.45264,
      "grad_norm": 0.3638022840023041,
      "learning_rate": 0.00016984664621949594,
      "loss": 0.6463,
      "step": 5658
    },
    {
      "epoch": 0.45272,
      "grad_norm": 0.338454931974411,
      "learning_rate": 0.00016984131217495668,
      "loss": 0.6804,
      "step": 5659
    },
    {
      "epoch": 0.4528,
      "grad_norm": 0.4570288062095642,
      "learning_rate": 0.00016983597813041742,
      "loss": 0.8289,
      "step": 5660
    },
    {
      "epoch": 0.45288,
      "grad_norm": 0.38824811577796936,
      "learning_rate": 0.00016983064408587813,
      "loss": 0.8073,
      "step": 5661
    },
    {
      "epoch": 0.45296,
      "grad_norm": 0.3939705789089203,
      "learning_rate": 0.00016982531004133887,
      "loss": 0.7805,
      "step": 5662
    },
    {
      "epoch": 0.45304,
      "grad_norm": 0.3979859948158264,
      "learning_rate": 0.00016981997599679958,
      "loss": 0.9804,
      "step": 5663
    },
    {
      "epoch": 0.45312,
      "grad_norm": 0.4586980938911438,
      "learning_rate": 0.00016981464195226032,
      "loss": 0.9178,
      "step": 5664
    },
    {
      "epoch": 0.4532,
      "grad_norm": 0.4913007318973541,
      "learning_rate": 0.00016980930790772103,
      "loss": 0.7444,
      "step": 5665
    },
    {
      "epoch": 0.45328,
      "grad_norm": 0.34777727723121643,
      "learning_rate": 0.00016980397386318177,
      "loss": 0.7929,
      "step": 5666
    },
    {
      "epoch": 0.45336,
      "grad_norm": 0.42797398567199707,
      "learning_rate": 0.00016979863981864251,
      "loss": 1.0991,
      "step": 5667
    },
    {
      "epoch": 0.45344,
      "grad_norm": 0.42835521697998047,
      "learning_rate": 0.00016979330577410323,
      "loss": 0.8366,
      "step": 5668
    },
    {
      "epoch": 0.45352,
      "grad_norm": 0.32750391960144043,
      "learning_rate": 0.00016978797172956397,
      "loss": 0.7066,
      "step": 5669
    },
    {
      "epoch": 0.4536,
      "grad_norm": 0.26449188590049744,
      "learning_rate": 0.00016978263768502468,
      "loss": 0.8388,
      "step": 5670
    },
    {
      "epoch": 0.45368,
      "grad_norm": 0.4294707775115967,
      "learning_rate": 0.00016977730364048542,
      "loss": 0.6512,
      "step": 5671
    },
    {
      "epoch": 0.45376,
      "grad_norm": 0.41787031292915344,
      "learning_rate": 0.00016977196959594613,
      "loss": 0.7681,
      "step": 5672
    },
    {
      "epoch": 0.45384,
      "grad_norm": 0.35470739006996155,
      "learning_rate": 0.00016976663555140687,
      "loss": 0.7647,
      "step": 5673
    },
    {
      "epoch": 0.45392,
      "grad_norm": 0.4285251200199127,
      "learning_rate": 0.00016976130150686759,
      "loss": 0.9349,
      "step": 5674
    },
    {
      "epoch": 0.454,
      "grad_norm": 0.4194049537181854,
      "learning_rate": 0.00016975596746232832,
      "loss": 0.6406,
      "step": 5675
    },
    {
      "epoch": 0.45408,
      "grad_norm": 0.34637096524238586,
      "learning_rate": 0.00016975063341778904,
      "loss": 0.9294,
      "step": 5676
    },
    {
      "epoch": 0.45416,
      "grad_norm": 0.3196318745613098,
      "learning_rate": 0.00016974529937324978,
      "loss": 0.7735,
      "step": 5677
    },
    {
      "epoch": 0.45424,
      "grad_norm": 0.40834489464759827,
      "learning_rate": 0.00016973996532871052,
      "loss": 0.6211,
      "step": 5678
    },
    {
      "epoch": 0.45432,
      "grad_norm": 0.3892609477043152,
      "learning_rate": 0.00016973463128417123,
      "loss": 0.8439,
      "step": 5679
    },
    {
      "epoch": 0.4544,
      "grad_norm": 0.37955421209335327,
      "learning_rate": 0.00016972929723963197,
      "loss": 0.7971,
      "step": 5680
    },
    {
      "epoch": 0.45448,
      "grad_norm": 0.38678908348083496,
      "learning_rate": 0.00016972396319509268,
      "loss": 0.7907,
      "step": 5681
    },
    {
      "epoch": 0.45456,
      "grad_norm": 0.44790366291999817,
      "learning_rate": 0.00016971862915055342,
      "loss": 0.7231,
      "step": 5682
    },
    {
      "epoch": 0.45464,
      "grad_norm": 0.3983011543750763,
      "learning_rate": 0.00016971329510601414,
      "loss": 0.8011,
      "step": 5683
    },
    {
      "epoch": 0.45472,
      "grad_norm": 0.6193169355392456,
      "learning_rate": 0.00016970796106147488,
      "loss": 0.6779,
      "step": 5684
    },
    {
      "epoch": 0.4548,
      "grad_norm": 0.3679691553115845,
      "learning_rate": 0.0001697026270169356,
      "loss": 0.9316,
      "step": 5685
    },
    {
      "epoch": 0.45488,
      "grad_norm": 0.34281125664711,
      "learning_rate": 0.00016969729297239633,
      "loss": 0.6832,
      "step": 5686
    },
    {
      "epoch": 0.45496,
      "grad_norm": 0.42694902420043945,
      "learning_rate": 0.00016969195892785704,
      "loss": 0.8629,
      "step": 5687
    },
    {
      "epoch": 0.45504,
      "grad_norm": 0.3450973629951477,
      "learning_rate": 0.00016968662488331778,
      "loss": 0.6474,
      "step": 5688
    },
    {
      "epoch": 0.45512,
      "grad_norm": 0.40271735191345215,
      "learning_rate": 0.0001696812908387785,
      "loss": 0.9034,
      "step": 5689
    },
    {
      "epoch": 0.4552,
      "grad_norm": 0.495786190032959,
      "learning_rate": 0.00016967595679423923,
      "loss": 0.5537,
      "step": 5690
    },
    {
      "epoch": 0.45528,
      "grad_norm": 0.3710266947746277,
      "learning_rate": 0.00016967062274969995,
      "loss": 0.5852,
      "step": 5691
    },
    {
      "epoch": 0.45536,
      "grad_norm": 0.44084882736206055,
      "learning_rate": 0.00016966528870516069,
      "loss": 0.7093,
      "step": 5692
    },
    {
      "epoch": 0.45544,
      "grad_norm": 0.459967702627182,
      "learning_rate": 0.00016965995466062143,
      "loss": 1.0874,
      "step": 5693
    },
    {
      "epoch": 0.45552,
      "grad_norm": 0.4428417384624481,
      "learning_rate": 0.00016965462061608214,
      "loss": 0.6745,
      "step": 5694
    },
    {
      "epoch": 0.4556,
      "grad_norm": 0.40853360295295715,
      "learning_rate": 0.00016964928657154288,
      "loss": 0.6963,
      "step": 5695
    },
    {
      "epoch": 0.45568,
      "grad_norm": 0.41768208146095276,
      "learning_rate": 0.0001696439525270036,
      "loss": 0.753,
      "step": 5696
    },
    {
      "epoch": 0.45576,
      "grad_norm": 0.32909953594207764,
      "learning_rate": 0.00016963861848246433,
      "loss": 0.8031,
      "step": 5697
    },
    {
      "epoch": 0.45584,
      "grad_norm": 0.4019732177257538,
      "learning_rate": 0.00016963328443792504,
      "loss": 0.7589,
      "step": 5698
    },
    {
      "epoch": 0.45592,
      "grad_norm": 0.5337046384811401,
      "learning_rate": 0.00016962795039338578,
      "loss": 0.9412,
      "step": 5699
    },
    {
      "epoch": 0.456,
      "grad_norm": 0.3412513732910156,
      "learning_rate": 0.00016962261634884652,
      "loss": 0.8593,
      "step": 5700
    },
    {
      "epoch": 0.45608,
      "grad_norm": 0.4023618698120117,
      "learning_rate": 0.00016961728230430724,
      "loss": 0.7621,
      "step": 5701
    },
    {
      "epoch": 0.45616,
      "grad_norm": 0.42050302028656006,
      "learning_rate": 0.00016961194825976798,
      "loss": 0.8549,
      "step": 5702
    },
    {
      "epoch": 0.45624,
      "grad_norm": 0.4300662577152252,
      "learning_rate": 0.0001696066142152287,
      "loss": 1.0829,
      "step": 5703
    },
    {
      "epoch": 0.45632,
      "grad_norm": 0.36791935563087463,
      "learning_rate": 0.00016960128017068943,
      "loss": 0.6744,
      "step": 5704
    },
    {
      "epoch": 0.4564,
      "grad_norm": 0.31032896041870117,
      "learning_rate": 0.00016959594612615014,
      "loss": 0.6988,
      "step": 5705
    },
    {
      "epoch": 0.45648,
      "grad_norm": 0.3697226941585541,
      "learning_rate": 0.00016959061208161088,
      "loss": 0.798,
      "step": 5706
    },
    {
      "epoch": 0.45656,
      "grad_norm": 0.38502493500709534,
      "learning_rate": 0.00016958527803707162,
      "loss": 1.0159,
      "step": 5707
    },
    {
      "epoch": 0.45664,
      "grad_norm": 0.4167308807373047,
      "learning_rate": 0.00016957994399253233,
      "loss": 0.7589,
      "step": 5708
    },
    {
      "epoch": 0.45672,
      "grad_norm": 0.3227614462375641,
      "learning_rate": 0.00016957460994799307,
      "loss": 0.6057,
      "step": 5709
    },
    {
      "epoch": 0.4568,
      "grad_norm": 0.40726497769355774,
      "learning_rate": 0.00016956927590345379,
      "loss": 0.8622,
      "step": 5710
    },
    {
      "epoch": 0.45688,
      "grad_norm": 0.3964962065219879,
      "learning_rate": 0.00016956394185891453,
      "loss": 0.8107,
      "step": 5711
    },
    {
      "epoch": 0.45696,
      "grad_norm": 0.4413396120071411,
      "learning_rate": 0.00016955860781437524,
      "loss": 0.7675,
      "step": 5712
    },
    {
      "epoch": 0.45704,
      "grad_norm": 0.3784877061843872,
      "learning_rate": 0.00016955327376983598,
      "loss": 0.4369,
      "step": 5713
    },
    {
      "epoch": 0.45712,
      "grad_norm": 0.3388471305370331,
      "learning_rate": 0.00016954793972529672,
      "loss": 0.8736,
      "step": 5714
    },
    {
      "epoch": 0.4572,
      "grad_norm": 0.39271366596221924,
      "learning_rate": 0.00016954260568075743,
      "loss": 0.8323,
      "step": 5715
    },
    {
      "epoch": 0.45728,
      "grad_norm": 0.47377943992614746,
      "learning_rate": 0.00016953727163621817,
      "loss": 0.8742,
      "step": 5716
    },
    {
      "epoch": 0.45736,
      "grad_norm": 0.4067842662334442,
      "learning_rate": 0.00016953193759167888,
      "loss": 0.6373,
      "step": 5717
    },
    {
      "epoch": 0.45744,
      "grad_norm": 0.4692898988723755,
      "learning_rate": 0.00016952660354713962,
      "loss": 0.9127,
      "step": 5718
    },
    {
      "epoch": 0.45752,
      "grad_norm": 0.47327420115470886,
      "learning_rate": 0.00016952126950260034,
      "loss": 0.7797,
      "step": 5719
    },
    {
      "epoch": 0.4576,
      "grad_norm": 0.41749507188796997,
      "learning_rate": 0.00016951593545806108,
      "loss": 0.715,
      "step": 5720
    },
    {
      "epoch": 0.45768,
      "grad_norm": 0.4543359577655792,
      "learning_rate": 0.00016951060141352182,
      "loss": 0.8311,
      "step": 5721
    },
    {
      "epoch": 0.45776,
      "grad_norm": 0.34798887372016907,
      "learning_rate": 0.00016950526736898253,
      "loss": 1.0222,
      "step": 5722
    },
    {
      "epoch": 0.45784,
      "grad_norm": 0.3283287584781647,
      "learning_rate": 0.00016949993332444327,
      "loss": 0.8038,
      "step": 5723
    },
    {
      "epoch": 0.45792,
      "grad_norm": 0.38183048367500305,
      "learning_rate": 0.00016949459927990398,
      "loss": 1.0635,
      "step": 5724
    },
    {
      "epoch": 0.458,
      "grad_norm": 0.3634709417819977,
      "learning_rate": 0.00016948926523536472,
      "loss": 0.8877,
      "step": 5725
    },
    {
      "epoch": 0.45808,
      "grad_norm": 0.45843270421028137,
      "learning_rate": 0.00016948393119082546,
      "loss": 0.9124,
      "step": 5726
    },
    {
      "epoch": 0.45816,
      "grad_norm": 0.5175939202308655,
      "learning_rate": 0.00016947859714628617,
      "loss": 0.7645,
      "step": 5727
    },
    {
      "epoch": 0.45824,
      "grad_norm": 0.44152504205703735,
      "learning_rate": 0.0001694732631017469,
      "loss": 0.7904,
      "step": 5728
    },
    {
      "epoch": 0.45832,
      "grad_norm": 0.4223681688308716,
      "learning_rate": 0.00016946792905720763,
      "loss": 0.723,
      "step": 5729
    },
    {
      "epoch": 0.4584,
      "grad_norm": 0.5186765193939209,
      "learning_rate": 0.00016946259501266837,
      "loss": 0.8944,
      "step": 5730
    },
    {
      "epoch": 0.45848,
      "grad_norm": 0.4167935252189636,
      "learning_rate": 0.00016945726096812908,
      "loss": 0.7574,
      "step": 5731
    },
    {
      "epoch": 0.45856,
      "grad_norm": 0.36109688878059387,
      "learning_rate": 0.00016945192692358982,
      "loss": 1.0406,
      "step": 5732
    },
    {
      "epoch": 0.45864,
      "grad_norm": 0.3657517731189728,
      "learning_rate": 0.00016944659287905056,
      "loss": 0.7086,
      "step": 5733
    },
    {
      "epoch": 0.45872,
      "grad_norm": 0.35581979155540466,
      "learning_rate": 0.00016944125883451127,
      "loss": 0.8969,
      "step": 5734
    },
    {
      "epoch": 0.4588,
      "grad_norm": 0.3107246160507202,
      "learning_rate": 0.000169435924789972,
      "loss": 0.5123,
      "step": 5735
    },
    {
      "epoch": 0.45888,
      "grad_norm": 0.5477519035339355,
      "learning_rate": 0.00016943059074543272,
      "loss": 0.9352,
      "step": 5736
    },
    {
      "epoch": 0.45896,
      "grad_norm": 0.36949923634529114,
      "learning_rate": 0.00016942525670089346,
      "loss": 0.9347,
      "step": 5737
    },
    {
      "epoch": 0.45904,
      "grad_norm": 0.3704075515270233,
      "learning_rate": 0.00016941992265635418,
      "loss": 1.165,
      "step": 5738
    },
    {
      "epoch": 0.45912,
      "grad_norm": 0.40989506244659424,
      "learning_rate": 0.00016941458861181492,
      "loss": 0.8864,
      "step": 5739
    },
    {
      "epoch": 0.4592,
      "grad_norm": 0.32518821954727173,
      "learning_rate": 0.00016940925456727566,
      "loss": 0.7216,
      "step": 5740
    },
    {
      "epoch": 0.45928,
      "grad_norm": 0.4002778232097626,
      "learning_rate": 0.00016940392052273637,
      "loss": 0.8145,
      "step": 5741
    },
    {
      "epoch": 0.45936,
      "grad_norm": 0.4582330584526062,
      "learning_rate": 0.0001693985864781971,
      "loss": 0.7475,
      "step": 5742
    },
    {
      "epoch": 0.45944,
      "grad_norm": 0.31674304604530334,
      "learning_rate": 0.00016939325243365782,
      "loss": 0.9355,
      "step": 5743
    },
    {
      "epoch": 0.45952,
      "grad_norm": 0.4366816580295563,
      "learning_rate": 0.00016938791838911856,
      "loss": 0.8674,
      "step": 5744
    },
    {
      "epoch": 0.4596,
      "grad_norm": 0.34372252225875854,
      "learning_rate": 0.00016938258434457927,
      "loss": 0.575,
      "step": 5745
    },
    {
      "epoch": 0.45968,
      "grad_norm": 0.33507341146469116,
      "learning_rate": 0.00016937725030004001,
      "loss": 0.9905,
      "step": 5746
    },
    {
      "epoch": 0.45976,
      "grad_norm": 0.42992669343948364,
      "learning_rate": 0.00016937191625550075,
      "loss": 0.7553,
      "step": 5747
    },
    {
      "epoch": 0.45984,
      "grad_norm": 0.33788618445396423,
      "learning_rate": 0.00016936658221096147,
      "loss": 1.0111,
      "step": 5748
    },
    {
      "epoch": 0.45992,
      "grad_norm": 0.33767634630203247,
      "learning_rate": 0.0001693612481664222,
      "loss": 1.203,
      "step": 5749
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.3980598449707031,
      "learning_rate": 0.00016935591412188292,
      "loss": 0.7539,
      "step": 5750
    },
    {
      "epoch": 0.46008,
      "grad_norm": 0.43605756759643555,
      "learning_rate": 0.00016935058007734366,
      "loss": 0.9342,
      "step": 5751
    },
    {
      "epoch": 0.46016,
      "grad_norm": 0.4817270040512085,
      "learning_rate": 0.00016934524603280437,
      "loss": 1.1161,
      "step": 5752
    },
    {
      "epoch": 0.46024,
      "grad_norm": 0.44135692715644836,
      "learning_rate": 0.0001693399119882651,
      "loss": 0.7618,
      "step": 5753
    },
    {
      "epoch": 0.46032,
      "grad_norm": 0.25595974922180176,
      "learning_rate": 0.00016933457794372585,
      "loss": 0.5376,
      "step": 5754
    },
    {
      "epoch": 0.4604,
      "grad_norm": 0.37219828367233276,
      "learning_rate": 0.00016932924389918656,
      "loss": 0.6834,
      "step": 5755
    },
    {
      "epoch": 0.46048,
      "grad_norm": 0.3629576861858368,
      "learning_rate": 0.0001693239098546473,
      "loss": 1.0175,
      "step": 5756
    },
    {
      "epoch": 0.46056,
      "grad_norm": 0.5025891661643982,
      "learning_rate": 0.00016931857581010802,
      "loss": 0.8889,
      "step": 5757
    },
    {
      "epoch": 0.46064,
      "grad_norm": 0.3887712359428406,
      "learning_rate": 0.00016931324176556876,
      "loss": 0.801,
      "step": 5758
    },
    {
      "epoch": 0.46072,
      "grad_norm": 0.5086892247200012,
      "learning_rate": 0.00016930790772102947,
      "loss": 1.1129,
      "step": 5759
    },
    {
      "epoch": 0.4608,
      "grad_norm": 0.49136799573898315,
      "learning_rate": 0.0001693025736764902,
      "loss": 0.9775,
      "step": 5760
    },
    {
      "epoch": 0.46088,
      "grad_norm": 0.3812214136123657,
      "learning_rate": 0.00016929723963195095,
      "loss": 0.9866,
      "step": 5761
    },
    {
      "epoch": 0.46096,
      "grad_norm": 0.3724660277366638,
      "learning_rate": 0.00016929190558741166,
      "loss": 0.7796,
      "step": 5762
    },
    {
      "epoch": 0.46104,
      "grad_norm": 0.47706034779548645,
      "learning_rate": 0.0001692865715428724,
      "loss": 0.9396,
      "step": 5763
    },
    {
      "epoch": 0.46112,
      "grad_norm": 0.44467878341674805,
      "learning_rate": 0.00016928123749833311,
      "loss": 0.8471,
      "step": 5764
    },
    {
      "epoch": 0.4612,
      "grad_norm": 0.3878631293773651,
      "learning_rate": 0.00016927590345379385,
      "loss": 1.0264,
      "step": 5765
    },
    {
      "epoch": 0.46128,
      "grad_norm": 0.45248034596443176,
      "learning_rate": 0.00016927056940925457,
      "loss": 0.9399,
      "step": 5766
    },
    {
      "epoch": 0.46136,
      "grad_norm": 0.3242575526237488,
      "learning_rate": 0.0001692652353647153,
      "loss": 1.0114,
      "step": 5767
    },
    {
      "epoch": 0.46144,
      "grad_norm": 0.48565980792045593,
      "learning_rate": 0.00016925990132017605,
      "loss": 0.8062,
      "step": 5768
    },
    {
      "epoch": 0.46152,
      "grad_norm": 0.4608457088470459,
      "learning_rate": 0.00016925456727563676,
      "loss": 0.8665,
      "step": 5769
    },
    {
      "epoch": 0.4616,
      "grad_norm": 0.460475355386734,
      "learning_rate": 0.0001692492332310975,
      "loss": 0.8799,
      "step": 5770
    },
    {
      "epoch": 0.46168,
      "grad_norm": 0.33738574385643005,
      "learning_rate": 0.0001692438991865582,
      "loss": 0.8534,
      "step": 5771
    },
    {
      "epoch": 0.46176,
      "grad_norm": 0.37424522638320923,
      "learning_rate": 0.00016923856514201895,
      "loss": 0.6887,
      "step": 5772
    },
    {
      "epoch": 0.46184,
      "grad_norm": 0.46604666113853455,
      "learning_rate": 0.0001692332310974797,
      "loss": 0.7264,
      "step": 5773
    },
    {
      "epoch": 0.46192,
      "grad_norm": 0.5523864030838013,
      "learning_rate": 0.0001692278970529404,
      "loss": 0.8965,
      "step": 5774
    },
    {
      "epoch": 0.462,
      "grad_norm": 0.3701479732990265,
      "learning_rate": 0.00016922256300840114,
      "loss": 0.8626,
      "step": 5775
    },
    {
      "epoch": 0.46208,
      "grad_norm": 0.5608081221580505,
      "learning_rate": 0.00016921722896386186,
      "loss": 0.6506,
      "step": 5776
    },
    {
      "epoch": 0.46216,
      "grad_norm": 0.36068782210350037,
      "learning_rate": 0.0001692118949193226,
      "loss": 0.884,
      "step": 5777
    },
    {
      "epoch": 0.46224,
      "grad_norm": 0.5199491381645203,
      "learning_rate": 0.0001692065608747833,
      "loss": 0.9861,
      "step": 5778
    },
    {
      "epoch": 0.46232,
      "grad_norm": 0.433319628238678,
      "learning_rate": 0.00016920122683024405,
      "loss": 0.8648,
      "step": 5779
    },
    {
      "epoch": 0.4624,
      "grad_norm": 0.28879377245903015,
      "learning_rate": 0.0001691958927857048,
      "loss": 0.7042,
      "step": 5780
    },
    {
      "epoch": 0.46248,
      "grad_norm": 0.44802722334861755,
      "learning_rate": 0.0001691905587411655,
      "loss": 0.689,
      "step": 5781
    },
    {
      "epoch": 0.46256,
      "grad_norm": 0.4127955734729767,
      "learning_rate": 0.00016918522469662624,
      "loss": 0.7412,
      "step": 5782
    },
    {
      "epoch": 0.46264,
      "grad_norm": 0.39729049801826477,
      "learning_rate": 0.00016917989065208695,
      "loss": 0.5179,
      "step": 5783
    },
    {
      "epoch": 0.46272,
      "grad_norm": 0.4060831069946289,
      "learning_rate": 0.0001691745566075477,
      "loss": 0.6003,
      "step": 5784
    },
    {
      "epoch": 0.4628,
      "grad_norm": 0.5156298279762268,
      "learning_rate": 0.0001691692225630084,
      "loss": 1.0256,
      "step": 5785
    },
    {
      "epoch": 0.46288,
      "grad_norm": 0.611190676689148,
      "learning_rate": 0.00016916388851846915,
      "loss": 1.2463,
      "step": 5786
    },
    {
      "epoch": 0.46296,
      "grad_norm": 0.3800579607486725,
      "learning_rate": 0.0001691585544739299,
      "loss": 0.8856,
      "step": 5787
    },
    {
      "epoch": 0.46304,
      "grad_norm": 0.35438650846481323,
      "learning_rate": 0.0001691532204293906,
      "loss": 1.0887,
      "step": 5788
    },
    {
      "epoch": 0.46312,
      "grad_norm": 0.37933292984962463,
      "learning_rate": 0.00016914788638485134,
      "loss": 0.5154,
      "step": 5789
    },
    {
      "epoch": 0.4632,
      "grad_norm": 0.31124597787857056,
      "learning_rate": 0.00016914255234031205,
      "loss": 0.9065,
      "step": 5790
    },
    {
      "epoch": 0.46328,
      "grad_norm": 0.42387649416923523,
      "learning_rate": 0.0001691372182957728,
      "loss": 0.761,
      "step": 5791
    },
    {
      "epoch": 0.46336,
      "grad_norm": 0.3736684024333954,
      "learning_rate": 0.0001691318842512335,
      "loss": 0.7722,
      "step": 5792
    },
    {
      "epoch": 0.46344,
      "grad_norm": 0.4026992917060852,
      "learning_rate": 0.00016912655020669424,
      "loss": 0.7022,
      "step": 5793
    },
    {
      "epoch": 0.46352,
      "grad_norm": 0.45674964785575867,
      "learning_rate": 0.00016912121616215498,
      "loss": 0.6364,
      "step": 5794
    },
    {
      "epoch": 0.4636,
      "grad_norm": 0.3629937767982483,
      "learning_rate": 0.0001691158821176157,
      "loss": 1.0048,
      "step": 5795
    },
    {
      "epoch": 0.46368,
      "grad_norm": 0.3441199064254761,
      "learning_rate": 0.00016911054807307644,
      "loss": 0.9814,
      "step": 5796
    },
    {
      "epoch": 0.46376,
      "grad_norm": 0.5405066013336182,
      "learning_rate": 0.00016910521402853715,
      "loss": 0.7517,
      "step": 5797
    },
    {
      "epoch": 0.46384,
      "grad_norm": 0.45248207449913025,
      "learning_rate": 0.0001690998799839979,
      "loss": 0.6879,
      "step": 5798
    },
    {
      "epoch": 0.46392,
      "grad_norm": 0.35344621539115906,
      "learning_rate": 0.0001690945459394586,
      "loss": 0.7692,
      "step": 5799
    },
    {
      "epoch": 0.464,
      "grad_norm": 0.31435421109199524,
      "learning_rate": 0.00016908921189491934,
      "loss": 0.5387,
      "step": 5800
    },
    {
      "epoch": 0.46408,
      "grad_norm": 0.3733358681201935,
      "learning_rate": 0.00016908387785038006,
      "loss": 0.7841,
      "step": 5801
    },
    {
      "epoch": 0.46416,
      "grad_norm": 0.33456218242645264,
      "learning_rate": 0.0001690785438058408,
      "loss": 0.5047,
      "step": 5802
    },
    {
      "epoch": 0.46424,
      "grad_norm": 0.376759797334671,
      "learning_rate": 0.0001690732097613015,
      "loss": 0.9469,
      "step": 5803
    },
    {
      "epoch": 0.46432,
      "grad_norm": 0.39316004514694214,
      "learning_rate": 0.00016906787571676225,
      "loss": 0.8272,
      "step": 5804
    },
    {
      "epoch": 0.4644,
      "grad_norm": 0.3523465096950531,
      "learning_rate": 0.000169062541672223,
      "loss": 0.6601,
      "step": 5805
    },
    {
      "epoch": 0.46448,
      "grad_norm": 0.3963000476360321,
      "learning_rate": 0.0001690572076276837,
      "loss": 0.8602,
      "step": 5806
    },
    {
      "epoch": 0.46456,
      "grad_norm": 0.28154218196868896,
      "learning_rate": 0.00016905187358314444,
      "loss": 0.6134,
      "step": 5807
    },
    {
      "epoch": 0.46464,
      "grad_norm": 0.4449785351753235,
      "learning_rate": 0.00016904653953860515,
      "loss": 0.6382,
      "step": 5808
    },
    {
      "epoch": 0.46472,
      "grad_norm": 0.39049050211906433,
      "learning_rate": 0.0001690412054940659,
      "loss": 0.5583,
      "step": 5809
    },
    {
      "epoch": 0.4648,
      "grad_norm": 0.36800041794776917,
      "learning_rate": 0.0001690358714495266,
      "loss": 0.7312,
      "step": 5810
    },
    {
      "epoch": 0.46488,
      "grad_norm": 0.3630598485469818,
      "learning_rate": 0.00016903053740498735,
      "loss": 0.9532,
      "step": 5811
    },
    {
      "epoch": 0.46496,
      "grad_norm": 0.45267564058303833,
      "learning_rate": 0.00016902520336044806,
      "loss": 0.6905,
      "step": 5812
    },
    {
      "epoch": 0.46504,
      "grad_norm": 0.3623550236225128,
      "learning_rate": 0.0001690198693159088,
      "loss": 0.7328,
      "step": 5813
    },
    {
      "epoch": 0.46512,
      "grad_norm": 0.40809008479118347,
      "learning_rate": 0.0001690145352713695,
      "loss": 0.799,
      "step": 5814
    },
    {
      "epoch": 0.4652,
      "grad_norm": 0.5190396308898926,
      "learning_rate": 0.00016900920122683025,
      "loss": 0.9809,
      "step": 5815
    },
    {
      "epoch": 0.46528,
      "grad_norm": 0.3652482330799103,
      "learning_rate": 0.00016900386718229096,
      "loss": 0.9907,
      "step": 5816
    },
    {
      "epoch": 0.46536,
      "grad_norm": 0.39107218384742737,
      "learning_rate": 0.0001689985331377517,
      "loss": 0.6677,
      "step": 5817
    },
    {
      "epoch": 0.46544,
      "grad_norm": 0.382746160030365,
      "learning_rate": 0.00016899319909321242,
      "loss": 0.9549,
      "step": 5818
    },
    {
      "epoch": 0.46552,
      "grad_norm": 0.4247351884841919,
      "learning_rate": 0.00016898786504867316,
      "loss": 0.6093,
      "step": 5819
    },
    {
      "epoch": 0.4656,
      "grad_norm": 0.374203085899353,
      "learning_rate": 0.00016898253100413387,
      "loss": 0.7883,
      "step": 5820
    },
    {
      "epoch": 0.46568,
      "grad_norm": 0.4188356399536133,
      "learning_rate": 0.0001689771969595946,
      "loss": 0.7314,
      "step": 5821
    },
    {
      "epoch": 0.46576,
      "grad_norm": 0.43928593397140503,
      "learning_rate": 0.00016897186291505535,
      "loss": 0.7491,
      "step": 5822
    },
    {
      "epoch": 0.46584,
      "grad_norm": 0.34144341945648193,
      "learning_rate": 0.00016896652887051606,
      "loss": 0.88,
      "step": 5823
    },
    {
      "epoch": 0.46592,
      "grad_norm": 0.5146058797836304,
      "learning_rate": 0.0001689611948259768,
      "loss": 1.0692,
      "step": 5824
    },
    {
      "epoch": 0.466,
      "grad_norm": 0.34475529193878174,
      "learning_rate": 0.0001689558607814375,
      "loss": 1.0445,
      "step": 5825
    },
    {
      "epoch": 0.46608,
      "grad_norm": 0.5183398127555847,
      "learning_rate": 0.00016895052673689825,
      "loss": 1.188,
      "step": 5826
    },
    {
      "epoch": 0.46616,
      "grad_norm": 0.4310908615589142,
      "learning_rate": 0.000168945192692359,
      "loss": 0.6703,
      "step": 5827
    },
    {
      "epoch": 0.46624,
      "grad_norm": 0.3299151659011841,
      "learning_rate": 0.0001689398586478197,
      "loss": 0.8924,
      "step": 5828
    },
    {
      "epoch": 0.46632,
      "grad_norm": 0.41145890951156616,
      "learning_rate": 0.00016893452460328045,
      "loss": 0.8274,
      "step": 5829
    },
    {
      "epoch": 0.4664,
      "grad_norm": 0.4070497453212738,
      "learning_rate": 0.00016892919055874116,
      "loss": 0.6542,
      "step": 5830
    },
    {
      "epoch": 0.46648,
      "grad_norm": 0.4140393137931824,
      "learning_rate": 0.0001689238565142019,
      "loss": 0.8423,
      "step": 5831
    },
    {
      "epoch": 0.46656,
      "grad_norm": 0.4930170476436615,
      "learning_rate": 0.0001689185224696626,
      "loss": 0.6937,
      "step": 5832
    },
    {
      "epoch": 0.46664,
      "grad_norm": 0.3555103838443756,
      "learning_rate": 0.00016891318842512335,
      "loss": 0.7567,
      "step": 5833
    },
    {
      "epoch": 0.46672,
      "grad_norm": 0.3548056185245514,
      "learning_rate": 0.0001689078543805841,
      "loss": 0.8968,
      "step": 5834
    },
    {
      "epoch": 0.4668,
      "grad_norm": 0.34903252124786377,
      "learning_rate": 0.0001689025203360448,
      "loss": 0.8291,
      "step": 5835
    },
    {
      "epoch": 0.46688,
      "grad_norm": 0.3544219732284546,
      "learning_rate": 0.00016889718629150554,
      "loss": 0.7705,
      "step": 5836
    },
    {
      "epoch": 0.46696,
      "grad_norm": 0.38383370637893677,
      "learning_rate": 0.00016889185224696626,
      "loss": 0.9948,
      "step": 5837
    },
    {
      "epoch": 0.46704,
      "grad_norm": 0.3810518682003021,
      "learning_rate": 0.000168886518202427,
      "loss": 0.6443,
      "step": 5838
    },
    {
      "epoch": 0.46712,
      "grad_norm": 0.37858641147613525,
      "learning_rate": 0.0001688811841578877,
      "loss": 0.9916,
      "step": 5839
    },
    {
      "epoch": 0.4672,
      "grad_norm": 0.4626862406730652,
      "learning_rate": 0.00016887585011334845,
      "loss": 0.7481,
      "step": 5840
    },
    {
      "epoch": 0.46728,
      "grad_norm": 0.39579054713249207,
      "learning_rate": 0.0001688705160688092,
      "loss": 0.9487,
      "step": 5841
    },
    {
      "epoch": 0.46736,
      "grad_norm": 0.33760523796081543,
      "learning_rate": 0.0001688651820242699,
      "loss": 0.7146,
      "step": 5842
    },
    {
      "epoch": 0.46744,
      "grad_norm": 0.36102744936943054,
      "learning_rate": 0.00016885984797973064,
      "loss": 0.7707,
      "step": 5843
    },
    {
      "epoch": 0.46752,
      "grad_norm": 0.30297762155532837,
      "learning_rate": 0.00016885451393519135,
      "loss": 0.5911,
      "step": 5844
    },
    {
      "epoch": 0.4676,
      "grad_norm": 0.3877100348472595,
      "learning_rate": 0.0001688491798906521,
      "loss": 0.9218,
      "step": 5845
    },
    {
      "epoch": 0.46768,
      "grad_norm": 0.49329230189323425,
      "learning_rate": 0.0001688438458461128,
      "loss": 1.3883,
      "step": 5846
    },
    {
      "epoch": 0.46776,
      "grad_norm": 0.32405853271484375,
      "learning_rate": 0.00016883851180157355,
      "loss": 0.4531,
      "step": 5847
    },
    {
      "epoch": 0.46784,
      "grad_norm": 0.4302245080471039,
      "learning_rate": 0.00016883317775703429,
      "loss": 0.6597,
      "step": 5848
    },
    {
      "epoch": 0.46792,
      "grad_norm": 0.30883705615997314,
      "learning_rate": 0.000168827843712495,
      "loss": 0.8435,
      "step": 5849
    },
    {
      "epoch": 0.468,
      "grad_norm": 0.48963209986686707,
      "learning_rate": 0.00016882250966795574,
      "loss": 0.7073,
      "step": 5850
    },
    {
      "epoch": 0.46808,
      "grad_norm": 0.33522576093673706,
      "learning_rate": 0.00016881717562341645,
      "loss": 1.0238,
      "step": 5851
    },
    {
      "epoch": 0.46816,
      "grad_norm": 0.4373321235179901,
      "learning_rate": 0.0001688118415788772,
      "loss": 0.532,
      "step": 5852
    },
    {
      "epoch": 0.46824,
      "grad_norm": 0.39973974227905273,
      "learning_rate": 0.0001688065075343379,
      "loss": 0.6368,
      "step": 5853
    },
    {
      "epoch": 0.46832,
      "grad_norm": 0.41222089529037476,
      "learning_rate": 0.00016880117348979864,
      "loss": 0.6296,
      "step": 5854
    },
    {
      "epoch": 0.4684,
      "grad_norm": 0.3634982109069824,
      "learning_rate": 0.00016879583944525938,
      "loss": 1.254,
      "step": 5855
    },
    {
      "epoch": 0.46848,
      "grad_norm": 0.3623935580253601,
      "learning_rate": 0.0001687905054007201,
      "loss": 0.8407,
      "step": 5856
    },
    {
      "epoch": 0.46856,
      "grad_norm": 0.4832395017147064,
      "learning_rate": 0.00016878517135618084,
      "loss": 0.9022,
      "step": 5857
    },
    {
      "epoch": 0.46864,
      "grad_norm": 0.3002760708332062,
      "learning_rate": 0.00016877983731164155,
      "loss": 0.6405,
      "step": 5858
    },
    {
      "epoch": 0.46872,
      "grad_norm": 0.4706999957561493,
      "learning_rate": 0.0001687745032671023,
      "loss": 0.9086,
      "step": 5859
    },
    {
      "epoch": 0.4688,
      "grad_norm": 0.38998204469680786,
      "learning_rate": 0.000168769169222563,
      "loss": 1.0051,
      "step": 5860
    },
    {
      "epoch": 0.46888,
      "grad_norm": 0.6249770522117615,
      "learning_rate": 0.00016876383517802374,
      "loss": 0.8686,
      "step": 5861
    },
    {
      "epoch": 0.46896,
      "grad_norm": 0.35106179118156433,
      "learning_rate": 0.00016875850113348448,
      "loss": 0.7132,
      "step": 5862
    },
    {
      "epoch": 0.46904,
      "grad_norm": 0.4292846918106079,
      "learning_rate": 0.0001687531670889452,
      "loss": 0.8199,
      "step": 5863
    },
    {
      "epoch": 0.46912,
      "grad_norm": 0.42064812779426575,
      "learning_rate": 0.00016874783304440593,
      "loss": 0.7353,
      "step": 5864
    },
    {
      "epoch": 0.4692,
      "grad_norm": 0.3593880534172058,
      "learning_rate": 0.00016874249899986665,
      "loss": 0.8617,
      "step": 5865
    },
    {
      "epoch": 0.46928,
      "grad_norm": 0.33552080392837524,
      "learning_rate": 0.00016873716495532739,
      "loss": 0.7908,
      "step": 5866
    },
    {
      "epoch": 0.46936,
      "grad_norm": 0.4778473973274231,
      "learning_rate": 0.0001687318309107881,
      "loss": 1.0051,
      "step": 5867
    },
    {
      "epoch": 0.46944,
      "grad_norm": 0.38062822818756104,
      "learning_rate": 0.00016872649686624884,
      "loss": 0.9685,
      "step": 5868
    },
    {
      "epoch": 0.46952,
      "grad_norm": 0.35359102487564087,
      "learning_rate": 0.00016872116282170958,
      "loss": 0.6856,
      "step": 5869
    },
    {
      "epoch": 0.4696,
      "grad_norm": 0.4018325209617615,
      "learning_rate": 0.0001687158287771703,
      "loss": 0.7168,
      "step": 5870
    },
    {
      "epoch": 0.46968,
      "grad_norm": 0.44014883041381836,
      "learning_rate": 0.00016871049473263103,
      "loss": 0.7443,
      "step": 5871
    },
    {
      "epoch": 0.46976,
      "grad_norm": 0.36728158593177795,
      "learning_rate": 0.00016870516068809174,
      "loss": 1.0005,
      "step": 5872
    },
    {
      "epoch": 0.46984,
      "grad_norm": 0.3522648215293884,
      "learning_rate": 0.00016869982664355248,
      "loss": 0.6437,
      "step": 5873
    },
    {
      "epoch": 0.46992,
      "grad_norm": 0.41054853796958923,
      "learning_rate": 0.00016869449259901322,
      "loss": 1.0603,
      "step": 5874
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.4056011438369751,
      "learning_rate": 0.00016868915855447394,
      "loss": 1.1586,
      "step": 5875
    },
    {
      "epoch": 0.47008,
      "grad_norm": 0.36188992857933044,
      "learning_rate": 0.00016868382450993468,
      "loss": 1.1957,
      "step": 5876
    },
    {
      "epoch": 0.47016,
      "grad_norm": 0.43084144592285156,
      "learning_rate": 0.0001686784904653954,
      "loss": 0.6594,
      "step": 5877
    },
    {
      "epoch": 0.47024,
      "grad_norm": 0.3148441016674042,
      "learning_rate": 0.00016867315642085613,
      "loss": 0.4875,
      "step": 5878
    },
    {
      "epoch": 0.47032,
      "grad_norm": 0.3201006352901459,
      "learning_rate": 0.00016866782237631684,
      "loss": 0.4889,
      "step": 5879
    },
    {
      "epoch": 0.4704,
      "grad_norm": 0.44343093037605286,
      "learning_rate": 0.00016866248833177758,
      "loss": 0.9995,
      "step": 5880
    },
    {
      "epoch": 0.47048,
      "grad_norm": 0.39349979162216187,
      "learning_rate": 0.00016865715428723832,
      "loss": 0.8318,
      "step": 5881
    },
    {
      "epoch": 0.47056,
      "grad_norm": 0.32304540276527405,
      "learning_rate": 0.00016865182024269903,
      "loss": 0.858,
      "step": 5882
    },
    {
      "epoch": 0.47064,
      "grad_norm": 0.4364137351512909,
      "learning_rate": 0.00016864648619815977,
      "loss": 0.6738,
      "step": 5883
    },
    {
      "epoch": 0.47072,
      "grad_norm": 0.44627249240875244,
      "learning_rate": 0.0001686411521536205,
      "loss": 0.7652,
      "step": 5884
    },
    {
      "epoch": 0.4708,
      "grad_norm": 0.4739437997341156,
      "learning_rate": 0.00016863581810908123,
      "loss": 1.0119,
      "step": 5885
    },
    {
      "epoch": 0.47088,
      "grad_norm": 0.3230496644973755,
      "learning_rate": 0.00016863048406454194,
      "loss": 0.8125,
      "step": 5886
    },
    {
      "epoch": 0.47096,
      "grad_norm": 0.37907424569129944,
      "learning_rate": 0.00016862515002000268,
      "loss": 0.7496,
      "step": 5887
    },
    {
      "epoch": 0.47104,
      "grad_norm": 0.3865816593170166,
      "learning_rate": 0.00016861981597546342,
      "loss": 0.9327,
      "step": 5888
    },
    {
      "epoch": 0.47112,
      "grad_norm": 0.4271667003631592,
      "learning_rate": 0.00016861448193092413,
      "loss": 0.8398,
      "step": 5889
    },
    {
      "epoch": 0.4712,
      "grad_norm": 0.3867068588733673,
      "learning_rate": 0.00016860914788638487,
      "loss": 0.7631,
      "step": 5890
    },
    {
      "epoch": 0.47128,
      "grad_norm": 0.46814367175102234,
      "learning_rate": 0.00016860381384184558,
      "loss": 0.5661,
      "step": 5891
    },
    {
      "epoch": 0.47136,
      "grad_norm": 0.4365675151348114,
      "learning_rate": 0.00016859847979730632,
      "loss": 0.7405,
      "step": 5892
    },
    {
      "epoch": 0.47144,
      "grad_norm": 0.3214791715145111,
      "learning_rate": 0.00016859314575276704,
      "loss": 0.7242,
      "step": 5893
    },
    {
      "epoch": 0.47152,
      "grad_norm": 0.4816702604293823,
      "learning_rate": 0.00016858781170822778,
      "loss": 0.798,
      "step": 5894
    },
    {
      "epoch": 0.4716,
      "grad_norm": 0.4714539051055908,
      "learning_rate": 0.00016858247766368852,
      "loss": 1.0242,
      "step": 5895
    },
    {
      "epoch": 0.47168,
      "grad_norm": 0.39081934094429016,
      "learning_rate": 0.00016857714361914923,
      "loss": 0.5676,
      "step": 5896
    },
    {
      "epoch": 0.47176,
      "grad_norm": 0.33126816153526306,
      "learning_rate": 0.00016857180957460997,
      "loss": 0.5929,
      "step": 5897
    },
    {
      "epoch": 0.47184,
      "grad_norm": 0.42280662059783936,
      "learning_rate": 0.00016856647553007068,
      "loss": 1.0959,
      "step": 5898
    },
    {
      "epoch": 0.47192,
      "grad_norm": 0.41091737151145935,
      "learning_rate": 0.00016856114148553142,
      "loss": 0.6758,
      "step": 5899
    },
    {
      "epoch": 0.472,
      "grad_norm": 0.3916332721710205,
      "learning_rate": 0.00016855580744099213,
      "loss": 0.7482,
      "step": 5900
    },
    {
      "epoch": 0.47208,
      "grad_norm": 0.39071568846702576,
      "learning_rate": 0.00016855047339645287,
      "loss": 0.694,
      "step": 5901
    },
    {
      "epoch": 0.47216,
      "grad_norm": 0.3341774344444275,
      "learning_rate": 0.00016854513935191361,
      "loss": 1.1823,
      "step": 5902
    },
    {
      "epoch": 0.47224,
      "grad_norm": 0.3884686529636383,
      "learning_rate": 0.00016853980530737433,
      "loss": 0.7949,
      "step": 5903
    },
    {
      "epoch": 0.47232,
      "grad_norm": 0.480954110622406,
      "learning_rate": 0.00016853447126283507,
      "loss": 0.8273,
      "step": 5904
    },
    {
      "epoch": 0.4724,
      "grad_norm": 0.33842408657073975,
      "learning_rate": 0.00016852913721829578,
      "loss": 0.6475,
      "step": 5905
    },
    {
      "epoch": 0.47248,
      "grad_norm": 0.37769830226898193,
      "learning_rate": 0.00016852380317375652,
      "loss": 0.8098,
      "step": 5906
    },
    {
      "epoch": 0.47256,
      "grad_norm": 0.40827444195747375,
      "learning_rate": 0.00016851846912921723,
      "loss": 1.0658,
      "step": 5907
    },
    {
      "epoch": 0.47264,
      "grad_norm": 0.39832061529159546,
      "learning_rate": 0.00016851313508467797,
      "loss": 0.7988,
      "step": 5908
    },
    {
      "epoch": 0.47272,
      "grad_norm": 0.3670337200164795,
      "learning_rate": 0.0001685078010401387,
      "loss": 1.0128,
      "step": 5909
    },
    {
      "epoch": 0.4728,
      "grad_norm": 0.34611397981643677,
      "learning_rate": 0.00016850246699559942,
      "loss": 0.5977,
      "step": 5910
    },
    {
      "epoch": 0.47288,
      "grad_norm": 0.3883821666240692,
      "learning_rate": 0.00016849713295106016,
      "loss": 0.9293,
      "step": 5911
    },
    {
      "epoch": 0.47296,
      "grad_norm": 0.42590129375457764,
      "learning_rate": 0.00016849179890652088,
      "loss": 0.9469,
      "step": 5912
    },
    {
      "epoch": 0.47304,
      "grad_norm": 0.40163448452949524,
      "learning_rate": 0.00016848646486198162,
      "loss": 0.9177,
      "step": 5913
    },
    {
      "epoch": 0.47312,
      "grad_norm": 0.3985399305820465,
      "learning_rate": 0.00016848113081744233,
      "loss": 0.8683,
      "step": 5914
    },
    {
      "epoch": 0.4732,
      "grad_norm": 0.47665074467658997,
      "learning_rate": 0.00016847579677290307,
      "loss": 1.0339,
      "step": 5915
    },
    {
      "epoch": 0.47328,
      "grad_norm": 0.3225591480731964,
      "learning_rate": 0.0001684704627283638,
      "loss": 0.6515,
      "step": 5916
    },
    {
      "epoch": 0.47336,
      "grad_norm": 0.34698954224586487,
      "learning_rate": 0.00016846512868382452,
      "loss": 0.8736,
      "step": 5917
    },
    {
      "epoch": 0.47344,
      "grad_norm": 0.3894367516040802,
      "learning_rate": 0.00016845979463928526,
      "loss": 1.2043,
      "step": 5918
    },
    {
      "epoch": 0.47352,
      "grad_norm": 0.4699413478374481,
      "learning_rate": 0.00016845446059474597,
      "loss": 0.7804,
      "step": 5919
    },
    {
      "epoch": 0.4736,
      "grad_norm": 0.42507654428482056,
      "learning_rate": 0.00016844912655020671,
      "loss": 0.9272,
      "step": 5920
    },
    {
      "epoch": 0.47368,
      "grad_norm": 0.5889617204666138,
      "learning_rate": 0.00016844379250566743,
      "loss": 1.244,
      "step": 5921
    },
    {
      "epoch": 0.47376,
      "grad_norm": 0.3702944815158844,
      "learning_rate": 0.00016843845846112817,
      "loss": 0.5648,
      "step": 5922
    },
    {
      "epoch": 0.47384,
      "grad_norm": 0.4291045665740967,
      "learning_rate": 0.0001684331244165889,
      "loss": 1.1598,
      "step": 5923
    },
    {
      "epoch": 0.47392,
      "grad_norm": 0.3419771194458008,
      "learning_rate": 0.00016842779037204962,
      "loss": 0.9791,
      "step": 5924
    },
    {
      "epoch": 0.474,
      "grad_norm": 0.3543022871017456,
      "learning_rate": 0.00016842245632751036,
      "loss": 0.9718,
      "step": 5925
    },
    {
      "epoch": 0.47408,
      "grad_norm": 0.3239482343196869,
      "learning_rate": 0.00016841712228297107,
      "loss": 0.9507,
      "step": 5926
    },
    {
      "epoch": 0.47416,
      "grad_norm": 0.32394978404045105,
      "learning_rate": 0.0001684117882384318,
      "loss": 0.9282,
      "step": 5927
    },
    {
      "epoch": 0.47424,
      "grad_norm": 0.45098212361335754,
      "learning_rate": 0.00016840645419389252,
      "loss": 0.7999,
      "step": 5928
    },
    {
      "epoch": 0.47432,
      "grad_norm": 0.4272007346153259,
      "learning_rate": 0.00016840112014935326,
      "loss": 0.986,
      "step": 5929
    },
    {
      "epoch": 0.4744,
      "grad_norm": 0.36589232087135315,
      "learning_rate": 0.00016839578610481398,
      "loss": 0.6637,
      "step": 5930
    },
    {
      "epoch": 0.47448,
      "grad_norm": 0.3831811547279358,
      "learning_rate": 0.00016839045206027472,
      "loss": 0.8134,
      "step": 5931
    },
    {
      "epoch": 0.47456,
      "grad_norm": 0.35426804423332214,
      "learning_rate": 0.00016838511801573543,
      "loss": 0.8992,
      "step": 5932
    },
    {
      "epoch": 0.47464,
      "grad_norm": 0.34674161672592163,
      "learning_rate": 0.00016837978397119617,
      "loss": 0.6676,
      "step": 5933
    },
    {
      "epoch": 0.47472,
      "grad_norm": 0.306522399187088,
      "learning_rate": 0.0001683744499266569,
      "loss": 0.4955,
      "step": 5934
    },
    {
      "epoch": 0.4748,
      "grad_norm": 0.4298846125602722,
      "learning_rate": 0.00016836911588211762,
      "loss": 0.8555,
      "step": 5935
    },
    {
      "epoch": 0.47488,
      "grad_norm": 0.3388547897338867,
      "learning_rate": 0.00016836378183757836,
      "loss": 0.889,
      "step": 5936
    },
    {
      "epoch": 0.47496,
      "grad_norm": 0.34900039434432983,
      "learning_rate": 0.00016835844779303908,
      "loss": 0.9681,
      "step": 5937
    },
    {
      "epoch": 0.47504,
      "grad_norm": 0.3714873492717743,
      "learning_rate": 0.00016835311374849982,
      "loss": 0.9027,
      "step": 5938
    },
    {
      "epoch": 0.47512,
      "grad_norm": 0.4272557199001312,
      "learning_rate": 0.00016834777970396053,
      "loss": 0.7892,
      "step": 5939
    },
    {
      "epoch": 0.4752,
      "grad_norm": 0.4530614912509918,
      "learning_rate": 0.00016834244565942127,
      "loss": 0.8706,
      "step": 5940
    },
    {
      "epoch": 0.47528,
      "grad_norm": 0.3955179452896118,
      "learning_rate": 0.00016833711161488198,
      "loss": 0.8723,
      "step": 5941
    },
    {
      "epoch": 0.47536,
      "grad_norm": 0.4798436760902405,
      "learning_rate": 0.00016833177757034272,
      "loss": 1.1608,
      "step": 5942
    },
    {
      "epoch": 0.47544,
      "grad_norm": 0.5618347525596619,
      "learning_rate": 0.00016832644352580343,
      "loss": 1.1059,
      "step": 5943
    },
    {
      "epoch": 0.47552,
      "grad_norm": 0.34874603152275085,
      "learning_rate": 0.00016832110948126417,
      "loss": 0.8788,
      "step": 5944
    },
    {
      "epoch": 0.4756,
      "grad_norm": 0.2359635978937149,
      "learning_rate": 0.00016831577543672489,
      "loss": 0.6446,
      "step": 5945
    },
    {
      "epoch": 0.47568,
      "grad_norm": 0.5020753145217896,
      "learning_rate": 0.00016831044139218563,
      "loss": 0.8417,
      "step": 5946
    },
    {
      "epoch": 0.47576,
      "grad_norm": 0.39461058378219604,
      "learning_rate": 0.00016830510734764634,
      "loss": 0.7342,
      "step": 5947
    },
    {
      "epoch": 0.47584,
      "grad_norm": 0.31913936138153076,
      "learning_rate": 0.00016829977330310708,
      "loss": 0.5829,
      "step": 5948
    },
    {
      "epoch": 0.47592,
      "grad_norm": 0.4049251079559326,
      "learning_rate": 0.00016829443925856782,
      "loss": 0.8524,
      "step": 5949
    },
    {
      "epoch": 0.476,
      "grad_norm": 0.3691076338291168,
      "learning_rate": 0.00016828910521402853,
      "loss": 0.6826,
      "step": 5950
    },
    {
      "epoch": 0.47608,
      "grad_norm": 0.3657100200653076,
      "learning_rate": 0.00016828377116948927,
      "loss": 0.9569,
      "step": 5951
    },
    {
      "epoch": 0.47616,
      "grad_norm": 0.39204874634742737,
      "learning_rate": 0.00016827843712494998,
      "loss": 0.8126,
      "step": 5952
    },
    {
      "epoch": 0.47624,
      "grad_norm": 0.3977537155151367,
      "learning_rate": 0.00016827310308041072,
      "loss": 0.7649,
      "step": 5953
    },
    {
      "epoch": 0.47632,
      "grad_norm": 0.36900195479393005,
      "learning_rate": 0.00016826776903587144,
      "loss": 0.9944,
      "step": 5954
    },
    {
      "epoch": 0.4764,
      "grad_norm": 0.4472220242023468,
      "learning_rate": 0.00016826243499133218,
      "loss": 0.8084,
      "step": 5955
    },
    {
      "epoch": 0.47648,
      "grad_norm": 0.4662659466266632,
      "learning_rate": 0.00016825710094679292,
      "loss": 0.8765,
      "step": 5956
    },
    {
      "epoch": 0.47656,
      "grad_norm": 0.28618282079696655,
      "learning_rate": 0.00016825176690225363,
      "loss": 0.7729,
      "step": 5957
    },
    {
      "epoch": 0.47664,
      "grad_norm": 0.2744036614894867,
      "learning_rate": 0.00016824643285771437,
      "loss": 0.4384,
      "step": 5958
    },
    {
      "epoch": 0.47672,
      "grad_norm": 0.30456605553627014,
      "learning_rate": 0.00016824109881317508,
      "loss": 0.5868,
      "step": 5959
    },
    {
      "epoch": 0.4768,
      "grad_norm": 0.4243723452091217,
      "learning_rate": 0.00016823576476863582,
      "loss": 0.9417,
      "step": 5960
    },
    {
      "epoch": 0.47688,
      "grad_norm": 0.38446369767189026,
      "learning_rate": 0.00016823043072409653,
      "loss": 0.6786,
      "step": 5961
    },
    {
      "epoch": 0.47696,
      "grad_norm": 0.3415071964263916,
      "learning_rate": 0.00016822509667955727,
      "loss": 0.6095,
      "step": 5962
    },
    {
      "epoch": 0.47704,
      "grad_norm": 0.45759522914886475,
      "learning_rate": 0.000168219762635018,
      "loss": 0.9461,
      "step": 5963
    },
    {
      "epoch": 0.47712,
      "grad_norm": 0.40265050530433655,
      "learning_rate": 0.00016821442859047873,
      "loss": 0.6907,
      "step": 5964
    },
    {
      "epoch": 0.4772,
      "grad_norm": 0.37768861651420593,
      "learning_rate": 0.00016820909454593947,
      "loss": 0.7654,
      "step": 5965
    },
    {
      "epoch": 0.47728,
      "grad_norm": 0.3232647180557251,
      "learning_rate": 0.00016820376050140018,
      "loss": 0.6613,
      "step": 5966
    },
    {
      "epoch": 0.47736,
      "grad_norm": 0.3496397137641907,
      "learning_rate": 0.00016819842645686092,
      "loss": 0.8218,
      "step": 5967
    },
    {
      "epoch": 0.47744,
      "grad_norm": 0.3428927958011627,
      "learning_rate": 0.00016819309241232163,
      "loss": 0.5854,
      "step": 5968
    },
    {
      "epoch": 0.47752,
      "grad_norm": 0.4477630853652954,
      "learning_rate": 0.00016818775836778237,
      "loss": 0.9685,
      "step": 5969
    },
    {
      "epoch": 0.4776,
      "grad_norm": 0.30513641238212585,
      "learning_rate": 0.0001681824243232431,
      "loss": 0.6127,
      "step": 5970
    },
    {
      "epoch": 0.47768,
      "grad_norm": 0.4428116977214813,
      "learning_rate": 0.00016817709027870382,
      "loss": 0.7555,
      "step": 5971
    },
    {
      "epoch": 0.47776,
      "grad_norm": 0.4737998843193054,
      "learning_rate": 0.00016817175623416456,
      "loss": 1.0549,
      "step": 5972
    },
    {
      "epoch": 0.47784,
      "grad_norm": 0.43995511531829834,
      "learning_rate": 0.00016816642218962528,
      "loss": 0.9651,
      "step": 5973
    },
    {
      "epoch": 0.47792,
      "grad_norm": 0.4207049012184143,
      "learning_rate": 0.00016816108814508602,
      "loss": 0.8384,
      "step": 5974
    },
    {
      "epoch": 0.478,
      "grad_norm": 0.3662116825580597,
      "learning_rate": 0.00016815575410054676,
      "loss": 0.9262,
      "step": 5975
    },
    {
      "epoch": 0.47808,
      "grad_norm": 0.3306950032711029,
      "learning_rate": 0.00016815042005600747,
      "loss": 0.6263,
      "step": 5976
    },
    {
      "epoch": 0.47816,
      "grad_norm": 0.2796168625354767,
      "learning_rate": 0.0001681450860114682,
      "loss": 0.6996,
      "step": 5977
    },
    {
      "epoch": 0.47824,
      "grad_norm": 0.38220545649528503,
      "learning_rate": 0.00016813975196692892,
      "loss": 0.7241,
      "step": 5978
    },
    {
      "epoch": 0.47832,
      "grad_norm": 0.3459092080593109,
      "learning_rate": 0.00016813441792238966,
      "loss": 0.4638,
      "step": 5979
    },
    {
      "epoch": 0.4784,
      "grad_norm": 0.40784838795661926,
      "learning_rate": 0.00016812908387785037,
      "loss": 0.9092,
      "step": 5980
    },
    {
      "epoch": 0.47848,
      "grad_norm": 0.4490830898284912,
      "learning_rate": 0.0001681237498333111,
      "loss": 0.8518,
      "step": 5981
    },
    {
      "epoch": 0.47856,
      "grad_norm": 0.5124590396881104,
      "learning_rate": 0.00016811841578877185,
      "loss": 0.7748,
      "step": 5982
    },
    {
      "epoch": 0.47864,
      "grad_norm": 0.4336957633495331,
      "learning_rate": 0.00016811308174423257,
      "loss": 0.7112,
      "step": 5983
    },
    {
      "epoch": 0.47872,
      "grad_norm": 0.4990687072277069,
      "learning_rate": 0.0001681077476996933,
      "loss": 0.7102,
      "step": 5984
    },
    {
      "epoch": 0.4788,
      "grad_norm": 0.4234769940376282,
      "learning_rate": 0.00016810241365515402,
      "loss": 0.7369,
      "step": 5985
    },
    {
      "epoch": 0.47888,
      "grad_norm": 0.36991140246391296,
      "learning_rate": 0.00016809707961061476,
      "loss": 0.6396,
      "step": 5986
    },
    {
      "epoch": 0.47896,
      "grad_norm": 0.33988088369369507,
      "learning_rate": 0.00016809174556607547,
      "loss": 0.8037,
      "step": 5987
    },
    {
      "epoch": 0.47904,
      "grad_norm": 0.3867737948894501,
      "learning_rate": 0.0001680864115215362,
      "loss": 0.7447,
      "step": 5988
    },
    {
      "epoch": 0.47912,
      "grad_norm": 0.3987494111061096,
      "learning_rate": 0.00016808107747699695,
      "loss": 0.7767,
      "step": 5989
    },
    {
      "epoch": 0.4792,
      "grad_norm": 0.5267616510391235,
      "learning_rate": 0.00016807574343245766,
      "loss": 0.8265,
      "step": 5990
    },
    {
      "epoch": 0.47928,
      "grad_norm": 0.5087209343910217,
      "learning_rate": 0.0001680704093879184,
      "loss": 0.7166,
      "step": 5991
    },
    {
      "epoch": 0.47936,
      "grad_norm": 0.38622426986694336,
      "learning_rate": 0.00016806507534337912,
      "loss": 0.8032,
      "step": 5992
    },
    {
      "epoch": 0.47944,
      "grad_norm": 0.35147005319595337,
      "learning_rate": 0.00016805974129883986,
      "loss": 0.92,
      "step": 5993
    },
    {
      "epoch": 0.47952,
      "grad_norm": 0.418986439704895,
      "learning_rate": 0.00016805440725430057,
      "loss": 0.7559,
      "step": 5994
    },
    {
      "epoch": 0.4796,
      "grad_norm": 0.41358986496925354,
      "learning_rate": 0.0001680490732097613,
      "loss": 0.8223,
      "step": 5995
    },
    {
      "epoch": 0.47968,
      "grad_norm": 0.4688016176223755,
      "learning_rate": 0.00016804373916522205,
      "loss": 1.3481,
      "step": 5996
    },
    {
      "epoch": 0.47976,
      "grad_norm": 0.40892305970191956,
      "learning_rate": 0.00016803840512068276,
      "loss": 0.8255,
      "step": 5997
    },
    {
      "epoch": 0.47984,
      "grad_norm": 0.3769281506538391,
      "learning_rate": 0.0001680330710761435,
      "loss": 0.8702,
      "step": 5998
    },
    {
      "epoch": 0.47992,
      "grad_norm": 0.3453288972377777,
      "learning_rate": 0.00016802773703160421,
      "loss": 0.8644,
      "step": 5999
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.40385791659355164,
      "learning_rate": 0.00016802240298706495,
      "loss": 0.492,
      "step": 6000
    },
    {
      "epoch": 0.48008,
      "grad_norm": 0.37771621346473694,
      "learning_rate": 0.00016801706894252567,
      "loss": 0.6657,
      "step": 6001
    },
    {
      "epoch": 0.48016,
      "grad_norm": 0.5685381293296814,
      "learning_rate": 0.0001680117348979864,
      "loss": 0.836,
      "step": 6002
    },
    {
      "epoch": 0.48024,
      "grad_norm": 0.3407668173313141,
      "learning_rate": 0.00016800640085344715,
      "loss": 0.5671,
      "step": 6003
    },
    {
      "epoch": 0.48032,
      "grad_norm": 0.3921881914138794,
      "learning_rate": 0.00016800106680890786,
      "loss": 0.658,
      "step": 6004
    },
    {
      "epoch": 0.4804,
      "grad_norm": 0.39615482091903687,
      "learning_rate": 0.0001679957327643686,
      "loss": 0.7082,
      "step": 6005
    },
    {
      "epoch": 0.48048,
      "grad_norm": 0.4158181846141815,
      "learning_rate": 0.0001679903987198293,
      "loss": 0.8629,
      "step": 6006
    },
    {
      "epoch": 0.48056,
      "grad_norm": 0.4333856701850891,
      "learning_rate": 0.00016798506467529005,
      "loss": 0.9883,
      "step": 6007
    },
    {
      "epoch": 0.48064,
      "grad_norm": 0.4054352641105652,
      "learning_rate": 0.00016797973063075076,
      "loss": 0.8329,
      "step": 6008
    },
    {
      "epoch": 0.48072,
      "grad_norm": 0.5376091599464417,
      "learning_rate": 0.0001679743965862115,
      "loss": 1.1864,
      "step": 6009
    },
    {
      "epoch": 0.4808,
      "grad_norm": 0.4916306734085083,
      "learning_rate": 0.00016796906254167224,
      "loss": 0.9828,
      "step": 6010
    },
    {
      "epoch": 0.48088,
      "grad_norm": 0.5277397632598877,
      "learning_rate": 0.00016796372849713296,
      "loss": 0.8733,
      "step": 6011
    },
    {
      "epoch": 0.48096,
      "grad_norm": 0.3202744722366333,
      "learning_rate": 0.0001679583944525937,
      "loss": 0.8514,
      "step": 6012
    },
    {
      "epoch": 0.48104,
      "grad_norm": 0.4271179437637329,
      "learning_rate": 0.0001679530604080544,
      "loss": 0.858,
      "step": 6013
    },
    {
      "epoch": 0.48112,
      "grad_norm": 0.4380754232406616,
      "learning_rate": 0.00016794772636351515,
      "loss": 0.9807,
      "step": 6014
    },
    {
      "epoch": 0.4812,
      "grad_norm": 0.4252682626247406,
      "learning_rate": 0.00016794239231897586,
      "loss": 0.9221,
      "step": 6015
    },
    {
      "epoch": 0.48128,
      "grad_norm": 0.38096439838409424,
      "learning_rate": 0.0001679370582744366,
      "loss": 0.6209,
      "step": 6016
    },
    {
      "epoch": 0.48136,
      "grad_norm": 0.5130658149719238,
      "learning_rate": 0.00016793172422989734,
      "loss": 0.9577,
      "step": 6017
    },
    {
      "epoch": 0.48144,
      "grad_norm": 0.3310808539390564,
      "learning_rate": 0.00016792639018535805,
      "loss": 0.6157,
      "step": 6018
    },
    {
      "epoch": 0.48152,
      "grad_norm": 0.4618539810180664,
      "learning_rate": 0.0001679210561408188,
      "loss": 0.9329,
      "step": 6019
    },
    {
      "epoch": 0.4816,
      "grad_norm": 0.4006079435348511,
      "learning_rate": 0.0001679157220962795,
      "loss": 1.2144,
      "step": 6020
    },
    {
      "epoch": 0.48168,
      "grad_norm": 0.3708784282207489,
      "learning_rate": 0.00016791038805174025,
      "loss": 0.8188,
      "step": 6021
    },
    {
      "epoch": 0.48176,
      "grad_norm": 0.5456551313400269,
      "learning_rate": 0.000167905054007201,
      "loss": 1.1154,
      "step": 6022
    },
    {
      "epoch": 0.48184,
      "grad_norm": 0.44242528080940247,
      "learning_rate": 0.0001678997199626617,
      "loss": 0.7724,
      "step": 6023
    },
    {
      "epoch": 0.48192,
      "grad_norm": 0.3811210095882416,
      "learning_rate": 0.00016789438591812244,
      "loss": 0.7303,
      "step": 6024
    },
    {
      "epoch": 0.482,
      "grad_norm": 0.5336686968803406,
      "learning_rate": 0.00016788905187358315,
      "loss": 0.6046,
      "step": 6025
    },
    {
      "epoch": 0.48208,
      "grad_norm": 0.3020351827144623,
      "learning_rate": 0.0001678837178290439,
      "loss": 1.0394,
      "step": 6026
    },
    {
      "epoch": 0.48216,
      "grad_norm": 0.45417162775993347,
      "learning_rate": 0.0001678783837845046,
      "loss": 0.9538,
      "step": 6027
    },
    {
      "epoch": 0.48224,
      "grad_norm": 0.38863542675971985,
      "learning_rate": 0.00016787304973996534,
      "loss": 0.6819,
      "step": 6028
    },
    {
      "epoch": 0.48232,
      "grad_norm": 0.39723148941993713,
      "learning_rate": 0.00016786771569542608,
      "loss": 1.173,
      "step": 6029
    },
    {
      "epoch": 0.4824,
      "grad_norm": 0.32256028056144714,
      "learning_rate": 0.0001678623816508868,
      "loss": 0.7253,
      "step": 6030
    },
    {
      "epoch": 0.48248,
      "grad_norm": 0.38514840602874756,
      "learning_rate": 0.00016785704760634754,
      "loss": 0.7114,
      "step": 6031
    },
    {
      "epoch": 0.48256,
      "grad_norm": 0.43174421787261963,
      "learning_rate": 0.00016785171356180825,
      "loss": 0.745,
      "step": 6032
    },
    {
      "epoch": 0.48264,
      "grad_norm": 0.32830727100372314,
      "learning_rate": 0.000167846379517269,
      "loss": 0.6631,
      "step": 6033
    },
    {
      "epoch": 0.48272,
      "grad_norm": 0.4117443263530731,
      "learning_rate": 0.0001678410454727297,
      "loss": 0.6842,
      "step": 6034
    },
    {
      "epoch": 0.4828,
      "grad_norm": 0.4039877951145172,
      "learning_rate": 0.00016783571142819044,
      "loss": 0.8692,
      "step": 6035
    },
    {
      "epoch": 0.48288,
      "grad_norm": 0.3554667830467224,
      "learning_rate": 0.00016783037738365118,
      "loss": 0.6655,
      "step": 6036
    },
    {
      "epoch": 0.48296,
      "grad_norm": 0.33106788992881775,
      "learning_rate": 0.0001678250433391119,
      "loss": 0.6606,
      "step": 6037
    },
    {
      "epoch": 0.48304,
      "grad_norm": 0.47950780391693115,
      "learning_rate": 0.00016781970929457263,
      "loss": 0.8556,
      "step": 6038
    },
    {
      "epoch": 0.48312,
      "grad_norm": 0.619114339351654,
      "learning_rate": 0.00016781437525003335,
      "loss": 1.2043,
      "step": 6039
    },
    {
      "epoch": 0.4832,
      "grad_norm": 0.5556662678718567,
      "learning_rate": 0.0001678090412054941,
      "loss": 0.9027,
      "step": 6040
    },
    {
      "epoch": 0.48328,
      "grad_norm": 0.4320657551288605,
      "learning_rate": 0.0001678037071609548,
      "loss": 0.8515,
      "step": 6041
    },
    {
      "epoch": 0.48336,
      "grad_norm": 0.43146613240242004,
      "learning_rate": 0.00016779837311641554,
      "loss": 0.8935,
      "step": 6042
    },
    {
      "epoch": 0.48344,
      "grad_norm": 0.3662046194076538,
      "learning_rate": 0.00016779303907187628,
      "loss": 0.8289,
      "step": 6043
    },
    {
      "epoch": 0.48352,
      "grad_norm": 0.49436748027801514,
      "learning_rate": 0.000167787705027337,
      "loss": 0.8024,
      "step": 6044
    },
    {
      "epoch": 0.4836,
      "grad_norm": 0.4478660225868225,
      "learning_rate": 0.00016778237098279773,
      "loss": 0.8114,
      "step": 6045
    },
    {
      "epoch": 0.48368,
      "grad_norm": 0.31477516889572144,
      "learning_rate": 0.00016777703693825844,
      "loss": 0.9459,
      "step": 6046
    },
    {
      "epoch": 0.48376,
      "grad_norm": 0.4954531490802765,
      "learning_rate": 0.00016777170289371918,
      "loss": 0.7564,
      "step": 6047
    },
    {
      "epoch": 0.48384,
      "grad_norm": 0.456683874130249,
      "learning_rate": 0.0001677663688491799,
      "loss": 0.8709,
      "step": 6048
    },
    {
      "epoch": 0.48392,
      "grad_norm": 0.3469759225845337,
      "learning_rate": 0.00016776103480464064,
      "loss": 0.9773,
      "step": 6049
    },
    {
      "epoch": 0.484,
      "grad_norm": 0.4695155620574951,
      "learning_rate": 0.00016775570076010138,
      "loss": 1.0346,
      "step": 6050
    },
    {
      "epoch": 0.48408,
      "grad_norm": 0.4407667815685272,
      "learning_rate": 0.0001677503667155621,
      "loss": 0.9877,
      "step": 6051
    },
    {
      "epoch": 0.48416,
      "grad_norm": 0.3870248794555664,
      "learning_rate": 0.00016774503267102283,
      "loss": 0.6987,
      "step": 6052
    },
    {
      "epoch": 0.48424,
      "grad_norm": 0.5140731930732727,
      "learning_rate": 0.00016773969862648354,
      "loss": 0.7645,
      "step": 6053
    },
    {
      "epoch": 0.48432,
      "grad_norm": 0.36813884973526,
      "learning_rate": 0.00016773436458194428,
      "loss": 0.9347,
      "step": 6054
    },
    {
      "epoch": 0.4844,
      "grad_norm": 0.3862474858760834,
      "learning_rate": 0.000167729030537405,
      "loss": 0.7604,
      "step": 6055
    },
    {
      "epoch": 0.48448,
      "grad_norm": 0.2851061224937439,
      "learning_rate": 0.00016772369649286573,
      "loss": 0.652,
      "step": 6056
    },
    {
      "epoch": 0.48456,
      "grad_norm": 0.4894050061702728,
      "learning_rate": 0.00016771836244832645,
      "loss": 0.6498,
      "step": 6057
    },
    {
      "epoch": 0.48464,
      "grad_norm": 0.45537954568862915,
      "learning_rate": 0.0001677130284037872,
      "loss": 0.8808,
      "step": 6058
    },
    {
      "epoch": 0.48472,
      "grad_norm": 0.4536104202270508,
      "learning_rate": 0.0001677076943592479,
      "loss": 1.1048,
      "step": 6059
    },
    {
      "epoch": 0.4848,
      "grad_norm": 0.3847481608390808,
      "learning_rate": 0.00016770236031470864,
      "loss": 0.7636,
      "step": 6060
    },
    {
      "epoch": 0.48488,
      "grad_norm": 0.37813517451286316,
      "learning_rate": 0.00016769702627016935,
      "loss": 0.6604,
      "step": 6061
    },
    {
      "epoch": 0.48496,
      "grad_norm": 0.4962947964668274,
      "learning_rate": 0.0001676916922256301,
      "loss": 0.731,
      "step": 6062
    },
    {
      "epoch": 0.48504,
      "grad_norm": 0.4117860198020935,
      "learning_rate": 0.00016768635818109083,
      "loss": 0.9427,
      "step": 6063
    },
    {
      "epoch": 0.48512,
      "grad_norm": 0.40031754970550537,
      "learning_rate": 0.00016768102413655155,
      "loss": 0.6597,
      "step": 6064
    },
    {
      "epoch": 0.4852,
      "grad_norm": 0.4473375380039215,
      "learning_rate": 0.00016767569009201229,
      "loss": 0.7541,
      "step": 6065
    },
    {
      "epoch": 0.48528,
      "grad_norm": 0.45180314779281616,
      "learning_rate": 0.000167670356047473,
      "loss": 0.6424,
      "step": 6066
    },
    {
      "epoch": 0.48536,
      "grad_norm": 0.35617783665657043,
      "learning_rate": 0.00016766502200293374,
      "loss": 0.8622,
      "step": 6067
    },
    {
      "epoch": 0.48544,
      "grad_norm": 0.36891356110572815,
      "learning_rate": 0.00016765968795839445,
      "loss": 0.6236,
      "step": 6068
    },
    {
      "epoch": 0.48552,
      "grad_norm": 0.3615514934062958,
      "learning_rate": 0.0001676543539138552,
      "loss": 0.5558,
      "step": 6069
    },
    {
      "epoch": 0.4856,
      "grad_norm": 0.46932798624038696,
      "learning_rate": 0.0001676490198693159,
      "loss": 0.7405,
      "step": 6070
    },
    {
      "epoch": 0.48568,
      "grad_norm": 0.3947960138320923,
      "learning_rate": 0.00016764368582477664,
      "loss": 0.7949,
      "step": 6071
    },
    {
      "epoch": 0.48576,
      "grad_norm": 0.4647114872932434,
      "learning_rate": 0.00016763835178023736,
      "loss": 0.6602,
      "step": 6072
    },
    {
      "epoch": 0.48584,
      "grad_norm": 0.5170386433601379,
      "learning_rate": 0.0001676330177356981,
      "loss": 0.869,
      "step": 6073
    },
    {
      "epoch": 0.48592,
      "grad_norm": 0.36173778772354126,
      "learning_rate": 0.0001676276836911588,
      "loss": 0.7506,
      "step": 6074
    },
    {
      "epoch": 0.486,
      "grad_norm": 0.3101114332675934,
      "learning_rate": 0.00016762234964661955,
      "loss": 0.5022,
      "step": 6075
    },
    {
      "epoch": 0.48608,
      "grad_norm": 0.39017850160598755,
      "learning_rate": 0.0001676170156020803,
      "loss": 0.671,
      "step": 6076
    },
    {
      "epoch": 0.48616,
      "grad_norm": 0.5289316773414612,
      "learning_rate": 0.000167611681557541,
      "loss": 0.8117,
      "step": 6077
    },
    {
      "epoch": 0.48624,
      "grad_norm": 0.41764509677886963,
      "learning_rate": 0.00016760634751300174,
      "loss": 0.7356,
      "step": 6078
    },
    {
      "epoch": 0.48632,
      "grad_norm": 0.40339967608451843,
      "learning_rate": 0.00016760101346846245,
      "loss": 0.7131,
      "step": 6079
    },
    {
      "epoch": 0.4864,
      "grad_norm": 0.4387427568435669,
      "learning_rate": 0.0001675956794239232,
      "loss": 0.7071,
      "step": 6080
    },
    {
      "epoch": 0.48648,
      "grad_norm": 0.4150722324848175,
      "learning_rate": 0.0001675903453793839,
      "loss": 0.8503,
      "step": 6081
    },
    {
      "epoch": 0.48656,
      "grad_norm": 0.41166073083877563,
      "learning_rate": 0.00016758501133484465,
      "loss": 0.6935,
      "step": 6082
    },
    {
      "epoch": 0.48664,
      "grad_norm": 0.4204650819301605,
      "learning_rate": 0.00016757967729030539,
      "loss": 0.6734,
      "step": 6083
    },
    {
      "epoch": 0.48672,
      "grad_norm": 0.3569405972957611,
      "learning_rate": 0.0001675743432457661,
      "loss": 0.6433,
      "step": 6084
    },
    {
      "epoch": 0.4868,
      "grad_norm": 0.437284916639328,
      "learning_rate": 0.00016756900920122684,
      "loss": 0.843,
      "step": 6085
    },
    {
      "epoch": 0.48688,
      "grad_norm": 0.33173611760139465,
      "learning_rate": 0.00016756367515668755,
      "loss": 1.2748,
      "step": 6086
    },
    {
      "epoch": 0.48696,
      "grad_norm": 0.4415333867073059,
      "learning_rate": 0.0001675583411121483,
      "loss": 0.7693,
      "step": 6087
    },
    {
      "epoch": 0.48704,
      "grad_norm": 0.41510462760925293,
      "learning_rate": 0.000167553007067609,
      "loss": 0.7862,
      "step": 6088
    },
    {
      "epoch": 0.48712,
      "grad_norm": 0.371932715177536,
      "learning_rate": 0.00016754767302306974,
      "loss": 0.5377,
      "step": 6089
    },
    {
      "epoch": 0.4872,
      "grad_norm": 0.4269111752510071,
      "learning_rate": 0.00016754233897853048,
      "loss": 1.3271,
      "step": 6090
    },
    {
      "epoch": 0.48728,
      "grad_norm": 0.45351383090019226,
      "learning_rate": 0.0001675370049339912,
      "loss": 0.9416,
      "step": 6091
    },
    {
      "epoch": 0.48736,
      "grad_norm": 0.46873167157173157,
      "learning_rate": 0.00016753167088945194,
      "loss": 0.679,
      "step": 6092
    },
    {
      "epoch": 0.48744,
      "grad_norm": 0.32747143507003784,
      "learning_rate": 0.00016752633684491265,
      "loss": 1.1933,
      "step": 6093
    },
    {
      "epoch": 0.48752,
      "grad_norm": 0.3679742217063904,
      "learning_rate": 0.0001675210028003734,
      "loss": 1.0049,
      "step": 6094
    },
    {
      "epoch": 0.4876,
      "grad_norm": 0.39675870537757874,
      "learning_rate": 0.0001675156687558341,
      "loss": 0.7735,
      "step": 6095
    },
    {
      "epoch": 0.48768,
      "grad_norm": 0.3948838710784912,
      "learning_rate": 0.00016751033471129484,
      "loss": 0.7912,
      "step": 6096
    },
    {
      "epoch": 0.48776,
      "grad_norm": 0.39581871032714844,
      "learning_rate": 0.00016750500066675558,
      "loss": 0.8189,
      "step": 6097
    },
    {
      "epoch": 0.48784,
      "grad_norm": 0.4093029499053955,
      "learning_rate": 0.0001674996666222163,
      "loss": 0.9473,
      "step": 6098
    },
    {
      "epoch": 0.48792,
      "grad_norm": 0.5074636936187744,
      "learning_rate": 0.00016749433257767703,
      "loss": 1.0083,
      "step": 6099
    },
    {
      "epoch": 0.488,
      "grad_norm": 0.382071316242218,
      "learning_rate": 0.00016748899853313775,
      "loss": 0.6439,
      "step": 6100
    },
    {
      "epoch": 0.48808,
      "grad_norm": 0.41352927684783936,
      "learning_rate": 0.00016748366448859849,
      "loss": 0.9597,
      "step": 6101
    },
    {
      "epoch": 0.48816,
      "grad_norm": 0.4381924271583557,
      "learning_rate": 0.0001674783304440592,
      "loss": 0.7958,
      "step": 6102
    },
    {
      "epoch": 0.48824,
      "grad_norm": 0.25827649235725403,
      "learning_rate": 0.00016747299639951994,
      "loss": 0.735,
      "step": 6103
    },
    {
      "epoch": 0.48832,
      "grad_norm": 0.3617122769355774,
      "learning_rate": 0.00016746766235498068,
      "loss": 0.9256,
      "step": 6104
    },
    {
      "epoch": 0.4884,
      "grad_norm": 0.3665428161621094,
      "learning_rate": 0.0001674623283104414,
      "loss": 0.6311,
      "step": 6105
    },
    {
      "epoch": 0.48848,
      "grad_norm": 0.3682451844215393,
      "learning_rate": 0.00016745699426590213,
      "loss": 0.464,
      "step": 6106
    },
    {
      "epoch": 0.48856,
      "grad_norm": 0.4590517580509186,
      "learning_rate": 0.00016745166022136284,
      "loss": 0.8666,
      "step": 6107
    },
    {
      "epoch": 0.48864,
      "grad_norm": 0.36861273646354675,
      "learning_rate": 0.00016744632617682358,
      "loss": 0.799,
      "step": 6108
    },
    {
      "epoch": 0.48872,
      "grad_norm": 0.34232038259506226,
      "learning_rate": 0.0001674409921322843,
      "loss": 0.7042,
      "step": 6109
    },
    {
      "epoch": 0.4888,
      "grad_norm": 0.44411730766296387,
      "learning_rate": 0.00016743565808774504,
      "loss": 0.709,
      "step": 6110
    },
    {
      "epoch": 0.48888,
      "grad_norm": 0.38825100660324097,
      "learning_rate": 0.00016743032404320578,
      "loss": 0.7683,
      "step": 6111
    },
    {
      "epoch": 0.48896,
      "grad_norm": 0.35999003052711487,
      "learning_rate": 0.0001674249899986665,
      "loss": 0.8503,
      "step": 6112
    },
    {
      "epoch": 0.48904,
      "grad_norm": 0.4095226526260376,
      "learning_rate": 0.00016741965595412723,
      "loss": 0.6875,
      "step": 6113
    },
    {
      "epoch": 0.48912,
      "grad_norm": 0.4618605673313141,
      "learning_rate": 0.00016741432190958794,
      "loss": 0.6115,
      "step": 6114
    },
    {
      "epoch": 0.4892,
      "grad_norm": 0.3177095949649811,
      "learning_rate": 0.00016740898786504868,
      "loss": 0.6769,
      "step": 6115
    },
    {
      "epoch": 0.48928,
      "grad_norm": 0.3088798522949219,
      "learning_rate": 0.0001674036538205094,
      "loss": 0.5815,
      "step": 6116
    },
    {
      "epoch": 0.48936,
      "grad_norm": 0.3157036006450653,
      "learning_rate": 0.00016739831977597013,
      "loss": 1.079,
      "step": 6117
    },
    {
      "epoch": 0.48944,
      "grad_norm": 0.36771392822265625,
      "learning_rate": 0.00016739298573143087,
      "loss": 0.9531,
      "step": 6118
    },
    {
      "epoch": 0.48952,
      "grad_norm": 0.4123239815235138,
      "learning_rate": 0.00016738765168689159,
      "loss": 0.8083,
      "step": 6119
    },
    {
      "epoch": 0.4896,
      "grad_norm": 0.4005720913410187,
      "learning_rate": 0.00016738231764235233,
      "loss": 0.6671,
      "step": 6120
    },
    {
      "epoch": 0.48968,
      "grad_norm": 0.4798239767551422,
      "learning_rate": 0.00016737698359781304,
      "loss": 0.7341,
      "step": 6121
    },
    {
      "epoch": 0.48976,
      "grad_norm": 0.3607439696788788,
      "learning_rate": 0.00016737164955327378,
      "loss": 0.7286,
      "step": 6122
    },
    {
      "epoch": 0.48984,
      "grad_norm": 0.3625739514827728,
      "learning_rate": 0.00016736631550873452,
      "loss": 0.81,
      "step": 6123
    },
    {
      "epoch": 0.48992,
      "grad_norm": 0.49661970138549805,
      "learning_rate": 0.00016736098146419523,
      "loss": 0.8275,
      "step": 6124
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.459681898355484,
      "learning_rate": 0.00016735564741965597,
      "loss": 1.0442,
      "step": 6125
    },
    {
      "epoch": 0.49008,
      "grad_norm": 0.40076136589050293,
      "learning_rate": 0.00016735031337511668,
      "loss": 0.5479,
      "step": 6126
    },
    {
      "epoch": 0.49016,
      "grad_norm": 0.5933115482330322,
      "learning_rate": 0.00016734497933057742,
      "loss": 1.0486,
      "step": 6127
    },
    {
      "epoch": 0.49024,
      "grad_norm": 0.3308587968349457,
      "learning_rate": 0.00016733964528603814,
      "loss": 0.9122,
      "step": 6128
    },
    {
      "epoch": 0.49032,
      "grad_norm": 0.32009223103523254,
      "learning_rate": 0.00016733431124149888,
      "loss": 1.0321,
      "step": 6129
    },
    {
      "epoch": 0.4904,
      "grad_norm": 0.46135878562927246,
      "learning_rate": 0.00016732897719695962,
      "loss": 0.7963,
      "step": 6130
    },
    {
      "epoch": 0.49048,
      "grad_norm": 0.5181089043617249,
      "learning_rate": 0.00016732364315242033,
      "loss": 0.9009,
      "step": 6131
    },
    {
      "epoch": 0.49056,
      "grad_norm": 0.26514479517936707,
      "learning_rate": 0.00016731830910788107,
      "loss": 0.3556,
      "step": 6132
    },
    {
      "epoch": 0.49064,
      "grad_norm": 0.42522794008255005,
      "learning_rate": 0.00016731297506334178,
      "loss": 0.6547,
      "step": 6133
    },
    {
      "epoch": 0.49072,
      "grad_norm": 0.38372889161109924,
      "learning_rate": 0.00016730764101880252,
      "loss": 1.0679,
      "step": 6134
    },
    {
      "epoch": 0.4908,
      "grad_norm": 0.40485310554504395,
      "learning_rate": 0.00016730230697426323,
      "loss": 0.8891,
      "step": 6135
    },
    {
      "epoch": 0.49088,
      "grad_norm": 0.4348410964012146,
      "learning_rate": 0.00016729697292972397,
      "loss": 0.9699,
      "step": 6136
    },
    {
      "epoch": 0.49096,
      "grad_norm": 0.39515209197998047,
      "learning_rate": 0.00016729163888518471,
      "loss": 0.8322,
      "step": 6137
    },
    {
      "epoch": 0.49104,
      "grad_norm": 0.31771641969680786,
      "learning_rate": 0.00016728630484064543,
      "loss": 0.8869,
      "step": 6138
    },
    {
      "epoch": 0.49112,
      "grad_norm": 0.3552774488925934,
      "learning_rate": 0.00016728097079610617,
      "loss": 1.0868,
      "step": 6139
    },
    {
      "epoch": 0.4912,
      "grad_norm": 0.4466751515865326,
      "learning_rate": 0.00016727563675156688,
      "loss": 0.9879,
      "step": 6140
    },
    {
      "epoch": 0.49128,
      "grad_norm": 0.3818294405937195,
      "learning_rate": 0.00016727030270702762,
      "loss": 0.8271,
      "step": 6141
    },
    {
      "epoch": 0.49136,
      "grad_norm": 0.3669879138469696,
      "learning_rate": 0.00016726496866248833,
      "loss": 0.9864,
      "step": 6142
    },
    {
      "epoch": 0.49144,
      "grad_norm": 0.4697749614715576,
      "learning_rate": 0.00016725963461794907,
      "loss": 0.668,
      "step": 6143
    },
    {
      "epoch": 0.49152,
      "grad_norm": 0.44699564576148987,
      "learning_rate": 0.0001672543005734098,
      "loss": 0.5482,
      "step": 6144
    },
    {
      "epoch": 0.4916,
      "grad_norm": 0.45227301120758057,
      "learning_rate": 0.00016724896652887052,
      "loss": 1.1354,
      "step": 6145
    },
    {
      "epoch": 0.49168,
      "grad_norm": 0.35887035727500916,
      "learning_rate": 0.00016724363248433126,
      "loss": 1.0555,
      "step": 6146
    },
    {
      "epoch": 0.49176,
      "grad_norm": 0.39576971530914307,
      "learning_rate": 0.00016723829843979198,
      "loss": 0.61,
      "step": 6147
    },
    {
      "epoch": 0.49184,
      "grad_norm": 0.36331266164779663,
      "learning_rate": 0.00016723296439525272,
      "loss": 0.6376,
      "step": 6148
    },
    {
      "epoch": 0.49192,
      "grad_norm": 0.3171655237674713,
      "learning_rate": 0.00016722763035071343,
      "loss": 1.1409,
      "step": 6149
    },
    {
      "epoch": 0.492,
      "grad_norm": 0.34667569398880005,
      "learning_rate": 0.00016722229630617417,
      "loss": 0.7455,
      "step": 6150
    },
    {
      "epoch": 0.49208,
      "grad_norm": 0.3987506031990051,
      "learning_rate": 0.0001672169622616349,
      "loss": 0.7993,
      "step": 6151
    },
    {
      "epoch": 0.49216,
      "grad_norm": 0.47814178466796875,
      "learning_rate": 0.00016721162821709562,
      "loss": 0.7003,
      "step": 6152
    },
    {
      "epoch": 0.49224,
      "grad_norm": 0.33894795179367065,
      "learning_rate": 0.00016720629417255636,
      "loss": 0.9827,
      "step": 6153
    },
    {
      "epoch": 0.49232,
      "grad_norm": 0.5644680261611938,
      "learning_rate": 0.00016720096012801707,
      "loss": 0.8204,
      "step": 6154
    },
    {
      "epoch": 0.4924,
      "grad_norm": 0.4347665011882782,
      "learning_rate": 0.00016719562608347781,
      "loss": 0.7132,
      "step": 6155
    },
    {
      "epoch": 0.49248,
      "grad_norm": 0.411531925201416,
      "learning_rate": 0.00016719029203893853,
      "loss": 0.717,
      "step": 6156
    },
    {
      "epoch": 0.49256,
      "grad_norm": 0.48893651366233826,
      "learning_rate": 0.00016718495799439927,
      "loss": 0.8602,
      "step": 6157
    },
    {
      "epoch": 0.49264,
      "grad_norm": 0.33776959776878357,
      "learning_rate": 0.00016717962394986,
      "loss": 0.517,
      "step": 6158
    },
    {
      "epoch": 0.49272,
      "grad_norm": 0.40342777967453003,
      "learning_rate": 0.00016717428990532072,
      "loss": 0.8953,
      "step": 6159
    },
    {
      "epoch": 0.4928,
      "grad_norm": 0.4210093319416046,
      "learning_rate": 0.00016716895586078146,
      "loss": 0.627,
      "step": 6160
    },
    {
      "epoch": 0.49288,
      "grad_norm": 0.571774423122406,
      "learning_rate": 0.00016716362181624217,
      "loss": 0.9572,
      "step": 6161
    },
    {
      "epoch": 0.49296,
      "grad_norm": 0.3782770335674286,
      "learning_rate": 0.0001671582877717029,
      "loss": 0.7596,
      "step": 6162
    },
    {
      "epoch": 0.49304,
      "grad_norm": 0.376983642578125,
      "learning_rate": 0.00016715295372716362,
      "loss": 0.9928,
      "step": 6163
    },
    {
      "epoch": 0.49312,
      "grad_norm": 0.48788899183273315,
      "learning_rate": 0.00016714761968262436,
      "loss": 0.9476,
      "step": 6164
    },
    {
      "epoch": 0.4932,
      "grad_norm": 0.349025160074234,
      "learning_rate": 0.0001671422856380851,
      "loss": 0.5925,
      "step": 6165
    },
    {
      "epoch": 0.49328,
      "grad_norm": 0.36325669288635254,
      "learning_rate": 0.00016713695159354582,
      "loss": 0.779,
      "step": 6166
    },
    {
      "epoch": 0.49336,
      "grad_norm": 0.26698431372642517,
      "learning_rate": 0.00016713161754900656,
      "loss": 0.6257,
      "step": 6167
    },
    {
      "epoch": 0.49344,
      "grad_norm": 0.3592253625392914,
      "learning_rate": 0.00016712628350446727,
      "loss": 0.4873,
      "step": 6168
    },
    {
      "epoch": 0.49352,
      "grad_norm": 0.34772926568984985,
      "learning_rate": 0.000167120949459928,
      "loss": 0.715,
      "step": 6169
    },
    {
      "epoch": 0.4936,
      "grad_norm": 0.47543865442276,
      "learning_rate": 0.00016711561541538872,
      "loss": 0.9807,
      "step": 6170
    },
    {
      "epoch": 0.49368,
      "grad_norm": 0.4221703112125397,
      "learning_rate": 0.00016711028137084946,
      "loss": 0.9864,
      "step": 6171
    },
    {
      "epoch": 0.49376,
      "grad_norm": 0.42767831683158875,
      "learning_rate": 0.0001671049473263102,
      "loss": 0.7773,
      "step": 6172
    },
    {
      "epoch": 0.49384,
      "grad_norm": 0.384090781211853,
      "learning_rate": 0.00016709961328177091,
      "loss": 0.8109,
      "step": 6173
    },
    {
      "epoch": 0.49392,
      "grad_norm": 0.3716299533843994,
      "learning_rate": 0.00016709427923723165,
      "loss": 0.986,
      "step": 6174
    },
    {
      "epoch": 0.494,
      "grad_norm": 0.362404465675354,
      "learning_rate": 0.00016708894519269237,
      "loss": 0.8114,
      "step": 6175
    },
    {
      "epoch": 0.49408,
      "grad_norm": 0.3609127104282379,
      "learning_rate": 0.0001670836111481531,
      "loss": 0.6774,
      "step": 6176
    },
    {
      "epoch": 0.49416,
      "grad_norm": 0.34237366914749146,
      "learning_rate": 0.00016707827710361385,
      "loss": 0.579,
      "step": 6177
    },
    {
      "epoch": 0.49424,
      "grad_norm": 0.4460488557815552,
      "learning_rate": 0.00016707294305907456,
      "loss": 0.6664,
      "step": 6178
    },
    {
      "epoch": 0.49432,
      "grad_norm": 0.3718934655189514,
      "learning_rate": 0.0001670676090145353,
      "loss": 0.9294,
      "step": 6179
    },
    {
      "epoch": 0.4944,
      "grad_norm": 0.39381515979766846,
      "learning_rate": 0.000167062274969996,
      "loss": 0.7755,
      "step": 6180
    },
    {
      "epoch": 0.49448,
      "grad_norm": 0.3693886399269104,
      "learning_rate": 0.00016705694092545675,
      "loss": 0.8908,
      "step": 6181
    },
    {
      "epoch": 0.49456,
      "grad_norm": 0.4910009801387787,
      "learning_rate": 0.00016705160688091746,
      "loss": 1.1011,
      "step": 6182
    },
    {
      "epoch": 0.49464,
      "grad_norm": 0.3917491137981415,
      "learning_rate": 0.0001670462728363782,
      "loss": 0.7609,
      "step": 6183
    },
    {
      "epoch": 0.49472,
      "grad_norm": 0.40234050154685974,
      "learning_rate": 0.00016704093879183892,
      "loss": 1.1014,
      "step": 6184
    },
    {
      "epoch": 0.4948,
      "grad_norm": 0.38471877574920654,
      "learning_rate": 0.00016703560474729966,
      "loss": 0.6474,
      "step": 6185
    },
    {
      "epoch": 0.49488,
      "grad_norm": 0.4590964913368225,
      "learning_rate": 0.00016703027070276037,
      "loss": 0.7349,
      "step": 6186
    },
    {
      "epoch": 0.49496,
      "grad_norm": 0.47385233640670776,
      "learning_rate": 0.0001670249366582211,
      "loss": 0.7374,
      "step": 6187
    },
    {
      "epoch": 0.49504,
      "grad_norm": 0.44299450516700745,
      "learning_rate": 0.00016701960261368182,
      "loss": 0.8328,
      "step": 6188
    },
    {
      "epoch": 0.49512,
      "grad_norm": 0.46581029891967773,
      "learning_rate": 0.00016701426856914256,
      "loss": 0.77,
      "step": 6189
    },
    {
      "epoch": 0.4952,
      "grad_norm": 0.3893089294433594,
      "learning_rate": 0.0001670089345246033,
      "loss": 0.8565,
      "step": 6190
    },
    {
      "epoch": 0.49528,
      "grad_norm": 0.385975182056427,
      "learning_rate": 0.00016700360048006402,
      "loss": 0.881,
      "step": 6191
    },
    {
      "epoch": 0.49536,
      "grad_norm": 0.33766430616378784,
      "learning_rate": 0.00016699826643552475,
      "loss": 1.2306,
      "step": 6192
    },
    {
      "epoch": 0.49544,
      "grad_norm": 0.410097599029541,
      "learning_rate": 0.00016699293239098547,
      "loss": 0.9579,
      "step": 6193
    },
    {
      "epoch": 0.49552,
      "grad_norm": 0.4012936055660248,
      "learning_rate": 0.0001669875983464462,
      "loss": 0.5774,
      "step": 6194
    },
    {
      "epoch": 0.4956,
      "grad_norm": 0.5052250027656555,
      "learning_rate": 0.00016698226430190692,
      "loss": 0.5828,
      "step": 6195
    },
    {
      "epoch": 0.49568,
      "grad_norm": 0.5115175247192383,
      "learning_rate": 0.00016697693025736766,
      "loss": 0.8043,
      "step": 6196
    },
    {
      "epoch": 0.49576,
      "grad_norm": 0.4288000762462616,
      "learning_rate": 0.00016697159621282837,
      "loss": 1.0695,
      "step": 6197
    },
    {
      "epoch": 0.49584,
      "grad_norm": 0.3171462118625641,
      "learning_rate": 0.0001669662621682891,
      "loss": 0.7454,
      "step": 6198
    },
    {
      "epoch": 0.49592,
      "grad_norm": 0.27099326252937317,
      "learning_rate": 0.00016696092812374983,
      "loss": 0.4816,
      "step": 6199
    },
    {
      "epoch": 0.496,
      "grad_norm": 0.4956197440624237,
      "learning_rate": 0.00016695559407921057,
      "loss": 0.8415,
      "step": 6200
    }
  ],
  "logging_steps": 1,
  "max_steps": 37500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 50,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.2839013400179507e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
