{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install vllm"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T11:36:24.704224Z",
     "start_time": "2024-11-22T11:36:24.688552Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = open(\".hftoken\").read().strip()"
   ],
   "id": "d7552134c51f5ea7",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T11:36:28.227278Z",
     "start_time": "2024-11-22T11:36:28.216770Z"
    }
   },
   "cell_type": "code",
   "source": "model_name_or_path = \"Taiwar/lab2_lora_model-200steps_vllm\"",
   "id": "28a2f29329af9835",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T11:36:46.431193Z",
     "start_time": "2024-11-22T11:36:29.968267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
    "llm = LLM(model=model_name_or_path)"
   ],
   "id": "ee1ec7a193d8ebc9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-22 12:36:34 _custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError(\"No module named 'vllm._C'\")\n",
      "INFO 11-22 12:36:43 config.py:350] This model supports multiple tasks: {'embedding', 'generate'}. Defaulting to 'generate'.\n",
      "WARNING 11-22 12:36:43 config.py:428] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "WARNING 11-22 12:36:43 arg_utils.py:1013] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 11-22 12:36:43 config.py:1136] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 11-22 12:36:43 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='Taiwar/lab2_lora_model-200steps_vllm', speculative_config=None, tokenizer='Taiwar/lab2_lora_model-200steps_vllm', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Taiwar/lab2_lora_model-200steps_vllm, num_scheduler_steps=1, chunked_prefill_enabled=True multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)\n",
      "INFO 11-22 12:36:44 selector.py:261] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 11-22 12:36:44 selector.py:144] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonas\\anaconda3\\envs\\unsloth_env\\lib\\site-packages\\xformers\\ops\\fmha\\flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "C:\\Users\\jonas\\anaconda3\\envs\\unsloth_env\\lib\\site-packages\\xformers\\ops\\fmha\\flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n",
      "C:\\Users\\jonas\\anaconda3\\envs\\unsloth_env\\lib\\site-packages\\torch\\distributed\\distributed_c10d.py:633: UserWarning: Attempted to get default timeout for nccl backend, but NCCL support is not compiled\n",
      "  warnings.warn(\"Attempted to get default timeout for nccl backend, but NCCL support is not compiled\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "use_libuv was requested but PyTorch was build without libuv support",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 9\u001B[0m\n\u001B[0;32m      2\u001B[0m prompts \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHello, my name is\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe president of the United States is\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe capital of France is\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe future of AI is\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m      7\u001B[0m ]\n\u001B[0;32m      8\u001B[0m sampling_params \u001B[38;5;241m=\u001B[39m SamplingParams(temperature\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.8\u001B[39m, top_p\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.95\u001B[39m)\n\u001B[1;32m----> 9\u001B[0m llm \u001B[38;5;241m=\u001B[39m \u001B[43mLLM\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_name_or_path\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\unsloth_env\\lib\\site-packages\\vllm\\utils.py:1028\u001B[0m, in \u001B[0;36mdeprecate_args.<locals>.wrapper.<locals>.inner\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m   1021\u001B[0m             msg \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00madditional_message\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1023\u001B[0m         warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m   1024\u001B[0m             \u001B[38;5;167;01mDeprecationWarning\u001B[39;00m(msg),\n\u001B[0;32m   1025\u001B[0m             stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m,  \u001B[38;5;66;03m# The inner function takes up one level\u001B[39;00m\n\u001B[0;32m   1026\u001B[0m         )\n\u001B[1;32m-> 1028\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\unsloth_env\\lib\\site-packages\\vllm\\entrypoints\\llm.py:210\u001B[0m, in \u001B[0;36mLLM.__init__\u001B[1;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_overrides, mm_processor_kwargs, task, override_pooler_config, **kwargs)\u001B[0m\n\u001B[0;32m    207\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mengine_class \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_engine_class()\n\u001B[0;32m    209\u001B[0m \u001B[38;5;66;03m# TODO(rob): enable mp by default (issue with fork vs spawn)\u001B[39;00m\n\u001B[1;32m--> 210\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine_class\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_engine_args\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    211\u001B[0m \u001B[43m    \u001B[49m\u001B[43mengine_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43musage_context\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mUsageContext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLLM_CLASS\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    213\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequest_counter \u001B[38;5;241m=\u001B[39m Counter()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\unsloth_env\\lib\\site-packages\\vllm\\engine\\llm_engine.py:585\u001B[0m, in \u001B[0;36mLLMEngine.from_engine_args\u001B[1;34m(cls, engine_args, usage_context, stat_loggers)\u001B[0m\n\u001B[0;32m    583\u001B[0m executor_class \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_get_executor_cls(engine_config)\n\u001B[0;32m    584\u001B[0m \u001B[38;5;66;03m# Create the LLM engine.\u001B[39;00m\n\u001B[1;32m--> 585\u001B[0m engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    586\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvllm_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mengine_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    587\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexecutor_class\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexecutor_class\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    588\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlog_stats\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mengine_args\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdisable_log_stats\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    589\u001B[0m \u001B[43m    \u001B[49m\u001B[43musage_context\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43musage_context\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    590\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstat_loggers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstat_loggers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    591\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    593\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m engine\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\unsloth_env\\lib\\site-packages\\vllm\\engine\\llm_engine.py:347\u001B[0m, in \u001B[0;36mLLMEngine.__init__\u001B[1;34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, input_registry, mm_registry, use_cached_outputs)\u001B[0m\n\u001B[0;32m    343\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_registry \u001B[38;5;241m=\u001B[39m input_registry\n\u001B[0;32m    344\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_processor \u001B[38;5;241m=\u001B[39m input_registry\u001B[38;5;241m.\u001B[39mcreate_input_processor(\n\u001B[0;32m    345\u001B[0m     model_config)\n\u001B[1;32m--> 347\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_executor \u001B[38;5;241m=\u001B[39m \u001B[43mexecutor_class\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvllm_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvllm_config\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    349\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_config\u001B[38;5;241m.\u001B[39mtask \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124membedding\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_initialize_kv_caches()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\unsloth_env\\lib\\site-packages\\vllm\\executor\\executor_base.py:36\u001B[0m, in \u001B[0;36mExecutorBase.__init__\u001B[1;34m(self, vllm_config)\u001B[0m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprompt_adapter_config \u001B[38;5;241m=\u001B[39m vllm_config\u001B[38;5;241m.\u001B[39mprompt_adapter_config\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobservability_config \u001B[38;5;241m=\u001B[39m vllm_config\u001B[38;5;241m.\u001B[39mobservability_config\n\u001B[1;32m---> 36\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_init_executor\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\unsloth_env\\lib\\site-packages\\vllm\\executor\\gpu_executor.py:39\u001B[0m, in \u001B[0;36mGPUExecutor._init_executor\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparallel_config\u001B[38;5;241m.\u001B[39mworld_size \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m, (\n\u001B[0;32m     36\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGPUExecutor only supports single GPU.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_worker \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_worker()\n\u001B[1;32m---> 39\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdriver_worker\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minit_device\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_worker\u001B[38;5;241m.\u001B[39mload_model()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\unsloth_env\\lib\\site-packages\\vllm\\worker\\worker.py:145\u001B[0m, in \u001B[0;36mWorker.init_device\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    142\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    143\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNot support device type: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice_config\u001B[38;5;241m.\u001B[39mdevice\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    144\u001B[0m \u001B[38;5;66;03m# Initialize the distributed environment.\u001B[39;00m\n\u001B[1;32m--> 145\u001B[0m \u001B[43minit_worker_distributed_environment\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparallel_config\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrank\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    146\u001B[0m \u001B[43m                                    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdistributed_init_method\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    147\u001B[0m \u001B[43m                                    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlocal_rank\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    148\u001B[0m \u001B[38;5;66;03m# Set random seed.\u001B[39;00m\n\u001B[0;32m    149\u001B[0m set_random_seed(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_config\u001B[38;5;241m.\u001B[39mseed)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\unsloth_env\\lib\\site-packages\\vllm\\worker\\worker.py:459\u001B[0m, in \u001B[0;36minit_worker_distributed_environment\u001B[1;34m(parallel_config, rank, distributed_init_method, local_rank)\u001B[0m\n\u001B[0;32m    456\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Initialize the distributed environment.\"\"\"\u001B[39;00m\n\u001B[0;32m    457\u001B[0m set_custom_all_reduce(\u001B[38;5;129;01mnot\u001B[39;00m parallel_config\u001B[38;5;241m.\u001B[39mdisable_custom_all_reduce)\n\u001B[1;32m--> 459\u001B[0m \u001B[43minit_distributed_environment\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparallel_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mworld_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrank\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    460\u001B[0m \u001B[43m                             \u001B[49m\u001B[43mdistributed_init_method\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlocal_rank\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    462\u001B[0m ensure_model_parallel_initialized(parallel_config\u001B[38;5;241m.\u001B[39mtensor_parallel_size,\n\u001B[0;32m    463\u001B[0m                                   parallel_config\u001B[38;5;241m.\u001B[39mpipeline_parallel_size)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\unsloth_env\\lib\\site-packages\\vllm\\distributed\\parallel_state.py:992\u001B[0m, in \u001B[0;36minit_distributed_environment\u001B[1;34m(world_size, rank, distributed_init_method, local_rank, backend)\u001B[0m\n\u001B[0;32m    988\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m distributed_init_method \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, (\n\u001B[0;32m    989\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdistributed_init_method must be provided when initializing \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    990\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdistributed environment\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    991\u001B[0m     \u001B[38;5;66;03m# this backend is used for WORLD\u001B[39;00m\n\u001B[1;32m--> 992\u001B[0m     \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdistributed\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minit_process_group\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    993\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbackend\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbackend\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    994\u001B[0m \u001B[43m        \u001B[49m\u001B[43minit_method\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdistributed_init_method\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    995\u001B[0m \u001B[43m        \u001B[49m\u001B[43mworld_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mworld_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    996\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrank\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrank\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    997\u001B[0m \u001B[38;5;66;03m# set the local rank\u001B[39;00m\n\u001B[0;32m    998\u001B[0m \u001B[38;5;66;03m# local_rank is not available in torch ProcessGroup,\u001B[39;00m\n\u001B[0;32m    999\u001B[0m \u001B[38;5;66;03m# see https://github.com/pytorch/pytorch/issues/122816\u001B[39;00m\n\u001B[0;32m   1000\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m local_rank \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   1001\u001B[0m     \u001B[38;5;66;03m# local rank not set, this usually happens in single-node\u001B[39;00m\n\u001B[0;32m   1002\u001B[0m     \u001B[38;5;66;03m# setting, where we can use rank as local rank\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\unsloth_env\\lib\\site-packages\\torch\\distributed\\c10d_logger.py:79\u001B[0m, in \u001B[0;36m_exception_logger.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     76\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m     77\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs: _P\u001B[38;5;241m.\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: _P\u001B[38;5;241m.\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m _T:\n\u001B[0;32m     78\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 79\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     80\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m     81\u001B[0m         msg_dict \u001B[38;5;241m=\u001B[39m _get_msg_dict(func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\unsloth_env\\lib\\site-packages\\torch\\distributed\\c10d_logger.py:93\u001B[0m, in \u001B[0;36m_time_logger.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     90\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m     91\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs: _P\u001B[38;5;241m.\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: _P\u001B[38;5;241m.\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m _T:\n\u001B[0;32m     92\u001B[0m     t1 \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime_ns()\n\u001B[1;32m---> 93\u001B[0m     func_return \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     94\u001B[0m     time_spent \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime_ns() \u001B[38;5;241m-\u001B[39m t1\n\u001B[0;32m     96\u001B[0m     msg_dict \u001B[38;5;241m=\u001B[39m _get_msg_dict(func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\unsloth_env\\lib\\site-packages\\torch\\distributed\\distributed_c10d.py:1361\u001B[0m, in \u001B[0;36minit_process_group\u001B[1;34m(backend, init_method, timeout, world_size, rank, store, group_name, pg_options, device_id)\u001B[0m\n\u001B[0;32m   1357\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m store \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1358\u001B[0m     rendezvous_iterator \u001B[38;5;241m=\u001B[39m rendezvous(\n\u001B[0;32m   1359\u001B[0m         not_none(init_method), rank, world_size, timeout\u001B[38;5;241m=\u001B[39mtimeout\n\u001B[0;32m   1360\u001B[0m     )\n\u001B[1;32m-> 1361\u001B[0m     store, rank, world_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mrendezvous_iterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1362\u001B[0m     store\u001B[38;5;241m.\u001B[39mset_timeout(timeout)\n\u001B[0;32m   1364\u001B[0m     \u001B[38;5;66;03m# Use a PrefixStore to avoid accidental overrides of keys used by\u001B[39;00m\n\u001B[0;32m   1365\u001B[0m     \u001B[38;5;66;03m# different systems (e.g. RPC) in case the store is multi-tenant.\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\unsloth_env\\lib\\site-packages\\torch\\distributed\\rendezvous.py:211\u001B[0m, in \u001B[0;36m_tcp_rendezvous_handler\u001B[1;34m(url, timeout, **kwargs)\u001B[0m\n\u001B[0;32m    207\u001B[0m use_libuv \u001B[38;5;241m=\u001B[39m _get_use_libuv_from_query_dict(query_dict)\n\u001B[0;32m    209\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m result\u001B[38;5;241m.\u001B[39mhostname \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 211\u001B[0m store \u001B[38;5;241m=\u001B[39m \u001B[43m_create_c10d_store\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhostname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mport\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrank\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mworld_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_libuv\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    213\u001B[0m \u001B[38;5;28;01myield\u001B[39;00m (store, rank, world_size)\n\u001B[0;32m    215\u001B[0m \u001B[38;5;66;03m# If this configuration is invalidated, there is nothing we can do about it\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\unsloth_env\\lib\\site-packages\\torch\\distributed\\rendezvous.py:185\u001B[0m, in \u001B[0;36m_create_c10d_store\u001B[1;34m(hostname, port, rank, world_size, timeout, use_libuv)\u001B[0m\n\u001B[0;32m    183\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    184\u001B[0m     start_daemon \u001B[38;5;241m=\u001B[39m rank \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m--> 185\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mTCPStore\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    186\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhostname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mport\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mworld_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstart_daemon\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmulti_tenant\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_libuv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_libuv\u001B[49m\n\u001B[0;32m    187\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: use_libuv was requested but PyTorch was build without libuv support"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T11:37:02.022979Z",
     "start_time": "2024-11-22T11:37:01.993245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ],
   "id": "5a8afd381514a8bb",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mllm\u001B[49m\u001B[38;5;241m.\u001B[39mgenerate(prompts, sampling_params)\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m output \u001B[38;5;129;01min\u001B[39;00m outputs:\n\u001B[0;32m      4\u001B[0m     prompt \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39mprompt\n",
      "\u001B[1;31mNameError\u001B[0m: name 'llm' is not defined"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
